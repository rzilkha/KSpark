<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.36.0-wmf.27</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Cloudera</title>
    <ns>0</ns>
    <id>27059655</id>
    <revision>
      <id>999935292</id>
      <parentid>999934986</parentid>
      <timestamp>2021-01-12T18:11:51Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/Mhollison|Mhollison]] ([[User talk:Mhollison|talk]]): not providing a [[WP:RS|reliable source]] ([[WP:CITE]], [[WP:RS]]) ([[WP:HG|HG]]) (3.4.10)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21586" xml:space="preserve">{{short description|American technology company}}
{{Infobox company
| name = Cloudera, Inc.
| logo = Cloudera logo darkorange.png
| logo_size = 220
| type = [[Public company|Public]]
| traded_as = {{NYSE|CLDR}}&lt;br&gt;[[Russell 2000 Index|Russell 2000 Component]]
| num_locations = 
| founded = 2008
| industry = Software Development
| products = Cloudera Enterprise Data Hub, Cloudera Analytic DB, Cloudera Operational DB, Cloudera Data Science and Data Engineering, Cloudera Fast Forward Labs, Cloudera Essentials, and Cloudera Altus. Components include: Cloudera Manager, Cloudera Navigator, Cloudera Data Science Workbench, Cloudera Navigator Optimizer, Cloudera Altus, Apache Hadoop, Apache Spark, Apache Impala, Apache Kudu, Apache Sentry, Apache Spot
| services = [[Apache Hadoop]] distribution with support, professional services and training
| key_people = Rob Bearden (CEO)&lt;br&gt;Scott Aronson (CRO)&lt;br&gt;Jim Frankola (CFO)&lt;br&gt;Mick Hollison (CMO)&lt;br&gt;David Middler (CLO)&lt;br&gt;Arun Murthy (CPO)&lt;br&gt;Anupam Singh (CCO)
| num_employees = 3000&lt;ref name="cloudera.com"&gt;{{cite web|url=http://www.cloudera.com/about-cloudera.html|title=About Cloudera|work=Cloudera}}&lt;/ref&gt;
| homepage = {{URL|http://cloudera.com/}}
| revenue = 794 million USD (2020) &lt;ref&gt;url=https://www.sec.gov/ix?doc=/Archives/edgar/data/1535379/000162828020004231/cldr-20200131.htm&lt;/ref&gt;
| location = [[Palo Alto, California]]
}}

'''Cloudera, Inc.''' is a [[United States|US]]-based software company that provides a software platform for [[data engineering]], [[Data warehouse|data warehousing]], [[machine learning]] and analytics that runs in the cloud or on premises.

Cloudera started as a hybrid open-source [[Apache Hadoop]] distribution, CDH (Cloudera Distribution Including Apache Hadoop), that targeted enterprise-class deployments of that technology. Cloudera states that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects ([[Apache Spark]], [[Apache Hive]], [[Apache Avro]], [[Apache HBase]], and so on) that combine to form the Apache Hadoop platform. Cloudera is also a sponsor of the [[Apache Software Foundation]].&lt;ref name="Apache Software Foundation Sponsorship"&gt;{{cite web|url=http://www.apache.org/foundation/sponsorship.html|title=Apache Software Foundation Sponsorship|access-date=28 August 2012}}&lt;/ref&gt;

==History==
Cloudera was founded in 2008 by three engineers from [[Google]], [[Yahoo!]] and [[Facebook Inc.|Facebook]] ([[Christophe Bisciglia]], Amr Awadallah and [[Jeff Hammerbacher]], respectively) joined by a former [[Oracle Corporation|Oracle]] executive (Mike Olson).&lt;ref&gt;{{cite news |url= http://bits.blogs.nytimes.com/2009/03/16/bottling-the-magic-behind-google-and-facebook/ |title= Bottling the Magic Behind Google and Facebook |last= Vance |first= Ashlee |date= 16 March 2009 |access-date= 20 January 2014 |work=The New York Times}}&lt;/ref&gt;

Olson was the CEO of [[Sleepycat Software]], the creator of the open-source embedded database engine [[Berkeley DB]] (acquired by Oracle in 2006).

Awadallah was from Yahoo!, where he ran one of the first business units using Apache Hadoop for data analysis.&lt;ref&gt;{{cite web|url=http://articles.businessinsider.com/2012-01-31/news/31008410_1_yahoo-servers-and-storage-systems-oracle |title=This Former Yahoo-er's Startup Is So Hot, Even the CIA Invested In It |access-date=28 August 2012 |url-status=dead |archive-url=https://web.archive.org/web/20120209182001/http://articles.businessinsider.com/2012-01-31/news/31008410_1_yahoo-servers-and-storage-systems-oracle |archive-date=February 9, 2012 }}&lt;/ref&gt;

At Facebook Hammerbacher used Hadoop for building analytic applications involving massive volumes of user data.&lt;ref&gt;{{cite web|url=http://www.businessweek.com/magazine/content/11_17/b4225060960537.htm|title=This Tech Bubble Is Different|access-date=28 August 2012}}&lt;/ref&gt;

Architect [[Doug Cutting]], also a former chairman of the [[Apache Software Foundation]], authored the open-source [[Lucene]] and [[Nutch]] search technologies before he and [[Mike Cafarella]] wrote the initial Hadoop software in 2004. He designed and managed a Hadoop storage and analysis cluster at Yahoo! before joining Cloudera in 2009.

The chief operating officer was Kirk Dunn until 2015.&lt;ref&gt;{{cite web
|url=http://investing.businessweek.com/research/stocks/private/person.asp?personId=1032567&amp;privcapId=5648440&amp;previousCapId=5648440&amp;previousTitle=PowerFile,%20Inc.
|title=Bloomberg Business Week, Executive Profile Kirk Dunn 
|access-date=30 September 2012}}&lt;/ref&gt;

In March 2009, Cloudera announced the availability of Cloudera Distribution Including [[Apache Software Foundation|Apache]] [[Hadoop]] in conjunction with a $5 million investment led by [[Accel Partners]].&lt;ref&gt;{{cite web|url=https://techcrunch.com/2009/03/16/cloudera-raises-5-million-series-a-round-for-hadoop-commercialization/|title=Cloudera Raises $5 Million Series A Round For Hadoop Commercialization|last=Wauters |first=Robin|date=16 March 2009|publisher=TechCrunch|access-date=22 April 2010}}&lt;/ref&gt; In 2011, the company raised a further $40 million from Ignition Partners, [[Accel Partners]], [[Greylock Partners]], [[Meritech Capital Partners]], and [[In-Q-Tel]], a [[venture capital]] firm with open connections to the [[CIA]].&lt;ref&gt;{{cite web|url=https://venturebeat.com/2011/11/07/hadoop-cloudera-funding-ignition-accel-greylock/|title=Hadoop-based startup Cloudera raises $40M from Ignition Partners, Accel, Greylock|access-date=28 August 2012}}&lt;/ref&gt;

In June 2013, Tom Reilly became chief executive, although Olson remained as chairman of the board and chief strategist. Reilly was chief executive at [[ArcSight]] when it was acquired by [[Hewlett-Packard]] in 2010.&lt;ref&gt;{{Cite news |title= Cloudera taps new CEO for inevitable IPO push or acquisition: Former CEO becomes chairman and chief strategist |author= Timothy Prickett Morgan |work= The Register |date= 20 June 2013 |url= https://www.theregister.co.uk/2013/06/20/cloudera_taps_new_ceo_for_inevitable_ipo_push_or_acquisition/ |access-date= 20 January 2014 }}&lt;/ref&gt; In March 2014 Cloudera announced a $900 million funding round, led by Intel Capital ($740 million), for which Intel received an 18% share in Cloudera and Intel dropped its own Hadoop distribution and dedicated 70 Intel engineers to work exclusively on Cloudera projects. Additional funds came from T Rowe Price; Google Ventures; an affiliate of MSD Capital, L.P., the private investment firm for Michael S. Dell; and others.&lt;ref&gt;{{cite web|url=https://www.reuters.com/article/2014/03/31/us-intel-cloudera-idUSBREA2U0ME20140331|title=Intel invested $740 million to buy 18 percent of Cloudera|author=Noel Randewich|date=31 March 2014|work=Reuters}}&lt;/ref&gt;

In January 2012, [[Oracle Corporation]] announced a partnership with Cloudera for its [[Oracle Big Data Appliance]].&lt;ref&gt;{{cite web|url=http://www.oracle.com/us/corporate/press/1453796|title=Oracle Selects Cloudera to Provide Apache Hadoop Distribution and Tools for Oracle Big Data Appliance}}&lt;/ref&gt;

In January 2013, [[Dell]] announced a partnership with Cloudera.&lt;ref&gt;{{cite web|url=http://www.dell.com/learn/us/en/555/solutions/hadoop-big-data-solution|title=Dell Apache Hadoop Solutions - Dell|author=Dell us|work=Dell|access-date=2016-03-19|archive-url=https://web.archive.org/web/20160329083821/http://www.dell.com/learn/us/en/555/solutions/hadoop-big-data-solution|archive-date=2016-03-29|url-status=dead}}&lt;/ref&gt;

In March 2013, [[Intel]] invested $740 million in Cloudera for an 18% investment.&lt;ref&gt;{{cite web|url=https://www.reuters.com/article/us-intel-cloudera-idUSBREA2U0ME20140331|title=Intel invested $740 million to buy 18 percent of Cloudera}}&lt;/ref&gt;
In May 2013, [[SAS Institute]] announced a partnership.&lt;ref&gt;{{cite web|url=https://blog.cloudera.com/blog/2013/05/how-the-sas-and-cloudera-platforms-work-together/|title=How the SAS and Cloudera Platforms Work Together|work=Cloudera Engineering Blog}}&lt;/ref&gt;

In June 2014, [[Accenture]] announced a service offering based on Cloudera.&lt;ref&gt;{{cite web|url=https://newsroom.accenture.com/industries/systems-integration-technology/accenture-forms-alliance-with-cloudera-to-empower-enterprises-with-a-data-as-a-platform-offering.htm|title=Accenture Forms Alliance with Cloudera to Empower Enterprises with Data as a Platform Offering}}&lt;/ref&gt;

In June 2014, Cloudera acquired Gazzang, which developed encryption and key management software.&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2014-06-03-cloudera-strengthens-hadoop-security-with-acquisition-of-gazzang.html|title=cloudera strengthens hadoop security with acquisition of gazzang|work=Cloudera}}&lt;/ref&gt;

In October 2014, Cloudera announced the first [[Payment Card Industry Data Security Standard]] (PCI) with [[MasterCard]].&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2014-10-22-cloudera-enterprise-certified-for-full-pci-compliance.html|title=cloudera enterprise certified for full pci compliance|work=Cloudera}}&lt;/ref&gt;

In February 2015, [[Deloitte]] announced an alliance with Cloudera.&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2015-02-19-cloudera-and-deloitte-announce-strategic-alliance.html|title=cloudera deloitte announce strategic alliance|work=Cloudera}}&lt;/ref&gt;

In May 2015, [[Capgemini]] announced a marketing program for [[SAP HANA]] and Cloudera.&lt;ref&gt;{{cite web|url=https://www.capgemini.com/resources/insights-driven-operations-with-sap-hana-and-cloudera-enterprise|title=Insights-Driven Operations with SAP HANA and Cloudera Enterprise|work=Capgemini Capgemini Worldwide}}&lt;/ref&gt;

On June 30, 2015, [[Kyvos Insights]] announced a Big Data BI Acceleration Layer based on its breakthrough OLAP on Big Data technology.&lt;ref&gt;{{cite web|url=http://www.kyvosinsights.com/newsroom/kyvos-insights-emerges-from-stealth-mode-to-change-the-landscape-of-big-data-analytics|title=Kyvos Emerges from Stealth Mode to Change the Landscape of Big Data Analytics|work=Kyvos}}&lt;/ref&gt;

On July 9, 2015, Cloudera announced a partnership with [[Teradata]].&lt;ref&gt;{{cite web|url=http://www.cloudera.com/about-cloudera/press-center/press-releases/2015-07-09-cloudera-and-terdata-announce-enterprise-ready-appliance-for-hadoop.html|title=Cloudera and Teradata Announce Integrated, Enterprise-Ready Appliance for Hadoop|work=Cloudera}}&lt;/ref&gt;

In September 2015, Cloudera announced the Kudu storage manager.&lt;ref&gt;{{cite web|url=https://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/|title=Kudu: New Apache Hadoop Storage for Fast Analytics on Fast Data|work=Cloudera Engineering Blog}}&lt;/ref&gt;

In September 2015, [[Microsoft Azure]] announced full support of Cloudera Enterprise.&lt;ref&gt;{{cite web|url=https://azure.microsoft.com/en-us/blog/full-cloudera-enterprise-edh-support-on-azure/|title=Full support of Cloudera Enterprise on Azure|publisher=Microsoft}}&lt;/ref&gt;

In January 2016, [[Tata Consultancy Services]] announced an [[Internet of things]] framework based on Cloudera for sensor data analytics.&lt;ref&gt;{{cite web|url=http://www.cloudera.com/resources/solution-brief/tcs-sensor-data-analytics-iot-framework-with-cloudera.html|title=TCS Sensor Data Analytics IoT Framework with Cloudera|work=Cloudera}}&lt;/ref&gt;

In February 2016, EMC announces evolution in advanced storage with DSSD support for Cloudera&lt;ref&gt;{{cite web |url=http://www.emc.com/collateral/data-sheet/h14899-ds-d5-emc-dssd-and-cloudera-solution-brief.pdf |title=EMC DSSD and Cloudera Evolve Hadoop: Innovating to deliver high-performance enterprise analytics on Hadoop |publisher=[[EMC Corporation]] |year=2016 |access-date=April 25, 2016 |archive-url=https://web.archive.org/web/20160304102300/http://www.emc.com/collateral/data-sheet/h14899-ds-d5-emc-dssd-and-cloudera-solution-brief.pdf |archive-date=March 4, 2016 |url-status=dead }}&lt;/ref&gt;

In 2016, Cloudera was ranked #5 on the [[Forbes|Forbes Cloud 100]] list.&lt;ref&gt;{{cite web|url=https://www.forbes.com/cloud100/|title=Forbes Cloud 100|work=[[Forbes]]|access-date=28 October 2016}}&lt;/ref&gt;

Cloudera filed for an [[initial public offering]] in March 2017,&lt;ref&gt;{{cite web|url=https://www.sec.gov/Archives/edgar/data/1535379/000162828017003221/projectthunders-1.htm|title=Cloudera SEC S-1 filing|access-date= April 5, 2017}}&lt;/ref&gt; and on April 28, 2017, its shares were listed on the [[New York Stock Exchange]] under the symbol CLDR.&lt;ref&gt;{{Cite web|url=https://www.cnbc.com/2017/04/28/cloudera-ipo-cldr-opening-price-on-first-trading-day.html|title=Cloudera shares close more than 20% higher on Day 1|last=Balakrishnan|first=Anita|date= April 28, 2017 |website=CNBC |access-date= April 29, 2017}}&lt;/ref&gt;

In September 2017, Cloudera acquired Fast Forward Labs (FFL), a leading machine learning and applied artificial intelligence research and development company in an effort to deepen Cloudera’s expertise in the application of machine learning to practical business problems. The new division is headed up by FFL co-founder and CEO Hilary Mason.&lt;ref&gt;{{cite web|url=https://techcrunch.com/2017/09/07/cloudera-acquires-ai-research-startup-fast-forward-labs/|title=Cloudera acquires AI research firm Fast Forward Labs|publisher=[[TechCrunch]]|access-date=3 January 2018}}&lt;/ref&gt;

In October 2018, Cloudera and [[Hortonworks]] announced they would be merging in an all-stock merger of equals.&lt;ref&gt;{{cite web|url=https://www.businesswire.com/news/home/20181003005869/en/Cloudera-Hortonworks-Announce-Merger-Create-World%E2%80%99s-Leading|title=Cloudera and Hortonworks Announce Merger to Create World's Leading Next Generation Data Platform and Deliver Industry's First Enterprise Data Cloud|publisher=[[BusinessWire]]|access-date=3 October 2018}}&lt;/ref&gt; The merger completed in January 2019.&lt;ref&gt;{{cite web |title=Cloudera Hortonworks completed planned merger. Jan 2019 |url=https://hortonworks.com/press-releases/cloudera-hortonworks-complete-planned-merger|date= 3 January 2019|access-date= 16 January 2019}}&lt;/ref&gt;

On June 21, 2019, Cloudera and [[IBM]] announced a strategic partnership&lt;ref&gt;{{cite web|url=https://newsroom.ibm.com/2019-06-21-IBM-Cloudera-Announce-Strategic-Partnership|title=IBM, Cloudera Announce Strategic Partnership|work=IBM|date= 21 June 2019|access-date= 3 January 2020}}&lt;/ref&gt; to "offer an industry-leading, enterprise-grade Big Data distribution plus an ecosystem of integrated products and services – all designed to help organizations achieve faster analytic results at scale."&lt;ref&gt;{{cite web|url=https://www.cloudera.com/partners/solutions/ibm.html|title=ICloudera and IBM - Delivering enterprise data and analytic solutions from the edge to AI|work=Cloudera.|access-date= 3 January 2020}}&lt;/ref&gt;

On September 4, 2019 Cloudera announced they would be acquiring Arcadia Data, a provider of cloud-native AI-powered business intelligence and real-time analytics.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/about/news-and-blogs/press-releases/2019-09-04-cloudera-agrees-to-acquire-arcadia-data-to-accelerate-time-to-insight-for-data-analytics.html|title=Cloudera Agrees to Acquire Arcadia Data to Accelerate Time-to-Insight for Data Analytics|date=4 September 2019|access-date= 8 May 2020}}&lt;/ref&gt;

On January 13, 2020, Cloudera announced that Rob Bearden will be appointed as the President and CEO of the company.&lt;ref&gt;{{Cite web|url=https://www.cloudera.com/content/www/en-us/about/news-and-blogs/press-releases/2020-01-13-cloudera-appoints-robert-bearden-president-and-chief-executive-officer.html|title=Cloudera Appoints Robert Bearden President and Chief Executive Officer|last1=Cloudera|first1=© 2020|last2=Terms|first2=Inc All rights reserved|website=Cloudera|language=en|access-date=2020-01-13|last3=Policy|first3=Conditions {{!}} Privacy|last4=Hadoop|first4=Data Policy {{!}} Unsubscribe / Do Not Sell My Personal Information Apache|last5=trademarks|first5=associated open source project names are trademarks of the Apache Software Foundation For a complete list of|last6=Here|first6=Click}}&lt;/ref&gt;

==Products and services==
{{Primary sources|date=February 2019}}

Cloudera offers software, services and support in five bundles available both on-premise and across multiple cloud providers:
*'''Cloudera Enterprise Data Hub''' - Cloudera’s comprehensive data management platform including all of Data Science &amp; Engineering, Operational DB, Analytic DB, and Cloudera Essentials.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/enterprise-data-hub.html|title=Cloudera Enterprise Data Hub|access-date=2 January 2018}}&lt;/ref&gt;
*'''Cloudera Analytic DB''' - Cloudera’s technologies built on the core Cloudera Essentials platform.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/analytic-db.html|title=Cloudera Analytic DB|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Operational DB - Cloudera’s high-scale NoSQL technologies for real-time, data applications built on the core Cloudera Essentials platform.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/operational-db.html|title=Cloudera Operational DB|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Data Science and Engineering - Cloudera’s technologies that enable efficient, high-scale data processing, data science, and machine learning on top of the Core Essentials platform.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/data-science-and-engineering.html|title=Cloudera Data Science and Engineering|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Essentials - Cloudera’s core data management platform for fast, easy, and secure large-scale data processing that includes Cloudera’s enterprise-ready management capabilities (Cloudera Manager) and open source platform distribution (CDH).&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/cloudera-essentials.html|title=Cloudera Essentials|access-date=2 January 2018}}&lt;/ref&gt;

Cloudera also offers a managed-service offering on the cloud:
* Altus Data Engineering which provides a cloud-native offering of Cloudera Data Engineering.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/altus.html|title=Altus Data Engineering|access-date=2 January 2018}}&lt;/ref&gt;

Cloudera also offers the following free software versions:
* Cloudera Express - includes Cloudera’s CDH open-source platform and a no-charge version of its deployment, monitoring, and administration suite, Cloudera Manager.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/downloads/manager.html|title=Cloudera Express|access-date=2 January 2018}}&lt;/ref&gt;
* CDH (Cloudera’s Distribution including Apache Hadoop) - is Cloudera’s 100% open source platform distribution including Apache Hadoop, Apache Spark, Apache Impala, Apache Kudu, Apache HBase, and many more.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/developers/inside-cdh.html|title=Cloudera's Distribution including Apache Hadoop|access-date=2 January 2018}}&lt;/ref&gt;

Additional technologies in addition to its open-source distribution:
* Cloudera Director - a tool distributed without charge that enables easy deployment of cloud-native Cloudera clusters on-demand across multiple cloud providers.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/product-components/cloudera-director.html|title=Cloudera Director|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Data Science Workbench - A data science tool for secure collaboration and model development add-on for Cloudera Enterprise Data Engineering and Data Science as well as Cloudera Enterprise Data Hub.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/data-science-and-engineering/data-science-workbench.html|title=Cloudera Data Science Workbench|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Navigator - critical data governance functionality for Cloudera’s platform, offering capabilities such as data discovery, audit, lineage, metadata management, encryption, encryption key management, and policy enforcement to help meet regulatory compliance requirements.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/product-components/cloudera-navigator.html|title=Cloudera Navigator|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Navigator Optimizer - a software-as-a-service tool to assist in identifying, migrating, and tuning traditional database workloads to Cloudera’s platform as well as analyze and tune workloads running on Cloudera’s platform.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/product-components/cloudera-navigator-optimizer.html|title=Cloudera Navigator Optimizer|access-date=2 January 2018}}&lt;/ref&gt;
* Cloudera Manager - an administrative tool for fast, easy, and secure deployment, monitoring, alerting, and management of Cloudera’s platform.&lt;ref&gt;{{cite web|url=https://www.cloudera.com/products/product-components/cloudera-manager.html|title=Cloudera Manager|access-date=2 January 2018}}&lt;/ref&gt;

==References==
{{reflist|30em}}

==External links==
* {{Official website|http://cloudera.com/}}
{{Finance links
| name = Cloudera
| symbol = CLDR.N
| reuters = CLDR.N
| bloomberg = CLDR:US
| sec_cik = CLDR
| yahoo = CLDR
| google = CLDR
}}

{{authority control}}

[[Category:2008 establishments in California]]
[[Category:2017 initial public offerings]]
[[Category:Big data companies]]
[[Category:Cloud computing providers]]
[[Category:Cloud infrastructure]]
[[Category:Companies based in Palo Alto, California]]
[[Category:Companies listed on the New York Stock Exchange]]
[[Category:Hadoop]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Software companies established in 2008]]
[[Category:Software companies of the United States]]</text>
      <sha1>4w7ggabv1aqti9jtdkpx04fpir8q9xn</sha1>
    </revision>
  </page>
  <page>
    <title>Cascading (software)</title>
    <ns>0</ns>
    <id>1915249</id>
    <revision>
      <id>967719474</id>
      <parentid>902369868</parentid>
      <timestamp>2020-07-14T21:43:41Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead.) #IABot (v2.0.1</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8555" xml:space="preserve">{{Infobox software
| name                   = Cascading
| status                 = Active
| latest release version = 3.1
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]]
| website                = http://www.cascading.org/
}}
'''Cascading''' is a software [[abstraction layer]] for [[Apache Hadoop]] and [[Apache Flink]].  Cascading is used to create and execute complex data processing workflows on a Hadoop cluster using any [[JVM]]-based language ([[Java (programming language)|Java]], [[JRuby]], [[Clojure]], etc.), hiding the underlying complexity of [[MapReduce]] jobs.  It is open source and available under the [[Apache License]]. Commercial support is available from Driven, Inc.&lt;ref&gt;[http://driven.io/support Cascading support page]&lt;/ref&gt;

Cascading was originally authored by Chris Wensel, who later founded Concurrent, Inc, which has been re-branded as Driven.&lt;ref&gt;[http://www.driven.io Driven, Inc.]&lt;/ref&gt; Cascading is being actively developed by the community{{citation needed|date=October 2014}} and a number of add-on modules are available.&lt;ref&gt;{{Cite web |url=http://www.cascading.org/modules.html |title=Cascading modules |access-date=2011-08-22 |archive-url=https://web.archive.org/web/20110811082852/http://www.cascading.org/modules.html |archive-date=2011-08-11 |url-status=dead }}&lt;/ref&gt;

==Architecture==
To use Cascading, Apache Hadoop must also be installed, and the Hadoop job .jar must contain the Cascading .jars. Cascading consists of a data processing API, integration API, process planner and process scheduler.

Cascading leverages the scalability of Hadoop but abstracts standard data processing operations away from underlying map and reduce tasks.&lt;ref&gt;[http://codeascraft.etsy.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/  Blog post by Etsy describing their use of Cascading with Hadoop]&lt;/ref&gt;{{Better source|date=October 2013}}  Developers use Cascading to create a .jar file that describes the required processes. It follows a ‘source-pipe-sink’ paradigm, where data is captured from sources, follows reusable ‘pipes’ that perform data analysis processes, where the results are stored in output files or ‘sinks’. Pipes are created independent from the data they will process. Once tied to data sources and sinks, it is called a ‘flow’. These flows can be grouped into a ‘cascade’, and the process scheduler will ensure a given flow does not execute until all its dependencies are satisfied. Pipes and flows can be reused and reordered to support different business needs.&lt;ref&gt;[http://www.cascading.org/1.2/userguide/pdf/userguide.pdf Cascading User Guide] {{webarchive |url=https://web.archive.org/web/20110206053054/http://www.cascading.org/1.2/userguide/pdf/userguide.pdf |date=February 6, 2011 }}&lt;/ref&gt;

Developers write the code in a JVM-based language and do not need to learn MapReduce. The resulting program can be regression tested and integrated with external applications like any other Java application.&lt;ref&gt;[http://www.driven.io/features/ Driven product page]&lt;/ref&gt;

Cascading is most often used for ad targeting, log file analysis, bioinformatics, machine learning, [[predictive analytics]], web content mining, and extract, transform and load (ETL) applications.&lt;ref&gt;[http://www.driven.io/ Driven home page]&lt;/ref&gt;

==Uses of Cascading==
Cascading was cited as one of the top five most powerful Hadoop projects by SD Times in 2011,&lt;ref name=sdtimes1&gt;{{cite news
| last = Handy
| first = Alex
| date = 1 June 2011
| title = The top five most powerful Hadoop projects
| url =http://www.sdtimes.com/content/article.aspx?ArticleID=35596&amp;page=1
| newspaper = [[SD Times]]
| location = 
| publisher = 
| accessdate = 26 October 2013
}}&lt;/ref&gt;{{Unreliable source?|date=October 2013}} as a major open source project relevant to bioinformatics&lt;ref name=biomedcent1&gt;{{cite news
| last = Taylor
| first = Ronald
| date = 21 December 2010
| title = An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics
| url = http://www.biomedcentral.com/1471-2105/11/S12/S1
| newspaper = [[BioMed Central]]
| location = 
| publisher = [[Springer Science+Business Media]]
| accessdate = 26 October 2013
}}&lt;/ref&gt;{{Unreliable source?|date=October 2013}} and is included in Hadoop: A Definitive Guide, by Tom White.&lt;ref&gt;[https://books.google.com/books?id=Nff49D7vnJcC&amp;lpg=PA539&amp;dq=cascading%20hadoop&amp;pg=PA548#v=onepage&amp;q=cascading%20hadoop&amp;f=false White, Tom, “Hadoop: The Definitive Guide,” O’Reilly Media, Inc., 2010, pp. 539 – 549.]&lt;/ref&gt; The project has also been cited in presentations, conference proceedings and Hadoop user group meetings as a useful tool for working with Hadoop&lt;ref&gt;[http://www.slideshare.net/pacoid/getting-started-on-hadoop Nathan, Paco (Wikipedia: [[Paco Nathan]]), “Getting Started on Hadoop” presentation for the SV Cloud Computing Meetup, 7/19/2010.]&lt;/ref&gt;&lt;ref&gt;{{Cite web |url=http://www.smartfrog.org/wiki/download/attachments/6193590/hadoop_and_beyond.pdf?version=1&amp;modificationDate=1238073739000 |title=Julio Guijarro, Steve Loughran and Paolo Castagna, “Hadoop and beyond,” HP Labs, Bristol UK, 2008. |access-date=2011-08-22 |archive-url=https://web.archive.org/web/20111001095030/http://www.smartfrog.org/wiki/download/attachments/6193590/hadoop_and_beyond.pdf?version=1&amp;modificationDate=1238073739000 |archive-date=2011-10-01 |url-status=dead }}&lt;/ref&gt;&lt;ref&gt;[http://www.slideshare.net/hadoopusergroup/flightcaster-presentation-hadoop Cross, Bradford, “Flightcaster_HUG,” Presentation at the Bay Area Hadoop Users’ Group, March 26, 2010]&lt;/ref&gt;&lt;ref&gt;[http://www.slideshare.net/chriscurtin/nosql-hadoop-cascading-june-2010?from=ss_embed Curtin, Christopher, “NoSQL, Hadoop and Cascading,” June 2010.]&lt;/ref&gt; and with [[Apache Spark]]&lt;ref&gt;{{Cite web|url=https://spark-summit.org/2014/talk/using-cascading-to-build-data-centric-applications-on-spark|title=Using Cascading to Build Data-centric Applications on Spark|date=2014-05-07|website=Spark Summit 2014|access-date=2016-03-25}}&lt;/ref&gt;

* MultiTool on [[Amazon Web Services]] was developed using Cascading.&lt;ref&gt;[http://aws.amazon.com/articles/2293?_encoding=UTF8&amp;jiveRedirect=1 Cascading{{Not a typo|.}}Multitool on AWS]&lt;/ref&gt;
* LogAnalyzer for [[Amazon CloudFront]] was developed using Cascading.&lt;ref&gt;[http://aws.amazon.com/articles/2440?_encoding=UTF8&amp;jiveRedirect=1 LogAnalyzer for Amazon CloudFront]&lt;/ref&gt; 
* BackType&lt;ref&gt;[http://tech.backtype.com/ BackType blog] {{webarchive |url=https://web.archive.org/web/20110825014616/http://tech.backtype.com/ |date=August 25, 2011 }}&lt;/ref&gt; - social analytics platform
* Etsy&lt;ref&gt;[http://codeascraft.etsy.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/  Blog post by Etsy describing their use of Cascading with Hadoop]&lt;/ref&gt; - marketplace
* FlightCaster&lt;ref&gt;[http://www.informationweek.com/news/software/infrastructure/224000240 FlightCaster]&lt;/ref&gt; - predicting flight delays
* Ion Flux&lt;ref&gt;[http://www.concurrentinc.com/casestudies/ion_flux Ion Flux] {{webarchive |url=https://web.archive.org/web/20111023203553/http://www.concurrentinc.com/casestudies/ion_flux |date=October 23, 2011 }}&lt;/ref&gt;  - analyzing DNA sequence data
* RapLeaf&lt;ref&gt;[http://blog.rapleaf.com/dev/2008/09/05/goodbye-mapreduce-hello-cascading/ RapLeaf Blog] {{webarchive |url=https://web.archive.org/web/20110201023302/http://blog.rapleaf.com/dev/2008/09/05/goodbye-mapreduce-hello-cascading/ |date=February 1, 2011 }}&lt;/ref&gt;  - personalization and recommendation systems
* Razorfish&lt;ref&gt;[http://aws.amazon.com/solutions/case-studies/razorfish/ Razorfish]&lt;/ref&gt;  - digital advertising

==Domain-Specific Languages Built on Cascading==
* PyCascading&lt;ref&gt;[https://github.com/twitter/pycascading]&lt;/ref&gt; - by Twitter, available on GitHub
* Cascading.jruby&lt;ref&gt;[https://github.com/gmarabout/cascading.jruby Cascading.jruby]&lt;/ref&gt; - developed by Gregoire Marabout, available on GitHub
* [[Cascalog]]&lt;ref&gt;[https://github.com/nathanmarz/cascalog Cascalog]&lt;/ref&gt; - authored by [[Nathan Marz]], available on GitHub
* Scalding&lt;ref&gt;[https://github.com/twitter/scalding Scalding]&lt;/ref&gt; - A Scala API for Cascading.  Makes it easier to transition Cascading/Scalding code to Spark. By Twitter, available on GitHub

==References==
{{Reflist|30em}}

==External links==
* [http://www.cascading.org/ Official website]

[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Cloud infrastructure]]
[[Category:Hadoop]]</text>
      <sha1>2pr2sfl8fdqok1l5l9gfa413mq3qa1k</sha1>
    </revision>
  </page>
  <page>
    <title>Apache HBase</title>
    <ns>0</ns>
    <id>16266878</id>
    <revision>
      <id>1000275186</id>
      <parentid>996867987</parentid>
      <timestamp>2021-01-14T12:43:23Z</timestamp>
      <contributor>
        <ip>153.185.210.28</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10217" xml:space="preserve">{{Use dmy dates|date=December 2020}}
{{Infobox software
| name = Apache HBase
| logo = Apache_HBase_Logo.svg
| logo size = 300px
| screenshot = 
| caption = 
| author = [[Powerset (company)|Powerset]]
| developer = [[Apache Software Foundation]]
| released = {{Start date and age|df=yes|2008|03|28}}
| latest release version = {{Multiple releases
  |branch1 = 1.4.x
  |version1 = 1.4.13
  |date1 = {{Start date and age|df=yes|2020|02|29}}&lt;ref name="releases"&gt;{{cite web|url=https://hbase.apache.org/downloads.html|title=Apache HBase – Apache HBase Downloads|access-date=13 January 2021}}&lt;/ref&gt;
  |branch2 = 1.6.x
  |version2 = 1.6.0
  |date2 = {{Start date and age|df=yes|2020|03|06}}&lt;ref name="releases" /&gt;
  |branch3 = 2.2.x
  |version3 = 2.2.6
  |date3 = {{Start date and age|df=yes|2020|09|04}}&lt;ref name="releases" /&gt;
  }}
| latest release date = &lt;!--{{release date|df=yes|2019|06|14}}--&gt;
| latest preview version = 2.4.0
| latest preview date = {{Start date and age|df=yes|2020|12|15}}&lt;ref name="releases" /&gt;
| operating system = [[Cross-platform]]
| genre = [[Distributed database]]
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;hbase.git|HBase Repository}}
| programming language = [[Java (programming language)|Java]]
| license = [[Apache License 2.0]]
| website = {{URL|//hbase.apache.org/}}
}}
'''HBase''' is an [[Open-source software|open-source]] [[Non-relational database|non-relational]] [[distributed database]] modeled after [[Google|Google's]] [[Bigtable]] and written in [[Java (programming language)|Java]]. It is developed as part of [[Apache Software Foundation]]'s [[Hadoop|Apache Hadoop]] project and runs on top of [[Hadoop Distributed File System|HDFS (Hadoop Distributed File System)]] or [[Alluxio]], providing Bigtable-like capabilities for Hadoop. That is, it provides a [[fault-tolerant]] way of storing large quantities of [[sparse file|sparse]] data (small amounts of information caught within a large collection of empty or unimportant data, such as finding the 50 largest items in a group of 2 billion records, or finding the non-zero items representing less than 0.1% of a huge collection).

HBase features compression, in-memory operation, and [[Bloom filter]]s on a per-column basis as outlined in the original Bigtable paper.&lt;ref&gt;[http://db.usenix.org//events/osdi06/tech/chang/chang_html/ Chang, et al. (2006). Bigtable: A Distributed Storage System for Structured Data]&lt;/ref&gt; Tables in HBase can serve as the input and output for [[Mapreduce|MapReduce]] jobs run in Hadoop, and may be accessed through the Java API but also through [[REST]], [[Avro (serialization system)|Avro]] or [[Thrift (protocol)|Thrift]] gateway APIs. HBase is a [[wide-column store]] and has been widely adopted because of its lineage with Hadoop and HDFS. HBase runs on top of HDFS and is well-suited for faster read and write operations on large datasets with high throughput and low input/output latency.

HBase is not a direct replacement for a classic [[SQL]] [[database]], however [[Apache Phoenix]] project provides a SQL layer for HBase as well as [[JDBC]] driver that can be integrated with various [[analytics]] and [[business intelligence]] applications.  The [[Apache Trafodion]] project provides a SQL query engine with [[ODBC]] and [[JDBC]] drivers and [[ACID#Distributed transactions|distributed ACID transaction protection]] across multiple statements, tables and rows that use HBase as a storage engine.

HBase is now serving several data-driven websites&lt;ref&gt;{{cite web|url=http://hbase.apache.org/poweredbyhbase.html|title=Apache HBase – Powered By Apache HBase™|website=hbase.apache.org|access-date=8 April 2018}}&lt;/ref&gt; but [[Facebook]]'s Messaging Platform recently migrated from HBase to [[MyRocks]].&lt;ref name="the-underlying-technology-of-messages"&gt;{{cite web|url=https://code.fb.com/data-infrastructure/migrating-messenger-storage-to-optimize-performance/|title=Migrating Messenger storage to optimize performance|website=www.facebook.com|access-date=5 July 2018}}&lt;/ref&gt;&lt;ref name="theregister"&gt;[https://www.theregister.co.uk/2010/12/17/facebook_messages_tech/ Facebook: Why our 'next-gen' comms ditched MySQL] Retrieved: 17 December 2010&lt;/ref&gt; Unlike relational and traditional databases, HBase does not support SQL scripting; instead the equivalent is written in Java, employing similarity with a MapReduce application.

In the parlance of Eric Brewer's [[CAP Theorem]], HBase is a CP type system.

==History==
Apache HBase began as a project by the company [[Powerset (company)|Powerset]] out of a need to process massive amounts of data for the purposes of [[natural-language user interface|natural-language search]]. Since 2010 it is a top-level Apache project.

[[Facebook]] elected to implement its new messaging platform using HBase in November 2010, but migrated away from HBase in 2018.&lt;ref name="the-underlying-technology-of-messages" /&gt;

The 2.2.z series is the current stable release line, it supersedes earlier release lines.

== Use cases &amp; production deployments ==

=== Enterprises that use HBase ===
The following is a list of notable enterprises that have used or are using HBase:
{{div col|colwidth=25em}}
* [[23andMe]]
* [[Adobe Systems|Adobe]]
* [[Airbnb]] uses HBase as part of its AirStream realtime stream computation framework&lt;ref&gt;{{cite web|url=http://www.slideshare.net/HBaseCon/apache-hbase-at-airbnb|title=Apache HBase at Airbnb|last=HBaseCon|date=2 August 2016|website=slideshare.net|access-date=8 April 2018}}&lt;/ref&gt;
* [[Alibaba Group]]
* [[Amadeus IT Group]], as its main long-term storage DB.
* [[Bloomberg L.P.|Bloomberg]], for time series data storage
* [[Facebook]] used HBase for its messaging platform between 2010 and 2018
* [[Flipkart]] uses HBase for its search index&lt;ref&gt;{{cite web|url=https://tech.flipkart.com/sherlock-near-real-time-search-indexing-95519783859d|title=Near Real Time Search Indexing}}&lt;/ref&gt; and user insights.&lt;ref&gt;{{cite web|url=https://tech.flipkart.com/is-data-locality-always-out-of-the-box-in-hadoop-not-really-2ae9c95163cb|title=Is data locality always out of the box in Hadoop?}}&lt;/ref&gt;
* [[Flurry (company)|Flurry]]
* [[HubSpot]] 
* [[Imgur]] uses HBase to power its notifications system&lt;ref&gt;{{cite web|url=https://dzone.com/articles/why-imgur-dropped-mysql-in-favor-of-hbase|title=Why Imgur Dropped MySQL in Favor of HBase - DZone Database|website=dzone.com|access-date=8 April 2018}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://blog.imgur.com/2015/09/15/tech-tuesday-imgur-notifications-from-mysql-to-hbase/|title=Tech Tuesday: Imgur Notifications: From MySQL to HBase - The Imgur Blog|website=blog.imgur.com|access-date=8 April 2018}}&lt;/ref&gt;
* [[Kakao]]&lt;ref&gt;{{cite web|url=http://apachebigdata2015.sched.org/event/de6abfbd8f0b9e66b1c03feb2b9e2078?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no |title=S2Graph : A Large-Scale Graph Database with HBase |author=Doyung Yoon}}&lt;/ref&gt;
*[[Meesho]]
* [[Netflix]]&lt;ref&gt;{{cite web|url=http://apachebigdata2015.sched.org/event/2a65daf0baa4cfbc227a8cb74a9103a2?iframe=no&amp;w=i:100;&amp;sidebar=yes&amp;bg=no |title=Netflix: Integrating Spark at Petabyte Scale |author=Cheolsoo Park and Ashwin Shankar}}&lt;/ref&gt;
* [[Pinterest]]&lt;ref&gt;{{Cite web|url=https://medium.com/pinterest-engineering/improving-hbase-backup-efficiency-at-pinterest-86159da4b954|title=Improving HBase backup efficiency at Pinterest|last=Engineering|first=Pinterest|date=2018-03-30|website=Medium|language=en|access-date=2020-04-14}}&lt;/ref&gt;
* [[Quicken Loans]]
* [[Richrelevance]]
* [[Rocket Fuel Inc.|Rocket Fuel]] 
* [[Salesforce.com]]&lt;ref&gt;{{cite web|url=https://www.slideshare.net/salesforceeng/hbase-at-salesforcecom|title=Hbase at Salesforce.com}}&lt;/ref&gt;
* [[Sears]]
* [[Sophos]], for some of their back-end systems.
* [[Spotify]] uses HBase as base for Hadoop and machine learning jobs.&lt;ref&gt;{{cite web|url=http://apachebigdata2015.sched.org/event/2a65daf0baa4cfbc227a8cb74a9103a2?iframe=no&amp;w=i:100;&amp;sidebar=yes&amp;bg=no |title=How Apache Drives Spotify's Music Recommendations |author=Josh Baer}}&lt;/ref&gt; 
* [[Tuenti]] uses HBase for its messaging platform.&lt;ref&gt;{{cite web|url=http://corporate.tuenti.com/en/dev/blog/tuenti-group-chat-simple-yet-complex |title=Tuenti Group Chat: Simple, yet complex}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://github.com/tuenti/asyncthrift |title=Tuenti Asyncthrift}}&lt;/ref&gt;
* [[Xiaomi]]
* [[Yahoo!]]
{{div col end}}

==See also==
{{Portal|Free and open-source software|Computer programming}}
* [[NoSQL]]
* [[Wide column store]]
* [[Bigtable]]
* [[Apache Cassandra]]
* [[Oracle NoSQL Database|Oracle NOSQL]]
* [[Hypertable]]
* [[Apache Accumulo]]
* [[MongoDB]]
* [[Project Voldemort]]
* [[Riak]]
* [[Sqoop]]
* [[Elasticsearch]]
* [[Apache Phoenix]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
* {{cite book
| first1       = Nick 
| last1        = Dimiduk
| first2      = Amandeep 
| last2       = Khurana 
| date        = 28 November 2012
| title       = HBase in Action
| publisher   = [[Manning Publications]]
| edition     = 1st
| page        = 350 
| isbn        = 978-1617290527
| url         = &lt;!-- http://www.manning.com/dimidukkhurana/ --&gt;
}}
* {{cite book
| first1      = Lars 
| last1       = George
| date        = 20 September 2011
| title       = HBase: The Definitive Guide
| publisher   = [[O'Reilly Media]]
| edition     = 1st
| page        = 556
| isbn        = 978-1449396107
| url         = http://shop.oreilly.com/product/0636920014348.do
}}
* {{cite book
| first       = Yifeng 
| last        = Jiang 
| date        = 16 August 2012
| title       = HBase Administration Cookbook 
| publisher   = [[Packt Publishing]]
| edition     = 1st
| page        = 332
| isbn        = 978-1849517140
| url         = http://www.packtpub.com/hbase-administration-for-optimum-database-performance-cookbook/book
}}
{{Refend}}

==External links==
*[//hbase.apache.org/ Official Apache HBase homepage]
*[https://www.netwoven.com/2013/10/10/hbase-overview-of-architecture-and-data-model/ HBase Overview of Architecture]

{{Apache Software Foundation}}

{{DEFAULTSORT:Hbase}}
[[Category:Apache Software Foundation projects|HBase]]
[[Category:Bigtable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:NoSQL]]
[[Category:Structured storage]]</text>
      <sha1>8p0uxb5gyqdlt3sx9ziajv0d9bswy9o</sha1>
    </revision>
  </page>
  <page>
    <title>MapR</title>
    <ns>0</ns>
    <id>31958369</id>
    <revision>
      <id>999555767</id>
      <parentid>963364670</parentid>
      <timestamp>2021-01-10T19:58:27Z</timestamp>
      <contributor>
        <username>SmartyPants22</username>
        <id>29913457</id>
      </contributor>
      <comment>Added for...see</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6809" xml:space="preserve">{{for|the South African Army infantry regiment|Mapungubwe Regiment}}
{{Infobox company
| name             = MapR Technologies, Inc.
| logo             = MapR Technologies Inc. logo.png
| logo_caption     = MapR Company Logo
| industry         = [[Business software]]
| founder          = M.C Srivas, John Schroeder
| location_city    = [[Santa Clara, California]]
| location_country = United States of America
| locations        = 10
| key_people       = John Schroeder {{small|(CEO and Chairman of the Board)}}&lt;ref&gt;{{Cite web |title= MapR Blogs |url= https://mapr.com/blog/author/john-schroeder/ |accessdate= February 21, 2018 }}&lt;/ref&gt;&lt;br&gt;MC Srivas {{small| (co-founder and former CTO)}}&lt;ref&gt;{{Cite web | title= Why MapR Just Shook-Up Its Management| url=https://www.cmswire.com/big-data/why-mapr-just-shook-up-its-management/ |author=Virginia Backaitis  |accessdate=February 21, 2018}}&lt;/ref&gt;
| products         = Converged Data Platform, [[Apache Hadoop]] Distribution
| website          = http://www.mapr.com/
}}

'''MapR''' was a [[business software]] company headquartered in [[Santa Clara, California]]. MapR software provides access to a variety of data sources from a single [[computer cluster]], including [[big data]] workloads such as [[Apache Hadoop]] and [[Apache Spark]], a [[distributed file system]], a multi-model [[database management system]],  and [[event stream processing]], combining [[analytics]] in [[Real-time computing|real-time]] with operational applications. Its technology runs on both [[commodity hardware]] and public [[cloud computing]] services. In August 2019, following financial difficulties, the technology and intellectual property of the company were sold to [[Hewlett Packard Enterprise]].&lt;ref&gt;[https://siliconangle.com/2019/08/05/end-big-data-era-hpe-acquire-maprs-assets/ The sun sets on the big-data era: HPE to acquire MapR’s assets]&lt;/ref&gt;&lt;ref&gt;{{cite web |title=Hewlett Packard Enterprise Advances its Intelligent Data Platform with Acquisition of MapR’s Business Assets |url=https://www.businesswire.com/news/home/20190805005247/en/Hewlett-Packard-Enterprise-Advances-Intelligent-Data-Platform |website=Business Wire |accessdate=5 August 2019}}&lt;/ref&gt;

== Funding ==
MapR was [[privately held]] with original funding of $9 million from [[Lightspeed Venture Partners]] and [[New Enterprise Associates]] in 2009. MapR executives come from [[Google]], [[Lightspeed Venture Partners]], [[Informatica]], [[EMC Corporation]] and [[Veoh]].  MapR had an additional round of funding led by [[Redpoint]] in August, 2011.&lt;ref name="nyt-redpoint"&gt;{{cite web|url=https://www.nytimes.com/external/venturebeat/2011/08/30/30venturebeat-mapr-makes-friends-of-hadoop-and-the-enterpr-34892.html|title=MapR Makes Friends of Hadoop|accessdate=19 Sep 2011}}&lt;/ref&gt;  A round in 2013 was led by [[Mayfield Fund]] that also included Greenspring Associates.&lt;ref&gt;{{cite web|url=http://allthingsd.com/20130318/mapr-lands-30-million-series-c-led-by-mayfield-fund/|title=MapR Lands $30 Million Series C Led by Mayfield Fund.|last=Hesseldahl|first=Arik|publisher=All Things D|accessdate=9 May 2013}}&lt;/ref&gt;  In June 2014, MapR closed a $110 million financing round that was led by [[Google Capital]].  [[Qualcomm Ventures]] also participated, along with existing investors [[Lightspeed Venture Partners]], [[Mayfield Fund]], [[New Enterprise Associates]] and [[Redpoint Ventures]].&lt;ref&gt;{{cite web|url=http://recode.net/2014/06/30/mapr-raises-110-million-in-round-led-by-google-capital/|title=MapR Raises $110 Million in Round Led by Google Capital|last1=Hesseldahl|first1=Arik|website=recode.net|publisher=Revere Digital LLC|accessdate=2 July 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.foxbusiness.com/technology/2014/06/30/google-capital-leads-110m-round-in-big-data-startup-mapr/|title=Google Capital Leads $110M Round in Big-Data Startup MapR|last1=Gabrielle|first1=Karol|website=foxbusiness.com|publisher=Fox News Network|accessdate=2 July 2014}}&lt;/ref&gt;

In May 2019, the company announced that it would shut down if it was unable to find additional funding.&lt;ref&gt;{{cite news | url=https://siliconangle.com/2019/05/30/mapr-may-shut-investor-pulls-following-extremely-poor-results/ | title=Big-data bombshell: MapR may shut down as investor pulls out after ‘extremely poor results’ | date=May 30, 2019 | publisher=Silicon Angle | author=Paul Gillin}}&lt;/ref&gt;

==History==

The company contributed to the Apache Hadoop projects [[HBase]], [[Pig (programming language)|Pig]], [[Apache Hive]], and [[Apache ZooKeeper]].&lt;ref name="contrib"&gt;{{cite web|url=http://gigaom.com/cloud/why-mapr-is-right-to-give-back-to-apache-hadoop/|title=Why MapR is Right to Give Back to Apache Hadoop|last=Harris|first=Derrick|date=2011-06-01|website=GigaOM|accessdate=1 June 2011}}&lt;/ref&gt;

MapR entered a technology licensing agreement with [[EMC Corporation]] on 2011, supporting an EMC-specific distribution of Apache Hadoop.&lt;ref name="emchadoop"&gt;{{cite web|url=https://gigaom.com/2011/05/25/startup-mapr-underpins-emcs-hadoop-effort/|title=Startup MapR Underpins EMC’s Hadoop Effort|last=Harris|first=Derrick|date=2011-05-25|website=GigaOM|archiveurl=https://web.archive.org/web/20110822161507/http://msoftnews.com/google/startup-mapr-underpins-emc%E2%80%99s-hadoop-effort/|archivedate=22 August 2011|url-status=dead|accessdate=1 June 2011}}&lt;/ref&gt;  MapR was selected by [[Amazon Web Services]] to provide an upgraded version of Amazon's Elastic MapReduce (EMR) service.&lt;ref name="emr-announce"&gt;{{cite web|url=http://gigaom.com/cloud/amazon-taps-mapr-for-high-powered-elastic-mapreduce/|title=Amazon Taps MapR for High Powered Elastic Map Reduce|last=Harris|first=Derrick|date=2012-06-13|website=GigaOM|accessdate=25 June 2011}}&lt;/ref&gt; MapR broke the minute sort speed record on Google's Compute platform.&lt;ref&gt;{{cite web|last=Metz|first=Cade|title=Google Teams With Prodigal Son to Bust Data Sort Record|url=https://www.wired.com/wiredenterprise/2013/02/google-mapr-data-sort-record/|publisher=Wired|accessdate=9 May 2013}}&lt;/ref&gt;

==See also==

* [[Apache Accumulo]]
* [[Apache Software Foundation]]
* [[Big data]]
* [[Bigtable]]
* [[Database-centric architecture]]
* [[Hadoop]]
* [[MapReduce]]
* [[HBase]]
* [[RainStor]]

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.mapr.com MapR Homepage]

{{DEFAULTSORT:Mapr}}
&lt;!--- Categories ---&gt;
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Hadoop]]
[[Category:Companies based in San Jose, California]]
[[Category:Big data companies]]
[[Category:Software companies of the United States]]</text>
      <sha1>dlkj4l813s5qw9cd03qqkrrlq6jyf14</sha1>
    </revision>
  </page>
  <page>
    <title>Hortonworks</title>
    <ns>0</ns>
    <id>35142247</id>
    <revision>
      <id>963160241</id>
      <parentid>960645340</parentid>
      <timestamp>2020-06-18T06:47:06Z</timestamp>
      <contributor>
        <username>Yoodaba</username>
        <id>34528355</id>
      </contributor>
      <comment>establishment categories</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5792" xml:space="preserve">{{Infobox company
| name             = Hortonworks, Inc.
| logo             = Hortonworks logo.svg
| type             = [[Subsidiary]]
| foundation       = {{start date and age|2011}}
| location_city    = [[Santa Clara, California]]
| location_country = United States
| num_employees    = ~1,110 {{small|(2017)}}&lt;ref&gt;{{cite web|url=http://hortonworks.com/about-us/quick-facts/|title=Hortonworks : Quick Facts - Hortonworks|publisher=|accessdate=15 July 2017}}&lt;/ref&gt;
| key_people       = &lt;!-- only if notable and cited --&gt;
| industry         = [[Software|Computer software]]
| products         = Hortonworks Data Platform, [[Hortonworks DataFlow]], Hortonworks DataPlane
| services         = 
| parent           = [[Cloudera]]
| homepage         = {{url|http://hortonworks.com/|Hortonworks.com}}
}}

'''Hortonworks''' was a data software company based in [[Santa Clara, California]] that developed and supported [[open-source software]] (primarily around [[Apache Hadoop]]) designed to manage [[Big Data]] and associated processing.

Hortonworks software was used to build enterprise data services and applications such as [[Internet of things|IOT]] (connected cars, for example), single view of X (such as customer, risk, patient), and advanced analytics and [[machine learning]] (such as next best action and realtime cybersecurity).  Hortonworks had three interoperable product lines:

* Hortonworks Data Platform (HDP): based on [[Apache Hadoop]], [[Apache Hive]], [[Apache Spark]]
* [[Hortonworks DataFlow]] (HDF): based on [[Apache NiFi]], [[Apache Storm]], [[Apache Kafka]]
* Hortonworks DataPlane services: based on Apache Atlas and Cloudbreak and a pluggable architecture into which partners such as [[IBM]] can add their services.&lt;ref&gt;{{Cite web|url=https://www.datanami.com/2018/04/17/hortonworks-upgrades-data-plane-service/|title=Hortonworks upgrades DataPlane Services|accessdate=April 30, 2018}}&lt;/ref&gt;

In January 2019, Hortonworks completed its merger with Cloudera.&lt;ref&gt;{{cite web|title=Feb 2019 Cloudera Hortonworks completed planned merger.|url=https://hortonworks.com/press-releases/cloudera-hortonworks-complete-planned-merger/}}&lt;/ref&gt;

==History==
Hortonworks was formed in June 2011 as an independent company, funded by $23 million [[venture capital]] from  [[Yahoo!]] and [[Benchmark Capital]]. Its first office was in [[Sunnyvale, California]].&lt;ref name="info"&gt;{{Cite news|title=Hadoop Big Data Startup Spins Out Of Yahoo |work=Information Week |date=June 29, 2011 |author=Charles Babcock |url=http://www.informationweek.com/news/development/database/231000658 |url-status=live |archiveurl=https://web.archive.org/web/20110704055947/http://www.informationweek.com/news/development/database/231000658 |archivedate=July 4, 2011 |accessdate=February 21, 2017 }}&lt;/ref&gt; The company employed contributors to the [[open source software]] project [[Apache Hadoop]].&lt;ref&gt;{{cite news |url= https://www.reuters.com/article/us-splunk-bigdata-idUSBRE83K00B20120421 |authors= Sarah McBride and Alistair Bar |date= April 20, 2012 |title= Big-data investors look for the next Splunk |publisher= Reuters |accessdate= February 21, 2017 }}&lt;/ref&gt; The Hortonworks Data Platform (HDP) product included Apache Hadoop and was used for storing, processing, and analyzing large volumes of data. The platform was designed to deal with data from many sources and formats. The platform included Hadoop technology such as the Hadoop Distributed File System, [[MapReduce]], Pig, Hive, [[HBase]], [[Apache ZooKeeper|ZooKeeper]], and additional components.&lt;ref&gt;{{Cite news |title= HortonWorks Hones a Hadoop Distribution |work= PC World |date= November 1, 2011 |author= Joab Jackson |url= http://www.pcworld.com/article/242916/hortonworks_hones_a_hadoop_distribution.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

Eric Baldeschweiler (from Yahoo) was initial chief executive, and Rob Bearden chief operating officer, formerly from [[SpringSource]]. Benchmark partner [[Peter Fenton (venture capitalist)|Peter Fenton]] was a board member. The company name refers to the character [[Horton the Elephant]], since the elephant is the symbol for Hadoop.&lt;ref name="info" /&gt;&lt;ref&gt;{{Cite news |title= Yahoo! seeds Hadoop startup on open source dream: Hortonworks hears a Big Data revolution |author= Cade Metz |date= June 28, 2011 |work= The Register |url= https://www.theregister.co.uk/2011/06/28/yahoo_spins_off_hadoop_startup/ |accessdate= February 21, 2017 }}&lt;/ref&gt;

In October 2018, Hortonworks and [[Cloudera]] announced they would be merging in an all-stock merger of equals.&lt;ref&gt;{{cite web|url=https://www.businesswire.com/news/home/20181003005869/en/Cloudera-Hortonworks-Announce-Merger-Create-World%E2%80%99s-Leading|title=Cloudera and Hortonworks Announce Merger to Create World’s Leading Next Generation Data Platform and Deliver Industry’s First Enterprise Data Cloud|publisher=[[BusinessWire]]|accessdate=3 October 2018}}&lt;/ref&gt;  After the merger, the Apache products of Hortonworks became Cloudera Data Platform.

==References==
{{reflist|30em}}

==External links==
{{Finance links historical
| name = Hortonworks
| sec_cik = 0001610532
}}

[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Companies based in Sunnyvale, California]]
[[Category:Companies based in Santa Clara, California]]
[[Category:Companies formerly listed on NASDAQ]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Software companies established in 2011]]
[[Category:2011 establishments in the United States]]
[[Category:2011 establishments in California]]
[[Category:Big data companies]]
[[Category:2014 initial public offerings]]
[[Category:2019 mergers and acquisitions]]
[[Category:Software companies of the United States]]
[[Category:Companies established in 2011]]</text>
      <sha1>484mwkg55rxi5s9y0kulx7fvkdmr11u</sha1>
    </revision>
  </page>
  <page>
    <title>Sqoop</title>
    <ns>0</ns>
    <id>36956866</id>
    <revision>
      <id>997551937</id>
      <parentid>972222993</parentid>
      <timestamp>2021-01-01T02:36:18Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 10 templates: hyphenate params (17×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5934" xml:space="preserve">{{Infobox software
| name = Apache Sqoop
| logo = Apache Sqoop logo.svg
| released = {{Start date and age|2009|06|01|df=yes}} &lt;!-- https://blog.cloudera.com/blog/2009/06/introducing-sqoop/ --&gt;
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| latest release version = 1.4.7
| latest release date = {{Start date and age|2017|12|06}}
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;sqoop.git|Sqoop Repository}}
| programming language = [[Java (programming language)|Java]]
| genre = [[Data management]]
| license = [[Apache License 2.0]]
| website = {{URL|https://sqoop.apache.org}}
}}
'''Sqoop''' is a [[command-line interface]] application for transferring data between [[relational database]]s and [[Hadoop]].&lt;ref name="mainpage"&gt;{{cite web |url=https://sqoop.apache.org|title=Hadoop: Apache Sqoop|access-date=Sep 8, 2012}}&lt;/ref&gt;

==Description==
Sqoop supports incremental loads of a single table or a free form [[SQL query]] as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in [[Apache Hive|Hive]] or [[HBase]].&lt;ref&gt;{{cite web |url=https://blogs.apache.org/sqoop/entry/apache_sqoop_overview|title=Apache Sqoop - Overview|access-date=Sep 8, 2012}}&lt;/ref&gt; Exports can be used to put data from Hadoop into a relational database. Sqoop got the name from "SQL-to-Hadoop".&lt;ref&gt;{{cite web |url=https://blog.cloudera.com/blog/2009/06/introducing-sqoop/|title=Introducing Sqoop|access-date=Jan 1, 2019}}&lt;/ref&gt;
Sqoop became a top-level [[Apache Software Foundation|Apache]] project in March 2012.&lt;ref&gt;{{cite web |url=https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator|title=Apache Sqoop Graduates from Incubator|access-date=Sep 8, 2012}}&lt;/ref&gt;

[[Informatica]] provides a Sqoop-based [[Connector (computer science)|connector]] from version 10.1.
[[Pentaho]] provides [[open-source software|open-source]] Sqoop based connector steps, ''Sqoop Import''&lt;ref name="2015-12-10_PSI" /&gt; and ''Sqoop Export'',&lt;ref name="2015-12-10_PSE"/&gt; in their [[Extract, transform, load|ETL]] suite [[Pentaho Data Integration]] since version 4.5 of the software.&lt;ref name="2012-07-27_dbta" /&gt; [[Microsoft]] uses a Sqoop-based connector to help transfer data from [[Microsoft SQL Server]] databases to Hadoop.&lt;ref&gt;{{cite web |url=https://www.microsoft.com/en-us/download/details.aspx?id=27584|title=Microsoft SQL Server Connector for Apache Hadoop|access-date=Sep 8, 2012}}&lt;/ref&gt;
[[Couchbase, Inc.]] also provides a [[Couchbase Server]]-Hadoop connector by means of Sqoop.&lt;ref&gt;{{cite web|url=http://www.couchbase.com/develop/connectors/hadoop|title=Couchbase Hadoop Connector|access-date=Sep 8, 2012|url-status=dead|archive-url=https://web.archive.org/web/20120825184036/http://www.couchbase.com/develop/connectors/hadoop|archive-date=2012-08-25}}&lt;/ref&gt;

==See also==
*[[Apache Hadoop]]
*[[Apache Hive]]
*[[Apache Accumulo]]
*[[Apache HBase]]

==References==
{{Reflist|refs=

 &lt;ref name="2012-07-27_dbta"&gt;{{cite web
  | url = http://www.dbta.com/Editorial/News-Flashes/Big-Data-Analytics-Vendor-Pentaho-Announces-Tighter-Integration-with-Cloudera-Extends-Visual-Interface-to-Include-Hadoop-Sqoop-and-Oozie-84025.aspx
  | title = Big Data Analytics Vendor Pentaho Announces Tighter Integration with Cloudera; Extends Visual Interface to Include Hadoop Sqoop and Oozie
  | publisher = [[Database Trends and Applications]] (dbta.com)
  | date = 2012-07-27
  | access-date =  2015-12-08
  | archive-url = https://web.archive.org/web/20151208144234/http://www.dbta.com/Editorial/News-Flashes/Big-Data-Analytics-Vendor-Pentaho-Announces-Tighter-Integration-with-Cloudera-Extends-Visual-Interface-to-Include-Hadoop-Sqoop-and-Oozie-84025.aspx
  | archive-date = 2015-12-08
  | quote = Pentaho’s Business Analytics 4.5 is now certified on Cloudera’s latest releases, Cloudera Enterprise 4.0 and CDH4. Pentaho also announced that its visual design studio capabilities have been extended to the Sqoop and Oozie components of Hadoop.
 }}&lt;/ref&gt;

 &lt;ref name="2015-12-10_PSE"&gt;{{cite web
  | url = http://wiki.pentaho.com/display/EAI/Sqoop+Export
  | title = Sqoop Export
  | publisher = [[Pentaho]]
  | date = 2015-12-10
  | access-date =  2015-12-10
  | archive-url = https://web.archive.org/web/20151210171525/http://wiki.pentaho.com/display/EAI/Sqoop+Export
  | archive-date = 2015-12-10
  | quote = The Sqoop Export job allows you to export data from Hadoop into an RDBMS using Apache Sqoop.
 }}&lt;/ref&gt;

 &lt;ref name="2015-12-10_PSI"&gt;{{cite web
  | url = http://wiki.pentaho.com/display/EAI/Sqoop+Import
  | title = Sqoop Import
  | publisher = [[Pentaho]]
  | date = 2015-12-10
  | access-date =  2015-12-10
  | archive-url = https://web.archive.org/web/20151210170913/http://wiki.pentaho.com/display/EAI/Sqoop+Import
  | archive-date = 2015-12-10
  | quote = The Sqoop Import job allows you to import data from a relational database into the Hadoop Distributed File System (HDFS) using Apache Sqoop.
 }}&lt;/ref&gt;

}}

==Bibliography==
{{Refbegin}}
*{{Cite book |first1    = Tom
 |last1     = White
 |title     = Hadoop: The Definitive Guide
 |edition   = 2nd
 |chapter   = Chapter 15: Sqoop
 |publisher = [[O'Reilly Media]]
 |pages     = [https://archive.org/details/hadoopdefinitive0000whit/page/477 477–495]
 |isbn      = 978-1-449-38973-4
 |url       = https://archive.org/details/hadoopdefinitive0000whit/page/477
}}
{{Refend}}

==External links==
*{{Official website|https://sqoop.apache.org}}
*[https://cwiki.apache.org/confluence/display/SQOOP/Home Sqoop Wiki]
*[https://web.archive.org/web/20140202154003/http://qnalist.com/q/sqoop-user Sqoop Users Mailing List Archives]

{{Apache Software Foundation}}

[[Category:Apache Software Foundation projects]]
[[Category:Cloud applications]]
[[Category:Hadoop]]</text>
      <sha1>6ohetzk9pcp0gtsizc92lhewxdmb8wz</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Impala</title>
    <ns>0</ns>
    <id>40147148</id>
    <revision>
      <id>997177616</id>
      <parentid>983306459</parentid>
      <timestamp>2020-12-30T09:44:27Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 11 templates: hyphenate params (9×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6559" xml:space="preserve">{{Infobox software
| name = Apache Impala
| logo = [[File:Apache_Impala_Logo.svg|50px|Apache Impala Logo]] 
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| latest release version = 3.3.0
| latest release date = {{Start date and age|2019|08|22}}&lt;ref&gt;{{cite web|url=https://git-wip-us.apache.org/repos/asf?p=impala.git;a=tag;h=8cf3b8621942df1fe1609214296e9c26ddcfcd4f|title=3.3.0 release|access-date=23 August 2019}}&lt;/ref&gt; 
| latest preview version = 
| latest preview date = 
| programming language = [[C++]], [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| genre = Relational [[Hadoop]]-analytics
| license = [[Apache License 2.0]]
| website = {{URL|https://impala.apache.org}}
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;impala.git|Impala Repository}}
| released = {{Start date and age|2013|04|28}}
}}
'''Apache Impala''' is an [[open source software|open source]] [[massively parallel processing]] (MPP) SQL query engine for data stored in a [[computer cluster]] running [[Apache Hadoop]].&lt;ref name="Apache Impala"&gt;{{cite web|title=Apache Impala|url=http://impala.apache.org/|access-date=15 September 2017}}&lt;/ref&gt; Impala has been described as the open-source equivalent of [[Google F1]], which inspired its development in 2012.&lt;ref&gt;{{Cite news |url= https://www.wired.com/2012/10/cloudera-impala-hadoop/ |title= Man Busts Out of Google, Rebuilds Top-Secret Query Machine |author= Cade Metz |work= Wired Magazine |date= October 24, 2012 |access-date= October 10, 2016 }}&lt;/ref&gt;

==Description==
Apache Impala is a query engine that runs on Apache Hadoop. 
The project was announced in October 2012 with a public [[beta test]] distribution&lt;ref&gt;{{cite web |url= http://www.zdnet.com/cloudera-aims-to-bring-real-time-queries-to-hadoop-big-data-7000005951/ |title=Cloudera aims to bring real-time queries to Hadoop, big data |author= Larry Digna |date= October 24, 2012 |work= Between the lines blog |publisher= ZDNet |access-date= January 20, 2014 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.zdnet.com/clouderas-impala-brings-hadoop-to-sql-and-bi-7000006413/ |title=Cloudera’s Impala brings Hadoop to SQL and BI |author= Andrew Brust |date= October 25, 2012 |work= ZDNet |access-date= January 20, 2014 }}&lt;/ref&gt; and became generally available in May 2013.&lt;ref&gt;{{cite web |url= http://blog.cloudera.com/blog/2013/05/cloudera-impala-1-0-its-here-its-real-its-already-the-standard-for-sql-on-hadoop/ |title= Cloudera Impala 1.0: It’s Here, It’s Real, It’s Already the Standard for SQL on Hadoop |author= Marcel Kornacker, Justin Erickson |date= May 1, 2013 |access-date= April 10, 2014 |archive-url= https://web.archive.org/web/20140413155045/http://blog.cloudera.com/blog/2013/05/cloudera-impala-1-0-its-here-its-real-its-already-the-standard-for-sql-on-hadoop/ |archive-date= April 13, 2014 |url-status= dead }}&lt;/ref&gt;

Impala brings scalable parallel database technology to Hadoop, enabling users to issue low-latency [[SQL]] queries to data stored in [[HDFS]] and [[Apache HBase]] without requiring data movement or transformation. Impala is integrated with Hadoop to use the same file and data formats, metadata, security and resource management frameworks used by [[MapReduce]], [[Apache Hive]], [[Apache Pig]] and other Hadoop software.

Impala is promoted for analysts and data scientists to perform analytics on data stored in Hadoop via SQL or [[business intelligence]]  tools. The result is that large-scale data processing (via MapReduce) and interactive queries can be done on the same system using the same data and metadata – removing the need to migrate data sets into specialized systems and/or proprietary formats simply to perform analysis.

Features include:
*Supports [[HDFS#Hadoop distributed file system|HDFS]] and [[Apache HBase]] storage,
*Reads Hadoop file formats, including text, [[Lempel–Ziv–Oberhumer|LZO]], [[SequenceFile]], [[Apache Avro|Avro]], [[RCFile]], and [[Apache Parquet|Parquet]],
*Supports Hadoop security ([[Kerberos (protocol)|Kerberos authentication]]),
*Fine-grained, role-based authorization with [[Apache Sentry]],
*Uses metadata, [[ODBC]] driver, and SQL syntax from [[Apache Hive]].

In early 2013, a [[column-oriented DBMS|column-oriented file format]] called [[Apache Parquet|Parquet]] was announced for architectures including Impala.&lt;ref&gt;{{Cite web |title= Parquet: Columnar Storage for Hadoop |work= Project web site |year= 2013 |url= http://parquet.io/ |access-date= January 20, 2014 }}&lt;/ref&gt;
In December 2013, [[Amazon Web Services]] announced support for Impala.&lt;ref&gt;{{Cite web |title= Announcing Support for Impala with Amazon Elastic MapReduce |publisher= Amazon.com |date= December 12, 2013 |url= http://aws.amazon.com/about-aws/whats-new/2013/12/12/announcing-support-for-impala-with-amazon-elastic-mapreduce/ |access-date= January 20, 2014 }}&lt;/ref&gt;
In early 2014, [[MapR]] added support for Impala.&lt;ref&gt;{{Cite web |title= Impala for MapR |publisher= MapR.com |date= February 2, 2014 |url= http://doc.mapr.com/display/MapR/Impala+for+MapR |access-date= April 10, 2014 }}&lt;/ref&gt;
In 2015, another format called Kudu was announced, which [[Cloudera]] proposed to donate to the [[Apache Software Foundation]] along with Impala.&lt;ref&gt;{{Cite news |title= Cloudera to Donate Impala and Kudu Big Data Projects to Apache |date= November 18, 2015 |author= David Ramel |work= Application Development Trends |url= https://adtmag.com/articles/2015/11/18/cloudera-donates-projects.aspx |access-date= October 10, 2016 }}&lt;/ref&gt;
Impala graduated to an Apache Top-Level Project (TLP) on 28 November 2017.&lt;ref&gt;{{Cite web |title= The Apache Software Foundation Announces Apache® Impala™ as a Top-Level Project |url= https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces24 |date= November 28, 2017 |access-date= November 30, 2017 }}&lt;/ref&gt;

==See also==
*[[Apache Drill]] — similar open source project inspired by Dremel
*[[Dremel (software)|Dremel]] — similar tool from Google
*[[Presto (SQL query engine)|Presto]] — open source SQL query engine created by Facebook and supported by [[Teradata]]

==References==
{{Reflist}}

==External links==
*[https://impala.apache.org/index.html Apache Impala] project website
*[https://github.com/cloudera/impala Impala GitHub] project source code

{{Apache Software Foundation}}

{{DEFAULTSORT:Impala}}
[[Category:Apache Software Foundation projects|Impala]]
[[Category:Cloud platforms]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:2013 software]]</text>
      <sha1>egtbtk5f2gcqnq6y4v5k7z1nb9si9vv</sha1>
    </revision>
  </page>
  <page>
    <title>Hue (software)</title>
    <ns>0</ns>
    <id>40564013</id>
    <revision>
      <id>981913117</id>
      <parentid>979909720</parentid>
      <timestamp>2020-10-05T03:47:02Z</timestamp>
      <contributor>
        <username>Palosirkka</username>
        <id>12717894</id>
      </contributor>
      <comment>dupe image &amp; superfluous parenthesis removed</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1629" xml:space="preserve">{{multiple issues|
{{notability|Products|date=September 2013}}
{{more citations needed|date=September 2013}}
}}
{{Infobox software
| name                   = Cloudera Hue
| logo                   = [[File:Hue official logo.png|frameless|Hue Logo]]
| latest release version = 4.8.0
| latest release date    = {{Start date and age|2020|09|23}}
| programming language   = [[Python (programming language)|Python]]
| genre                  = Web platform
| license                = [[Apache License 2.0]]
| website                = {{URL|http://gethue.com/}}
| screenshot             = Hue 4.8 Editor.png
| screenshot alt         = Interface and the SQL Editor
| caption                = Interface and the SQL Editor
}}
'''Hue''' is an [[open-source software|open-source]] SQL Cloud Editor, licensed under the [[Apache License 2.0]].&lt;ref name="Hue license"&gt;{{cite web|url=https://github.com/cloudera/hue#license|title=Apache v2 License}}&lt;/ref&gt;

==Overview==
Hue is an open-source SQL Assistant for querying Databases &amp; Data Warehouses and collaborating. Its goal is to make self service data querying more widespread in organizations.

The Hue team provides releases on its website.&lt;ref name="Hue releases"&gt;{{cite web|url=http://gethue.com/category/release/|title=Hue releases}}&lt;/ref&gt; Hue is also present in the [[Cloudera]] Data Platform and the Hadoop services of the cloud providers [[Amazon AWS]], [[Google Cloud Platform]], and [[Microsoft Azure]].

==References==
{{Reflist|2}}

==External links==
* [http://gethue.com/ Hue - The open source SQL Assistant for Data Warehouses]

[[Category:Hadoop]]
[[Category:Big data products]]</text>
      <sha1>646jsp8nfu06memtj4hkpjj3b6ps4ba</sha1>
    </revision>
  </page>
  <page>
    <title>Quantcast File System</title>
    <ns>0</ns>
    <id>37854704</id>
    <revision>
      <id>987887605</id>
      <parentid>875140864</parentid>
      <timestamp>2020-11-09T20:46:51Z</timestamp>
      <contributor>
        <username>Sammi Brie</username>
        <id>575347</id>
      </contributor>
      <comment>Adding [[Wikipedia:Short description|short description]]: "File system" ([[Wikipedia:Shortdesc helper|Shortdesc helper]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3900" xml:space="preserve">{{short description|File system}}
{{For|the hierarchical storage manager filesystem from Oracle|QFS}}
{{Infobox software
| name                   = Quantcast File System (QFS)
| logo                   = 
| screenshot             =
| caption                =
| developer              = Sriram Rao, Michael Ovsiannikov, [[Quantcast]]
| latest release version = 1.1.4
| latest release date    = {{start date and age|2015|03|05}}&lt;ref&gt;[https://github.com/quantcast/qfs/tree/1.1.4 Release 1.1.4]&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| operating system       = 
| programming language   = [[C++]]
| genre                  = [[Distributed file system|Distributed File System]]
| license                = [[Apache License]] 2.0
| website                = {{URL|quantcast.github.com/qfs}}
}}

'''Quantcast File System''' ('''QFS''') is an open-source [[distributed file system]] software package for large-scale [[MapReduce]] or other batch-processing workloads. It was designed as an alternative to the [[Apache Hadoop]] Distributed File System ([[Apache Hadoop#HDFS|HDFS]]), intended to deliver better performance and cost-efficiency for large-scale processing clusters.

==Design==
QFS is software that runs on a cluster of hundreds or thousands of commodity [[Linux]] servers and allows other software layers to interact with them as if they were one giant hard drive. It has three components:
*A chunk server runs on each machine that will host data, manages I/O to its hard drives, and monitors its activity and capacity.
*A central process called the metaserver keeps the directory structure and maps of files to physical storage. It coordinates activities of all the chunk servers and monitors the overall health of the file system.  For high performance it holds all its data in memory, writing checkpoints and transaction logs to disk for recovery.
*A client component is the interface point that presents a file system [[application programming interface]] (API) to other layers of the software. It makes requests of the metaserver to identify which chunk servers hold (or will hold) its data, then interacts with the chunk servers directly to read and write.

In a cluster of hundreds or thousands of machines, the odds are low that all will be running and reachable at any given moment, so fault tolerance is the central design challenge. QFS meets it with [[Reed–Solomon error correction]]. The form of Reed–Solomon encoding used in QFS stores redundant data in nine places and can reconstruct the file from any six of these stripes.&lt;ref&gt;[http://strata.oreilly.com/2012/09/qfs-improves-performance-of-hadoop-file-system.html QFS improves performance of Hadoop file system - Strata]&lt;/ref&gt; When it writes a file, it by default stripes it across nine physically different machines — six holding the data, three holding parity information. Any three of those can become unavailable. If any six remain readable, QFS can reconstruct the original data. The result is fault tolerance at a cost of a 50% expansion of data. 

QFS is written in the programming language [[C++]], operates within a fixed memory footprint, and uses direct input and output (I/O).

==History==
QFS evolved from the Kosmos File System (KFS), an open source project started by [[Kosmix]] in 2005.  [[Quantcast]] adopted KFS in 2007, built its own improvements on it over the next several years, and released QFS 1.0 as an open source project in September, 2012.&lt;ref&gt;[http://gigaom.com/data/quantcast-releases-bigger-faster-stronger-hadoop-file-system/ Quantcast releases bigger, faster, stronger Hadoop file system — Tech News and Analysis]&lt;/ref&gt;

==References==
&lt;references /&gt;

==External links==
{{Portal|Free and open-source software}}
*{{Official website|quantcast.github.com/qfs}}

{{File systems}}

[[Category:Hadoop]]
[[Category:Free system software]]
[[Category:Distributed file systems]]</text>
      <sha1>akrr4j9jbs9oiwlpliyo4772o3ysygx</sha1>
    </revision>
  </page>
  <page>
    <title>GeoMesa</title>
    <ns>0</ns>
    <id>43326864</id>
    <revision>
      <id>808466494</id>
      <parentid>808374180</parentid>
      <timestamp>2017-11-03T01:13:51Z</timestamp>
      <contributor>
        <username>Mwtoews</username>
        <id>711150</id>
      </contributor>
      <minor/>
      <comment>use template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2961" xml:space="preserve">{{multiple issues|
{{notability|Products|date=July 2014}}
{{primary sources|date=July 2014}}
}}

{{Infobox software
| name                   = GeoMesa
| logo                   = 
| screenshot             =
| caption                =
| developer              = LocationTech, CCRi

| status                 = Active
| latest release version = 
| latest release date    = 
| operating system       = [[Linux]]
| programming language   = [[Scala (programming language)|Scala]]
| genre                  = [[Spatiotemporal database]]
| license                = [[Apache License]] 2.0
| website                = {{URL|geomesa.org}}
}}

'''GeoMesa''' is an open-source, distributed, spatio-temporal index built on top of [[Bigtable]]-style databases using an implementation of the [[Geohash]] algorithm.&lt;ref&gt;{{cite conference |url=https://geomesa.github.io/assets/outreach/SpatioTemporalIndexing_IEEEcopyright.pdf|first1=Anthony|last1=Fox|first2=Chris|last2=Eichelberger|first3=James|last3=Hughes|first4=Skylar|last4=Lyon|title=Spatio-temporal Indexing in Non-relational Distributed Databases |conference=IEEE BigData 2013 |year=2013}}&lt;/ref&gt; 

==Description==
Written in [[Scala (programming language)|Scala]], GeoMesa is capable of ingesting, indexing, and querying billions of geometry features using a highly parallelized index scheme. GeoMesa builds on top of open source geo (OSG) libraries. It implements the GeoTools DataStore interface providing standardized access to feature collections as well as implementing a GeoServer plugin.

Google announced that GeoMesa supported the [[Bigtable|Google Cloud Bigtable]]&lt;ref&gt;{{cite web |url=http://googlecloudplatform.blogspot.com/2015/05/introducing-Google-Cloud-Bigtable.html |title=Google Cloud Platform Blog: Announcing Google Bigtable |date=2015-05-06 |last1=O’Connor |first1=Cory |accessdate=2015-05-06}}&lt;/ref&gt; hosted NoSQL service in their release blog post in May 2015. GeoMesa also supports Bigtable-derivative implementations [[Apache Accumulo]] and  [[Apache HBase]].&lt;ref&gt;{{cite web |url=http://www.ccri.com/ |title=CCRi web site |publisher= Commonwealth Computer Research, Inc. |accessdate= September 24, 2016 }}&lt;/ref&gt;
GeoMesa implements a [[Z-order curve]] via a custom [[Geohash]] implementation to combine three dimensions of geometry and time (i.e. latitude/longitude/timestamp) into a single-dimension lexicographic key space provided by Accumulo.&lt;ref&gt;{{cite web |url=http://www.geomesa.org/documentation/tutorials/geohash-substrings.html#geohash-ranges |title=Geohash Ranges |publisher=Commonwealth Computer Research, Inc. |date=2016 |website=GeoMesa |accessdate=2017-01-10 }}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{Official website|http://geomesa.org}}
*[https://github.com/locationtech/geomesa GeoMesa Source]
*[http://www.locationtech.org/ LocationTech homepage]
*[https://github.com/ngageoint/geowave Geowave]

[[Category:Hadoop]]
[[Category:Free software programmed in Scala]]</text>
      <sha1>pscw1vlbzymotletr9nkn6sye54rlyz</sha1>
    </revision>
  </page>
  <page>
    <title>Deeplearning4j</title>
    <ns>0</ns>
    <id>43169442</id>
    <revision>
      <id>1000025056</id>
      <parentid>979513982</parentid>
      <timestamp>2021-01-13T04:13:41Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>Expanded [[Template:Notability]] and [[WP:AWB/GF|general fixes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14694" xml:space="preserve">{{cleanup-PR|1=article|date=November 2017}}
{{notability|Products|date=July 2020}}
{{Infobox software
| name                   = Eclipse Deeplearning4j
| logo                   = 
| screenshot             = 
| caption                =
| collapsible            =
| author                 = Alex D. Black, Adam Gibson, Vyacheslav Kokorin, Josh Patterson
| developer              = [https://github.com/eclipse/deeplearning4j/graphs/contributors Various]
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes}} --&gt;
| latest release version = 1.0.0-beta6
| latest release date    = {{Start date and age|2019|9|10|df=yes}}
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes}} --&gt;
| programming language   = [[Java (programming language)|Java]], [[CUDA]], [[C (programming language)|C]], [[C++]], 
| operating system       = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]], [[Android (operating system)|Android]], [[iOS]]
| platform               = [[Cross-platform]]
| size                   =
| language               = English
| status                 = Active
| genre                  = [[Natural language processing]], [[deep learning]], [[machine vision]], [[artificial intelligence]]
| license                = [[Apache License 2.0]]
}}

{{machine learning bar}}
Eclipse '''Deeplearning4j''' is the [[Comparison of deep-learning software|only]] [[deep learning]] programming [[Library (computing)|library]] written in [[Java (programming language)|Java]] for the [[Java virtual machine]] (JVM).&lt;ref name="wired"&gt;{{cite magazine|first=Cade|last=Metz|title=The Mission to Bring Google's AI to the Rest of the World|magazine=[[Wired.com]]|date=2014-06-02|url=https://www.wired.com/2014/06/skymind-deep-learning/|accessdate=2014-06-28}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.businessweek.com/articles/2014-06-03/teaching-smaller-companies-how-to-probe-deep-learning-on-their-own|title=Deep Learning for (Some of) the People|last=Vance|first=Ashlee|work=[[Bloomberg Businessweek]]|date=2014-06-03|accessdate=2014-06-28}}&lt;/ref&gt; It is a [[Software framework|framework]] with wide support for deep learning algorithms.&lt;ref&gt;{{cite web|url=https://venturebeat.com/2015/11/14/deep-learning-frameworks/|title=Want an open-source deep learning framework? Take your pick|last=Novet|first=Jordan|work=[[VentureBeat]]|date=2015-11-14|accessdate=2015-11-24}}&lt;/ref&gt; Deeplearning4j includes implementations of the [[restricted Boltzmann machine]], [[deep belief net]], deep autoencoder, stacked denoising autoencoder and [[Recursive neural network#Tensor|recursive neural tensor network]], [[word2vec]], doc2vec, and [[GloVe (machine learning)|GloVe]]. These algorithms all include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions that integrate with [[Apache Hadoop]] and [[Apache Spark|Spark]].&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=LCsc1hFuNac|title=Adam Gibson, DeepLearning4j on Spark and Data Science on JVM with nd4j, SF Spark @Galvanize 20150212 |last=TV|first=Functional|work=SF Spark Meetup|date=2015-02-12|accessdate=2015-03-01}}&lt;/ref&gt;

Deeplearning4j is [[open-source software]] released under [[Apache License]] 2.0,&lt;ref&gt;{{cite web|title=Github Repository|date=April 2020|url=https://github.com/agibsonccc/java-deeplearning}}&lt;/ref&gt; developed mainly by a [[machine learning]] group headquartered in [[San Francisco]].&lt;ref name="deeplearning4j.org"&gt;{{cite web|url=http://deeplearning4j.org/|title=deeplearning4j.org}}&lt;/ref&gt; It is supported commercially by the startup Skymind, which bundles DL4J, [[TensorFlow]], [[Keras]] and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer.&lt;ref&gt;{{cite web|title=Skymind Intelligence Layer Community Edition|url=https://skymind.ai/quickstart|access-date=2017-11-02|archive-url=https://web.archive.org/web/20171107015537/https://skymind.ai/quickstart|archive-date=2017-11-07|url-status=dead}}&lt;/ref&gt; Deeplearning4j was contributed to the [[Eclipse Foundation]] in October 2017.&lt;ref&gt;{{cite web|title=Eclipse Deeplearning4j Project Page|date=22 June 2017|url=https://projects.eclipse.org/proposals/deeplearning4j}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Skymind's Deeplearning4j, the Eclipse Foundation, and scientific computing in the JVM|url=https://jaxenter.com/skymind-deeplearning4j-eclipse-138872.html|work=Jaxenter|date=13 November 2017|accessdate=2017-11-15}}&lt;/ref&gt;

==Introduction==
Deeplearning4j relies on the widely used programming language [[Java (programming language)|Java]], though it is compatible with [[Clojure]] and includes a [[Scala (programming language)|Scala]] [[application programming interface]] (API). It is powered by its own open-source numerical computing library, [[ND4J (software)|ND4J]], and works with both [[central processing unit]]s (CPUs) and [[graphics processing unit]]s (GPUs).&lt;ref name="om"&gt;{{cite web|first=Derrick|last=Harris|title=A startup called Skymind launches, pushing open source deep learning|work=[[GigaOM.com]]|date=2014-06-02|url=http://gigaom.com/2014/06/02/a-startup-called-skymind-launches-pushing-open-source-deep-learning/|accessdate=2014-06-29}}&lt;/ref&gt;&lt;ref name="vb"&gt;{{cite web|first=Jordan|last=Novet|title=Skymind launches with open-source, plug-and-play deep learning features for your app|date=2014-06-02|url=https://venturebeat.com/2014/06/02/skymind-launches-with-open-source-plug-and-play-deep-learning-features-for-your-app//|accessdate=2014-06-29}}&lt;/ref&gt;

Deeplearning4j has been used in several commercial and academic applications. The code is hosted on [[GitHub]].&lt;ref&gt;[https://github.com/deeplearning4j/deeplearning4j Deeplearning4j source code]&lt;/ref&gt; A support forum is maintained on [[Gitter]].&lt;ref&gt;[https://gitter.im/deeplearning4j/deeplearning4j Deeplearning4j Gitter Support Channel]&lt;/ref&gt;

The framework is composable, meaning shallow neural nets such as restricted Boltzmann machines, convolutional nets, autoencoders, and recurrent nets can be added to one another to create deep nets of varying types. It also has extensive visualization tools,&lt;ref&gt;[http://deeplearning4j.org/visualization Deeplearning4j Visualization Tools]&lt;/ref&gt; and a computation graph.&lt;ref&gt;[http://deeplearning4j.org/compgraph Deeplearning4j Computation Graph]&lt;/ref&gt;

==Distributed==
Training with Deeplearning4j occurs in a cluster. Neural nets are trained in parallel via iterative reduce, which works on [[Hadoop]]-YARN and on [[Apache Spark|Spark]].&lt;ref name="deeplearning4j.org"/&gt;&lt;ref&gt;{{cite web|url=https://github.com/emsixteeen/IterativeReduce|title=Iterative reduce|date=15 March 2020}}&lt;/ref&gt; Deeplearning4j also integrates with CUDA kernels to conduct pure GPU operations, and works with distributed GPUs.

==Scientific computing for the JVM==
Deeplearning4j includes an n-dimensional array class using [[ND4J (software)|ND4J]] that allows scientific computing in Java and Scala, similar to the functions that [[NumPy]] provides to [[Python (programming language)|Python]]. It's effectively based on a library for [[linear algebra]] and [[Matrix (mathematics)|matrix]] manipulation in a production environment.

==DataVec vectorization library for machine-learning==
DataVec vectorizes various file formats and data types using an [[input/output]] format system similar to Hadoop's use of MapReduce; that is, it turns various data types into columns of scalars termed [[Vector (mathematics and physics)|vectors]]. DataVec is designed to vectorize CSVs, images, sound, text, video, and time series.&lt;ref&gt;[http://deeplearning4j.org/datavec DataVec ETL for Machine Learning]&lt;/ref&gt;&lt;ref&gt;[https://www.infoq.com/articles/deep-learning-time-series-anomaly-detection Anomaly Detection for Time Series Data with Deep Learning]&lt;/ref&gt;

==Text and NLP==
Deeplearning4j includes a [[vector space model]]ing and [[topic model]]ing toolkit, implemented in Java and integrating with parallel GPUs for performance. It is designed to handle large text sets.

Deeplearning4j includes implementations of term frequency–inverse document frequency ([[tf–idf]]), [[deep learning]], and Mikolov's word2vec algorithm,&lt;ref&gt;[https://code.google.com/p/word2vec/ word2vec]&lt;/ref&gt; doc2vec, and GloVe, reimplemented and optimized in Java. It relies on [[t-distributed stochastic neighbor embedding]] (t-SNE) for word-cloud visualizations.

==Real-world use cases and integrations==
Real-world use cases for Deeplearning4j include network intrusion detection and cybersecurity, fraud detection for the financial sector,&lt;ref&gt;{{Cite web |url=http://www.skymind.io/finance/ |title=Archived copy |access-date=2016-02-22 |archive-url=https://web.archive.org/web/20160310082208/http://www.skymind.io/finance/ |archive-date=2016-03-10 |url-status=dead }}&lt;/ref&gt;&lt;ref&gt;https://skymind.ai/bsa-aml{{Dead link|date=September 2018 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; anomaly detection in industries such as manufacturing, recommender systems in e-commerce and advertising,&lt;ref&gt;{{cite web |url=http://www.skymind.io/commerce/ |title=Archived copy |accessdate=2016-02-22 |url-status=dead |archiveurl=https://web.archive.org/web/20160310082156/http://www.skymind.io/commerce/ |archivedate=2016-03-10 }}&lt;/ref&gt; and image recognition.&lt;ref&gt;https://skymind.ai/image{{Dead link|date=September 2018 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; Deeplearning4j has integrated with other machine-learning platforms such as RapidMiner, Prediction.io,&lt;ref&gt;https://www.rapidminerchina.com/en/products/shop/product/deeplearning4j/{{Dead link|date=September 2018 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; and [[Weka (machine learning)|Weka]].&lt;ref&gt;https://deeplearning.cms.waikato.ac.nz/&lt;/ref&gt;

==Machine Learning Model Server==

Deeplearning4j serves machine-learning models for inference in production using the free developer edition of SKIL, the Skymind Intelligence Layer.&lt;ref&gt;{{Cite web |url=https://skymind.ai/products |title=Archived copy |access-date=2017-09-20 |archive-url=https://web.archive.org/web/20170921001159/https://skymind.ai/products |archive-date=2017-09-21 |url-status=dead }}&lt;/ref&gt;&lt;ref&gt;{{Cite web |url=https://deeplearning4j.org/modelserver |title=Archived copy |access-date=2017-09-20 |archive-url=https://web.archive.org/web/20170921001516/https://deeplearning4j.org/modelserver |archive-date=2017-09-21 |url-status=dead }}&lt;/ref&gt; A model server serves the parametric machine-learning models that makes decisions about data. It is used for the inference stage of a machine-learning workflow, after data pipelines and model training. A model server is the tool that allows data science research to be deployed in a real-world production environment.

What a Web server is to the Internet, a model server is to AI. Where a Web server receives an HTTP request and returns data about a Web site, a model server receives data, and returns a decision or prediction about that data: e.g. sent an image, a model server might return a label for that image, identifying faces or animals in photographs.

The SKIL model server is able to import models from Python frameworks such as Tensorflow, Keras, Theano and CNTK, overcoming a major barrier in deploying deep learning models.

==Benchmarks==
Deeplearning4j is as fast as Caffe for non-trivial image recognition tasks using multiple GPUs.&lt;ref&gt;{{Cite web | url=https://github.com/deeplearning4j/dl4j-benchmark | title=GitHub - deeplearning4j/Dl4j-benchmark: Repo to track dl4j benchmark code | date=19 December 2019}}&lt;/ref&gt; For programmers unfamiliar with HPC on the JVM, there are several parameters that must be adjusted to optimize neural network training time. These include setting the heap space, the garbage collection algorithm, employing off-heap memory and pre-saving data (pickling) for faster ETL.&lt;ref&gt;https://deeplearning4j.org/benchmark&lt;/ref&gt; Together, these optimizations can lead to a 10x acceleration in performance with Deeplearning4j.

==API Languages: Java, Scala, Python , Clojure &amp; Kotlin==
Deeplearning4j can be used via multiple API languages including Java, Scala, Python, Clojure and Kotlin. Its Scala API is called ScalNet.&lt;ref&gt;https://deeplearning4j.org/scala&lt;/ref&gt;  Keras serves as its Python API.&lt;ref&gt;{{Cite web |url=https://deeplearning4j.org/keras# |title=Archived copy |access-date=2017-02-25 |archive-url=https://web.archive.org/web/20170225133010/https://deeplearning4j.org/keras# |archive-date=2017-02-25 |url-status=dead }}&lt;/ref&gt;  And its Clojure wrapper is known as DL4CLJ.&lt;ref&gt;{{Cite web |url=https://deeplearning4j.org/clojure |title=Archived copy |access-date=2017-02-25 |archive-url=https://web.archive.org/web/20170225133007/https://deeplearning4j.org/clojure |archive-date=2017-02-25 |url-status=dead }}&lt;/ref&gt; The core languages performing the large-scale mathematical operations necessary for deep learning are C, C++ and CUDA C.

==Tensorflow, Keras &amp; Deeplearning4j==

Tensorflow, Keras and Deeplearning4j work together. Deeplearning4j can import models from Tensorflow and other Python frameworks if they have been created with Keras.&lt;ref&gt;{{Cite web |url=https://deeplearning4j.org/tensorflow |title=Archived copy |access-date=2017-09-07 |archive-url=https://web.archive.org/web/20170908021856/https://deeplearning4j.org/tensorflow |archive-date=2017-09-08 |url-status=dead }}&lt;/ref&gt;

==See also==
{{Portal|Free and open-source software|Computer programming}}
* [[Comparison of deep learning software]]
* [[Artificial intelligence]]
* [[Machine learning]]
* [[Deep learning]]

==References==
{{Reflist|30em}}

{{Computer vision footer}}
{{Deep Learning Software}}

[[Category:Applied machine learning]]
[[Category:Artificial neural networks]]
[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Deep learning]]
[[Category:Neural network software]]
[[Category:Free data analysis software]]
[[Category:Free science software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Java (programming language) software]]
[[Category:Free software programmed in Scala]]
[[Category:Free statistical software]]
[[Category:Hadoop]]
[[Category:Image processing]]
[[Category:Information technology companies of the United States]]
[[Category:Java (programming language) libraries]]
[[Category:Java platform]]
[[Category:Java programming language family]]
[[Category:JVM programming languages]]
[[Category:Machine learning]]
[[Category:Natural language processing]]
[[Category:Numerical programming languages]]
[[Category:Open-source artificial intelligence]]
[[Category:Scala (programming language)]]
[[Category:Software using the Apache license]]
[[Category:Technology companies based in the San Francisco Bay Area]]</text>
      <sha1>9hzaafi9ke9ngfg64zpaz1yea872oeu</sha1>
    </revision>
  </page>
  <page>
    <title>Gremlin (query language)</title>
    <ns>0</ns>
    <id>33800942</id>
    <revision>
      <id>964054298</id>
      <parentid>956277077</parentid>
      <timestamp>2020-06-23T09:49:10Z</timestamp>
      <contributor>
        <ip>155.145.195.133</ip>
      </contributor>
      <comment>Current Ontotext GraphDB versions don't support Gremlin / Tinkerpop - source: https://stackoverflow.com/questions/53045005/how-to-connect-gremlin-with-graphdb-ontotext</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14069" xml:space="preserve">{{Infobox programming language
| name = Gremlin
| logo = File:Gremlin (programming language).png|255px
| year = {{Start date and age|2009}}
| designer = Marko A. Rodriguez
| developer = Apache TinkerPop of the [[Apache Software Foundation]]
| written_in = [[Java (programming language)]]
| latest release version = Gremlin 3.3.3&lt;ref name="Gremlin Releases"&gt;{{cite web|url=http://tinkerpop.apache.org/docs/3.3.3/upgrade/#_tinkerpop_3_3_3|title=Gremlin 3.3.3|accessdate=May 8, 2018}}&lt;/ref&gt;
| dialects = Gremlin-Java8, Gremlin-Groovy, Gremlin-Python, Gremlin-Scala, Gremlin-Clojure, Gremlin-PHP, Gremlin-JavaScript, Gremlin-Typeset
| operating_system = [[Cross-platform]] (multi-platform)
| license = [[Apache License 2.0]]
| website =  [http://tinkerpop.apache.org/ Official Site]
| status = Active
| influenced_by = [[Regular expression]], [[XPath]], Ripple, [[SPARQL]], [[SQL]], [[Java (programming language)|Java]]/[[Java virtual machine|JVM]]
}}

'''Gremlin''' is a [[graph traversal]] language and [[virtual machine]] developed by Apache TinkerPop of the [[Apache Software Foundation]]. Gremlin works for both [[Online transaction processing|OLTP]]-based graph databases as well as [[Online analytical processing|OLAP]]-based graph processors. Gremlin's [[Automata theory|automata]] and [[Functional programming|functional language]] foundation enable Gremlin to naturally support [[Imperative programming|imperative]] and [[Declarative programming|declarative]] querying, host language agnosticism, user-defined [[Domain-specific language|domain specific languages]], an extensible compiler/optimizer, single- and multi-machine execution models, hybrid depth- and breadth-first evaluation, as well as [[Turing Complete]]ness.&lt;ref name="Gremlin Machine and Language"&gt;{{cite book|arxiv=1508.03843|title=The Gremlin Graph Traversal Machine and Language|pages=1–10|last1= Rodriguez|first1=Marko A.|year=2015|doi=10.1145/2815072.2815073|chapter=The Gremlin graph traversal machine and language (invited talk)|isbn=9781450339025}}&lt;/ref&gt;

As an explanatory analogy, Apache TinkerPop and Gremlin are to [[graph databases]] what the [[Java Database Connectivity|JDBC]] and [[SQL]] are to [[RDBMS|relational databases]]. Likewise, the Gremlin traversal machine is to graph computing as what the [[Java virtual machine]] is to general purpose computing.&lt;ref name="The Benefits of the Gremlin Graph Traversal Machine"&gt;{{cite web|url=http://www.datastax.com/dev/blog/the-benefits-of-the-gremlin-graph-traversal-machine|title=The Benefits of the Gremlin Graph Traversal Machine|accessdate=September 17, 2015|date=2015-09-14}}&lt;/ref&gt;

== History ==
* 2009-10-30 the project is born, and immediately named "TinkerPop"
* 2009-12-25 v0.1 is the first release
* 2011-05-21 v1.0 is released
* 2012-05-24 v2.0 is released
* 2015-01-16 TinkerPop becomes an Apache Incubator project
* 2015-07-09 v3.0.0-incubating is released
* 2016-05-23 Apache TinkerPop becomes a top-level project
* 2016-07-18 v3.1.3 and v3.2.1 are first releases as Apache TinkerPop
* 2017-12-17 v3.3.1 is released
* 2018-05-08 v3.3.3 is released
* 2019-08-05 v3.4.3 is released
* 2020-02-20 v3.4.6 is released

== Vendor integration ==

Gremlin is an [[Apache License|Apache2-licensed]] graph traversal language that can be used by graph system vendors. There are typically two types of graph system vendors: OLTP [[graph databases]] and OLAP graph processors. The table below outlines those graph vendors that support Gremlin.

{| class="wikitable"
|-
! Vendor
! Graph System
|-
| [[Neo4j]]
| graph database
|-
| [[OrientDB]]
| graph database
|-
| [[DataStax]] Enterprise (5.0+)
| graph database
|-
| [[Apache Hadoop|Hadoop]] ([[Apache Giraph|Giraph]])
| graph processor
|-
| [[Apache Hadoop|Hadoop]] ([[Apache Spark|Spark]])
| graph processor
|-
| [[InfiniteGraph]]
| graph database
|-
| [[JanusGraph]]
| graph database
|-
| [[Cosmos DB]]
| graph database
|-
| [[Amazon Neptune]]
| graph database
|}

== Traversal examples ==
The following examples of Gremlin queries and responses in a Gremlin-Groovy environment are relative to a graph representation of the [http://grouplens.org/datasets/movielens/ MovieLens] dataset.&lt;ref name="NoSQLNow Slides"&gt;{{cite web|url=http://www.slideshare.net/slidarko/the-gremlin-traversal-language|title=The Gremlin Graph Traversal Language|accessdate=August 22, 2015|date=2015-08-19}}&lt;/ref&gt; The dataset includes users who rate movies. Users each have one occupation, and each movie has one or more categories associated with it. The MovieLens graph schema is detailed below.
&lt;syntaxhighlight lang="cypher"&gt;
user--rated[stars:0-5]--&gt;movie
user--occupation--&gt;occupation
movie--category--&gt;category
&lt;/syntaxhighlight&gt;

=== Simple traversals ===

{{pull quote|For each vertex in the graph, emit its label, then group and count each distinct label.}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g.V().label().groupCount()
==&gt;[occupation:21, movie:3883, category:18, user:6040]
&lt;/syntaxhighlight&gt;

{{pull quote|What year was the oldest movie made?}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g.V().hasLabel('movie').values('year').min()
==&gt;1919
&lt;/syntaxhighlight&gt;

{{pull quote|What is Die Hard's average rating?}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g.V().has('movie','name','Die Hard').inE('rated').values('stars').mean()
==&gt;4.121848739495798
&lt;/syntaxhighlight&gt;

=== Projection traversals ===

{{pull quote|For each category, emit a map of its name and the number of movies it represents.}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g.V().hasLabel('category').as('a','b').
           select('a','b').
             by('name').
             by(inE('category').count())
==&gt;[a:Animation, b:105]
==&gt;[a:Children's, b:251]
==&gt;[a:Comedy, b:1200]
==&gt;[a:Adventure, b:283]
==&gt;[a:Fantasy, b:68]
==&gt;[a:Romance, b:471]
==&gt;[a:Drama, b:1603]
==&gt;[a:Action, b:503]
==&gt;[a:Crime, b:211]
==&gt;[a:Thriller, b:492]
==&gt;[a:Horror, b:343]
==&gt;[a:Sci-Fi, b:276]
==&gt;[a:Documentary, b:127]
==&gt;[a:War, b:143]
==&gt;[a:Musical, b:114]
==&gt;[a:Mystery, b:106]
==&gt;[a:Film-Noir, b:44]
==&gt;[a:Western, b:68]
&lt;/syntaxhighlight&gt;

{{pull quote|For each movie with at least 11 ratings, emit a map of its name and average rating. Sort the maps in decreasing order by their average rating. Emit the first 10 maps (i.e. top 10).}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g.V().hasLabel('movie').as('a','b').
           where(inE('rated').count().is(gt(10))).
           select('a','b').
             by('name').
             by(inE('rated').values('stars').mean()).
           order().by(select('b'),decr).
           limit(10)
==&gt;[a:Sanjuro, b:4.608695652173913]
==&gt;[a:Seven Samurai (The Magnificent Seven), b:4.560509554140127]
==&gt;[a:Shawshank Redemption, The, b:4.554557700942973]
==&gt;[a:Godfather, The, b:4.524966261808367]
==&gt;[a:Close Shave, A, b:4.52054794520548]
==&gt;[a:Usual Suspects, The, b:4.517106001121705]
==&gt;[a:Schindler's List, b:4.510416666666667]
==&gt;[a:Wrong Trousers, The, b:4.507936507936508]
==&gt;[a:Sunset Blvd. (a.k.a. Sunset Boulevard), b:4.491489361702127]
==&gt;[a:Raiders of the Lost Ark, b:4.47772]
&lt;/syntaxhighlight&gt;

=== Declarative pattern matching traversals ===

Gremlin supports declarative graph pattern matching similar to [[SPARQL]]. For instance, the following query below uses Gremlin's ''match()''-step.

{{pull quote|What 80's action movies do 30-something programmers like? Group count the movies by their name and sort the group count map in decreasing order by value.  Clip the map to the top 10 and emit the map entries.}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g.V().
           match(
             __.as('a').hasLabel('movie'),
             __.as('a').out('category').has('name','Action'),
             __.as('a').has('year',between(1980,1990)),
             __.as('a').inE('rated').as('b'),
             __.as('b').has('stars',5),
             __.as('b').outV().as('c'),
             __.as('c').out('occupation').has('name','programmer'),
             __.as('c').has('age',between(30,40))).
           select('a').groupCount().by('name').
           order(local).by(valueDecr).
           limit(local,10)
==&gt;Raiders of the Lost Ark=26
==&gt;Star Wars Episode V - The Empire Strikes Back=26
==&gt;Terminator, The=23
==&gt;Star Wars Episode VI - Return of the Jedi=22
==&gt;Princess Bride, The=19
==&gt;Aliens=18
==&gt;Boat, The (Das Boot)=11
==&gt;Indiana Jones and the Last Crusade=11
==&gt;Star Trek The Wrath of Khan=10
==&gt;Abyss, The=9
&lt;/syntaxhighlight&gt;

=== OLAP traversal ===

{{pull quote|Which movies are most central in the ''implicit'' 5-stars graph?}}

&lt;syntaxhighlight lang="groovy"&gt;
gremlin&gt; g = graph.traversal(computer(SparkGraphComputer))
==&gt;graphtraversalsource[hadoopgraph[gryoinputformat-&gt;gryooutputformat], sparkgraphcomputer]
gremlin&gt; g.V().repeat(outE('rated').has('stars', 5).inV().
                 groupCount('m').by('name').
                 inE('rated').has('stars', 5).outV()).
               times(4).cap('m')
==&gt;Star Wars Episode IV - A New Hope	  35405394353105332
==&gt;American Beauty	  31943228282020585
==&gt;Raiders of the Lost Ark	31224779793238499
==&gt;Star Wars Episode V - The Empire Strikes Back  30434677119726223
==&gt;Godfather, The	30258518523013057
==&gt;Shawshank Redemption, The	28297717387901031
==&gt;Schindler's List	27539336654199309
==&gt;Silence of the Lambs, The	26736276376806173
==&gt;Fargo	 26531050311325270
==&gt;Matrix, The	 26395118239203191
&lt;/syntaxhighlight&gt;

== Gremlin graph traversal machine ==

Gremlin is a [[virtual machine]] composed of an [[instruction set]] as well as an execution engine. An analogy is drawn between Gremlin and [[Java (programming language)|Java]].

{| class="wikitable"
|-
! Java Ecosystem
! Gremlin Ecosystem
|-
| [[Groovy (programming language)|Apache Groovy programming language]]
| Gremlin-Groovy
|-
| [[Scala (programming language)|Scala programming language]]
| Gremlin-Scala
|-
| [[Clojure (programming language)|Clojure programming language]]
| Gremlin-Clojure
|-
| ...
| ...
|-
| [[Java (programming language)|Java programming language]]
| Gremlin-Java8
|-
| Java instruction set
| Gremlin step library
|-
| [[Java virtual machine]]
| Gremlin traversal machine
|}

=== Gremlin steps (instruction set) ===

The following traversal is a Gremlin traversal in the Gremlin-Java8 dialect.

&lt;syntaxhighlight lang="java"&gt;
g.V().as("a").out("knows").as("b").
  select("a","b").
    by("name").
    by("age")
&lt;/syntaxhighlight&gt;
The Gremlin language (i.e. the [[Fluent interface|fluent-style]] of expressing a graph traversal) can be represented in any host language that supports [[function composition]] and [[Nested function|function nesting]]. Due to this simple requirement, there exists various Gremlin dialects including Gremlin-Groovy, Gremlin-Scala, Gremlin-Clojure, etc. The above Gremlin-Java8 traversal is ultimately compiled down to a step sequence called a ''traversal''. A string representation of the traversal above provided below.
&lt;syntaxhighlight lang="java"&gt;
[GraphStep([],vertex)@[a], VertexStep(OUT,[knows],vertex)@[b], SelectStep([a, b],[value(name), value(age)])]
&lt;/syntaxhighlight&gt;
The ''steps'' are the primitives of the Gremlin graph traversal machine. They are the parameterized instructions that the machine ultimately executes. The Gremlin [[instruction set]] is approximately 30 steps. These steps are sufficient to provide general purpose computing and what is typically required to express the common motifs of any graph traversal query.

Given that Gremlin is a language, an instruction set, and a virtual machine, it is possible to design another traversal language that compiles to the Gremlin traversal machine (analogous to how Scala compiles to the [[Java virtual machine|JVM]]). For instance, the popular [[SPARQL]] graph pattern match language can be compiled to execute on the Gremlin machine. The following SPARQL query
&lt;syntaxhighlight lang="sparql"&gt;
SELECT ?a ?b ?c
WHERE {
  ?a a Person .
  ?a ex:knows ?b .
  ?a ex:created ?c .
  ?b ex:created ?c .
  ?b ex:age ? d .
    FILTER(?d &lt; 30)
}
&lt;/syntaxhighlight&gt;
would compile to
&lt;syntaxhighlight lang="java"&gt;
[GraphStep([],vertex), MatchStep(AND,[[MatchStartStep(a), LabelStep, IsStep(eq(Person)), MatchEndStep], [MatchStartStep(a), VertexStep(OUT,[knows],vertex), MatchEndStep(b)], [MatchStartStep(a), VertexStep(OUT,[created],vertex), MatchEndStep(c)], [MatchStartStep(b), VertexStep(OUT,[created],vertex), MatchEndStep(c)], [MatchStartStep(b), PropertiesStep([age],value), MatchEndStep(d)], [MatchStartStep(d), IsStep(gt(30)), MatchEndStep]]), SelectStep([a, b, c])].
&lt;/syntaxhighlight&gt;
In Gremlin-Java8, the SPARQL query above would be represented as below and compile to the identical Gremlin step sequence (i.e. traversal).
&lt;syntaxhighlight lang="java"&gt;
g.V().match(
  as("a").label().is("person"),
  as("a").out("knows").as("b"),
  as("a").out("created").as("c"),
  as("b").out("created").as("c"),
  as("b").values("age").as("d"),
  as("d").is(gt(30))).
    select("a","b","c")
&lt;/syntaxhighlight&gt;

=== Gremlin Machine (virtual machine) ===

The Gremlin graph traversal machine can execute on a single machine or across a multi-machine compute cluster. Execution agnosticism allows Gremlin to run over both [[graph databases]] (OLTP) and graph processors (OLAP).

== See also ==

* [[Cypher Query Language]], another query language on graph data
* [[SPARQL]], another query language on graph data

== References ==
{{reflist}}

== External links ==
# [http://tinkerpop.apache.org/ Apache TinkerPop Homepage]
# [http://sql2gremlin.com/ sql2gremlin.com (TinkerPop2)]
# Rodriguez, M.A., "[https://arxiv.org/abs/1508.03843 The Gremlin Graph Traversal Machine and Language]," Proceedings of the ACM Database Programming Languages Conference, October, 2015.

{{Apache Software Foundation}}

{{Query languages}}

[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Free software programmed in Scala]]
[[Category:Declarative programming languages]]
[[Category:Query languages]]</text>
      <sha1>kh6hh753w9bisrjuuvahdz77jttgonf</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Hadoop</title>
    <ns>0</ns>
    <id>5919308</id>
    <revision>
      <id>999885262</id>
      <parentid>999839039</parentid>
      <timestamp>2021-01-12T12:45:08Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <comment>source states Fortune 50 (it is from 2012, however, so things may have changed, but it requires a new source)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="46700" xml:space="preserve">{{short description|Distributed data processing framework}}
{{Infobox software
| name = Apache Hadoop
| logo = Hadoop logo new.svg
| screenshot = 
| caption = 
| author = [[Doug Cutting]], [[Mike Cafarella]]
| developer = [[Apache Software Foundation]]
| released = {{Start date and age|2006|04|01}}&lt;ref&gt;{{cite web |url=https://archive.apache.org/dist/hadoop/common/ |title=Hadoop Releases &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |access-date=2019-04-28}}&lt;/ref&gt;
| latest release version = {{Multiple releases
  |branch1 = 2.7.x
  |version1 = 2.7.7
  |date1 = {{Start date and age|df=yes|2018|05|31}}&lt;ref name="releases"&gt;{{cite web|url=https://hadoop.apache.org/releases.html|title=Apache Hadoop|access-date=7 September 2019}}&lt;/ref&gt;
  |branch2 = 2.8.x
  |version2 = 2.8.5
  |date2 = {{Start date and age|df=yes|2018|09|15}}&lt;ref name="releases"&gt;&lt;/ref&gt;
  |branch3 = 2.9.x
  |version3 = 2.9.2
  |date3 = {{Start date and age|df=yes|2018|11|09}}&lt;ref name="releases"&gt;&lt;/ref&gt;
  |branch4 = 2.10.x
  |version4 = 2.10.1
  |date4 = {{Start date and age|df=yes|2020|09|21}}&lt;ref name="releases"&gt;&lt;/ref&gt;
  |branch5 = 3.1.x
  |version5 = 3.1.4
  |date5 = {{Start date and age|df=yes|2020|08|3}}&lt;ref name="releases"&gt;&lt;/ref&gt;
  |branch6 = 3.2.x
  |version6 = 3.2.1
  |date6 = {{Start date and age|df=yes|2019|09|22}}&lt;ref name="releases"&gt;&lt;/ref&gt;
  |branch7 = 3.3.x
  |version7 = 3.3.0
  |date7 = {{Start date and age|df=yes|2020|07|14}}&lt;ref name="releases"&gt;&lt;/ref&gt;
  }}
| latest release date = &lt;!--{{release date|2019|01|16}}&lt;ref name="homepage" /&gt;--&gt;
| operating system = [[Cross-platform]]
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;hadoop.git|Hadoop Repository}}
| programming language = [[Java (programming language)|Java]]
| genre = [[Distributed file system]]
&lt;!--| posix compliant        = Not [[POSIX]]-compliant--&gt;| license = [[Apache License 2.0]]
| website = {{Official URL}}
}}
'''Apache Hadoop''' ({{IPAc-en|pron|h|ə|ˈ|d|u:|p}}) is a collection of [[Open-source software|open-source]] software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It provides a [[software framework]] for [[Clustered file system|distributed storage]] and processing of [[big data]] using the [[MapReduce]] [[programming model]]. Hadoop was originally designed for [[computer cluster]]s built from [[commodity hardware]], which is still the common use.&lt;ref&gt;{{cite web |url=http://www.silicon.co.uk/workspace/doug-cutting-big-data-is-not-a-bubble-96694 |title=Doug Cutting: Big Data Is No Bubble |last=Judge |first=Peter |date=2012-10-22 |website=silicon.co.uk |access-date=2018-03-11}}&lt;/ref&gt;  It has since also found use on clusters of higher-end hardware.&lt;ref&gt;{{cite web |url=https://www.datanami.com/2014/05/12/hadoop-ibm-power/ |title=Why Hadoop on IBM Power |last=Woodie |first=Alex |date=2014-05-12 |website=datanami.com |publisher=Datanami |access-date=2018-03-11}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.hpcwire.com/2014/10/15/cray-launches-hadoop-hpc-airspace/ |title=Cray Launches Hadoop into HPC Airspace |last=Hemsoth |first=Nicole |date=2014-10-15 |website=hpcwire.com |access-date=2018-03-11}}&lt;/ref&gt; All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.&lt;ref name="homepage"&gt;{{Cite web|title= Welcome to Apache Hadoop!|url= http://hadoop.apache.org|website= hadoop.apache.org|access-date = 2016-08-25}}&lt;/ref&gt;

The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers [[JAR (file format)|packaged code]] into nodes to process the data in parallel. This approach takes advantage of [[data locality]],&lt;ref&gt;{{cite web |url=http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ |title=What is the Hadoop Distributed File System (HDFS)? &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=ibm.com |publisher=[[IBM]] |access-date=2014-10-30 }}&lt;/ref&gt; where nodes manipulate the data they have access to. This allows the dataset to be [[distributed processing|processed]] faster and more efficiently than it would be in a more conventional [[supercomputer architecture]] that relies on a [[parallel file system]] where computation and data are distributed via high-speed networking.&lt;ref&gt;{{cite web |url=http://www.datascienceassn.org/content/data-locality-hpc-vs-hadoop-vs-spark |title=Data Locality: HPC vs. Hadoop vs. Spark |last1=Malak |first1=Michael |date=2014-09-19 |website=datascienceassn.org |publisher=Data Science Association |access-date=2014-10-30 }}&lt;/ref&gt;&lt;ref&gt;{{cite book|chapter=Characterization and Optimization of Memory-Resident MapReduce on HPC Systems|publisher=IEEE|date=October 2014|doi=10.1109/IPDPS.2014.87|title=2014 IEEE 28th International Parallel and Distributed Processing Symposium|pages=799–808|last1=Wang|first1=Yandong|last2=Goldstone|first2=Robin|last3=Yu|first3=Weikuan|last4=Wang|first4=Teng|isbn=978-1-4799-3800-1}}&lt;/ref&gt;

The base Apache Hadoop framework is composed of the following modules:

* ''Hadoop Common'' – contains libraries and utilities needed by other Hadoop modules;
* ''Hadoop Distributed File System (HDFS)'' – a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster;
* ''Hadoop YARN'' – (introduced in 2012) a platform responsible for managing computing resources in clusters and using them for scheduling users' applications;&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/yarn/api/records/Resource.html#newInstance(int,%20int) |title=Resource (Apache Hadoop Main 2.5.1 API) &lt;!-- |author=Staff writer(s); no by-line.--&gt; |date=2014-09-12 |website=apache.org |publisher=Apache Software Foundation |access-date=2014-09-30 |archive-url=https://web.archive.org/web/20141006090717/http://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/yarn/api/records/Resource.html#newInstance(int,%20int) |archive-date=2014-10-06 |url-status=dead }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/ |title=Apache Hadoop YARN – Concepts and Applications |last1=Murthy |first1=Arun |date=2012-08-15 |website=hortonworks.com |publisher=Hortonworks |access-date=2014-09-30 }}&lt;/ref&gt;
* ''Hadoop MapReduce'' – an implementation of the MapReduce programming model for large-scale data processing.
* ''Hadoop Ozone'' – (introduced in 2020) An object store for Hadoop

The term ''Hadoop'' is often used for both base modules and sub-modules and also the ''ecosystem'',&lt;ref&gt;{{cite web |url=https://finance.yahoo.com/news/continuuity-raises-10-million-series-120500471.html |title=Continuuity Raises $10 Million Series A Round to Ignite Big Data Application Development Within the Hadoop Ecosystem &lt;!-- |author=Staff writer(s); no by-line.--&gt; |date=2012-11-14 |website=finance.yahoo.com |publisher=[[Marketwired]] |access-date=2014-10-30 }}&lt;/ref&gt; or collection of additional software packages that can be installed on top of or alongside Hadoop, such as [[Pig (programming tool)|Apache Pig]], [[Apache Hive]], [[Apache HBase]], [[Apache Phoenix]], [[Apache Spark]], [[Apache ZooKeeper]], [[Cloudera Impala]], [[Apache Flume]], [[Apache Sqoop]], [[Apache Oozie]], and [[Apache Storm]].&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/ |title=Hadoop-related projects at |publisher=Hadoop.apache.org |access-date=2013-10-17 }}&lt;/ref&gt;

Apache Hadoop's MapReduce and HDFS components were inspired by [[Google]] papers on [[MapReduce]] and [[Google File System]].&lt;ref&gt;{{cite book &lt;!-- |author=Staff writer(s); no by-line.--&gt; |title=Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data |url=https://books.google.com/books?id=axruBQAAQBAJ&amp;pg=PA300|publisher=John Wiley &amp; Sons |page=300 |date=2014-12-19 |isbn=9781118876220 |access-date=2015-01-29 }}&lt;/ref&gt;

The Hadoop framework itself is mostly written in the [[Java (programming language)|Java programming language]], with some native code in [[C (programming language)|C]] and [[Command-line interface|command line]] utilities written as [[shell scripts]]. Though MapReduce Java code is common, any programming language can be used with Hadoop Streaming to implement the map and reduce parts of the user's program.&lt;ref&gt;{{cite web |url=http://www.mail-archive.com/nlpatumd@yahoogroups.com/msg00570.html |title=[nlpatumd&amp;#93; Adventures with Hadoop and Perl |publisher=Mail-archive.com |date=2010-05-02 |access-date=2013-04-05 }}&lt;/ref&gt; Other projects in the Hadoop ecosystem expose richer user interfaces.

==History==
According to its co-founders, [[Doug Cutting]] and [[Mike Cafarella]], the genesis of Hadoop was the Google File System paper that was published in October 2003.&lt;ref&gt;{{Cite news|url=https://www.oreilly.com/ideas/the-next-10-years-of-apache-hadoop|title=The next 10 years of Apache Hadoop|last1=Cutting|first1=Mike|last2=Cafarella|first2=Ben|last3=Lorica|first3=Doug|date=2016-03-31|work=O'Reilly Media|access-date=2017-10-12|language=en}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://research.google.com/archive/gfs.html|title=The Google File System|first1=Sanjay|last1=Ghemawat|first2=Howard|last2=Gobioff|first3=Shun-Tak|last3=Leung}}&lt;/ref&gt; This paper spawned another one from Google{{snd}} "MapReduce: Simplified Data Processing on Large Clusters".&lt;ref&gt;{{cite web|url=http://research.google.com/archive/mapreduce.html|title=MapReduce: Simplified Data Processing on Large Clusters|first1=Jeffrey|last1=Dean|first2=Sanjay|last2=Ghemawat|year=2004}}&lt;/ref&gt; Development started on the [[Apache Nutch]] project, but was moved to the new Hadoop subproject in January 2006.&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/browse/INFRA-700|title=new mailing lists request: hadoop|last=Cutting|first=Doug|date=28 Jan 2006|website=issues.apache.org|quote=The Lucene PMC has voted to split part of Nutch into a new sub-project named Hadoop}}&lt;/ref&gt; Doug Cutting, who was working at [[Yahoo!]] at the time, named it after his son's toy elephant.&lt;ref&gt;{{cite news |title=Hadoop, a Free Software Program, Finds Uses Beyond Search |first=Ashlee |last=Vance |newspaper=The New York Times |date=2009-03-17 |url=https://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html |access-date=2010-01-20 | archive-url= https://web.archive.org/web/20110830130350/http://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html|archive-date=30 August 2011| url-status=live }}&lt;/ref&gt; The initial code that was factored out of Nutch consisted of about 5,000 lines of code for HDFS and about 6,000 lines of code for MapReduce.

In March 2006, Owen O’Malley was the first committer to add to the Hadoop project;&lt;ref&gt;{{cite mailing list|first=Doug|last=Cutting|author-link=Doug Cutting|title=[RESULT] VOTE: add Owen O'Malley as Hadoop committer|mailing-list=hadoop-common-dev|date=30 March 2006|url=http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/200603.mbox/%3C442B27A6.8080500@apache.org%3E}}&lt;/ref&gt; Hadoop 0.1.0 was released in April 2006.&lt;ref&gt;{{cite web|url=https://archive.apache.org/dist/hadoop/core/|title=Index of /dist/hadoop/core|website=archive.apache.org|access-date=11 December 2017}}&lt;/ref&gt; It continues to evolve through contributions that are being made to the project.&lt;ref&gt;{{cite web|url=https://hadoop.apache.org/who.html|title=Who We Are|website=hadoop.apache.org|access-date=11 December 2017}}&lt;/ref&gt; The very first design document for the Hadoop Distributed File System was written by Dhruba Borthakur in 2007.&lt;ref&gt;{{Cite web|last=Borthakur|first=Dhruba|date=2006|title=The Hadoop Distributed File System: Architecture and Design|url=http://svn.apache.org/repos/asf/hadoop/common/tags/release-0.10.0/docs/hdfs_design.pdf|website=Apache Hadoop Code Repository}}&lt;/ref&gt;

==Architecture==
{{See also|#Hadoop_distributed_file_system|Apache HBase|MapReduce|l1=Hadoop Distributed File System}}

Hadoop consists of the ''Hadoop Common'' package, which provides file system and operating system level abstractions, a MapReduce engine (either MapReduce/MR1 or YARN/MR2)&lt;ref&gt;{{cite web |url=http://blog.cloudera.com/blog/2012/10/mr2-and-yarn-briefly-explained/ |title=MR2 and YARN Briefly Explained |first=Harsh |last=Chouraria |date=21 October 2012 |website=Cloudera.com |access-date=23 October 2013 |archive-url=https://web.archive.org/web/20131022080058/http://blog.cloudera.com/blog/2012/10/mr2-and-yarn-briefly-explained/ |archive-date=22 October 2013 |url-status=dead }}&lt;/ref&gt; and the [[#Hadoop distributed file system|Hadoop Distributed File System]] (HDFS). The Hadoop Common package contains the [[JAR (file format)|Java Archive (JAR)]] files and scripts needed to start Hadoop.

For effective scheduling of work, every Hadoop-compatible file system should provide location awareness, which is the name of the rack, specifically the network switch where a worker node is. Hadoop applications can use this information to execute code on the node where the data is, and, failing that, on the same rack/switch to reduce backbone traffic. HDFS uses this method when replicating data for data redundancy across multiple racks. This approach reduces the impact of a rack power outage or switch failure; if any of these hardware failures occurs, the data will remain available.&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html |title=HDFS User Guide |publisher=Hadoop.apache.org |access-date=2014-09-04 }}&lt;/ref&gt;
[[File:Hadoop 1.png|thumb|upright=1.2|right|alt=Hadoop cluster|A multi-node Hadoop cluster]]

A small Hadoop cluster includes a single master and multiple worker nodes. The master node consists of a Job Tracker, Task Tracker, NameNode, and DataNode. A slave or ''worker node'' acts as both a DataNode and TaskTracker, though it is possible to have data-only and compute-only worker nodes. These are normally used only in nonstandard applications.&lt;ref name="michael-noll.com_2"&gt;{{cite web |title=Running Hadoop on Ubuntu Linux System(Multi-Node Cluster) |url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ }}&lt;/ref&gt;

Hadoop requires [[JRE|Java Runtime Environment]] (JRE) 1.6 or higher. The standard startup and shutdown scripts require that [[Secure Shell]] (SSH) be set up between nodes in the cluster.&lt;ref name="michael-noll.com_1"&gt;{{cite web |title=Running Hadoop on Ubuntu Linux (Single-Node Cluster) |url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#prerequisites |access-date=6 June 2013 }}&lt;/ref&gt;

In a larger cluster, HDFS nodes are managed through a dedicated NameNode server to host the file system index, and a secondary NameNode that can generate snapshots of the namenode's memory structures, thereby preventing file-system corruption and loss of data. Similarly, a standalone JobTracker server can manage job scheduling across nodes. When Hadoop MapReduce is used with an alternate file system, the NameNode, secondary NameNode, and DataNode architecture of HDFS are replaced by the file-system-specific equivalents.

===File systems===
===={{Anchor|HDFS}}Hadoop distributed file system====
The ''Hadoop distributed file system'' (HDFS) is a distributed, scalable, and portable [[distributed file system|file system]] written in Java for the Hadoop framework. Some consider it to instead be a [[Distributed data store|data store]] due to its lack of [[POSIX]] compliance,&lt;ref&gt;{{cite web |url=http://www.computerweekly.com/feature/Big-data-storage-Hadoop-storage-basics |title=Big data storage: Hadoop storage basics |last1=Evans |first1=Chris |date=Oct 2013 |website=computerweekly.com |publisher=[[Computer Weekly]] |access-date=21 June 2016 |quote=HDFS is not a file system in the traditional sense and isn't usually directly mounted for a user to view }}&lt;/ref&gt; but it does provide shell commands and Java application programming interface (API) [[Method (computer programming)|methods]] that are similar to other file systems.&lt;ref&gt;{{cite web |url=http://www.dummies.com/how-to/content/managing-files-with-the-hadoop-file-system-command.html |title=Managing Files with the Hadoop File System Commands |last1=deRoos |first1=Dirk |website=dummies.com |publisher=[[For Dummies]] |access-date=21 June 2016}}&lt;/ref&gt; A Hadoop instance is divided into HDFS and MapReduce. HDFS is used for storing the data and MapReduce is used for processing data.
HDFS has five services as follows:
# Name Node
# Secondary Name Node
# Job tracker
# Data Node
# Task Tracker

Top three are Master Services/Daemons/Nodes and bottom two are Slave Services. Master Services can communicate with each other and in the same way Slave services can communicate with each other. Name Node is a master node and Data node is its corresponding Slave node and can talk with each other.

'''Name Node:''' HDFS consists of only one Name Node  that is called the Master Node. The master node can track files, manage the file system and has the metadata of all of the stored data within it. In particular, the name node contains the details of the number of blocks, locations of the data node that the data is stored in, where the replications are stored, and other details. The name node has direct contact with the client.

'''Data Node:''' A Data Node stores data in it as blocks. This is also known as the slave node and it stores the actual data into HDFS which is responsible for the client to read and write. These are slave daemons. Every Data node sends a Heartbeat message to the Name node every 3 seconds and conveys that it is alive. In this way when Name Node does not receive a heartbeat from a data node for 2 minutes, it will take that data node as dead and starts the process of block replications on some other Data node.

'''Secondary Name Node:''' This is only to take care of the checkpoints of the file system metadata which is in the Name Node. This is also known as the checkpoint Node. It is the helper Node for the Name Node.

'''Job Tracker:''' Job Tracker receives the requests for Map Reduce execution from the client. Job tracker talks to the Name Node to know about the location of the data that will be used in processing. The Name Node responds with the metadata of the required processing data.

'''Task Tracker:''' It is the Slave Node for the Job Tracker and it will take the task from the Job Tracker. It also receives code from the Job Tracker. Task Tracker will take the code and apply on the file. The process of applying that code on the file is known as Mapper.&lt;ref&gt;{{Cite web |url=https://hadoop.apache.org/DOCS/R2.7.5/HADOOP-PROJECT-DIST/HADOOP-HDFS/HDFSUSERGUIDE.HTML |title=Archived copy |access-date=19 June 2020 |archive-url=https://web.archive.org/web/20191023001222/http://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html |archive-date=23 October 2019 |url-status=dead }}&lt;/ref&gt;

Hadoop cluster has nominally a single namenode plus a cluster of datanodes, although [[redundancy (engineering)|redundancy]] options are available for the namenode due to its criticality. Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses [[TCP/IP]] [[Internet socket|sockets]] for communication. Clients use [[remote procedure call]]s (RPC) to communicate with each other.

HDFS stores large files (typically in the range of gigabytes to terabytes&lt;ref&gt;{{cite web |title=HDFS Architecture |url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Large_Data_Sets |access-date=1 September 2013 }}&lt;/ref&gt;) across multiple machines. It achieves reliability by [[Replication (computer science)|replicating]] the data across multiple hosts, and hence theoretically does not require [[RAID|redundant array of independent disks (RAID)]] storage on hosts (but to increase input-output (I/O) performance some RAID configurations are still useful). With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully POSIX-compliant, because the requirements for a POSIX file-system differ from the target goals of a Hadoop application. The trade-off of not having a fully POSIX-compliant file-system is increased performance for data [[throughput]] and support for non-POSIX operations such as Append.&lt;ref name="openlibrary1"&gt;{{Cite journal|first = Yaniv |last = Pessach|title = Distributed Storage|edition = Distributed Storage: Concepts, Algorithms, and Implementations|date = 2013
|ol = 25423189M }}&lt;/ref&gt;

In May 2012, high-availability capabilities were added to HDFS,&lt;ref name="failover"&gt;{{cite web |title=Version 2.0 provides for manual failover and they are working on automatic failover |url=https://hadoop.apache.org/releases.html#23+May%2C+2012%3A+Release+2.0.0-alpha+available  |access-date= 30 July 2013 |publisher=Hadoop.apache.org }}&lt;/ref&gt; letting the main metadata server called the NameNode manually fail-over onto a backup. The project has also started developing automatic [[fail-over]]s.

The HDFS file system includes a so-called ''secondary namenode'', a misleading term that some might incorrectly interpret as a backup namenode when the primary namenode goes offline. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple namespaces served by separate namenodes. Moreover, there are some issues in HDFS such as small file issues, scalability problems, Single Point of Failure (SPoF), and bottlenecks in huge metadata requests.
One advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location. For example: if node A contains data (a, b, c) and node X contains data (x, y, z), the job tracker schedules node A to perform map or reduce tasks on (a, b, c) and node X would be scheduled to perform map or reduce tasks on (x, y, z). This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems, this advantage is not always available. This can have a significant impact on job-completion times as demonstrated with data-intensive jobs.&lt;ref&gt;{{cite web |url=http://www.eng.auburn.edu/~xqin/pubs/hcw10.pdf |title= Improving MapReduce performance through data placement in heterogeneous Hadoop Clusters |date=April 2010 |publisher=Eng.auburn.ed }}&lt;/ref&gt;

HDFS was designed for mostly immutable files and may not be suitable for systems requiring concurrent write operations.&lt;ref name="openlibrary1" /&gt;

HDFS can be [[Mount (computing)|mounted]] directly with a [[Filesystem in Userspace]] (FUSE) [[virtual file system]] on [[Linux]] and some other [[Unix]] systems.

File access can be achieved through the native Java API, the [[Thrift (protocol)|Thrift]] API (generates a client in a number of languages e.g. C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, [[Cocoa (API)|Cocoa]], Smalltalk, and [[OCaml]]), the [[command-line interface]], the HDFS-UI [[web application]] over [[HTTP]], or via 3rd-party network client libraries.&lt;ref&gt;{{cite web |url=https://wiki.apache.org/hadoop/MountableHDFS |title=Mounting HDFS |access-date=2016-08-05 }}&lt;/ref&gt;

HDFS is designed for portability across various hardware platforms and for compatibility with a variety of underlying operating systems. The HDFS design introduces portability limitations that result in some performance bottlenecks, since the Java implementation cannot use features that are exclusive to the platform on which HDFS is running.&lt;ref&gt;{{cite web |url=http://www.jeffshafer.com/publications/papers/shafer_ispass10.pdf |title=The Hadoop Distributed Filesystem: Balancing Portability and Performance |last1=Shafer |first1=Jeffrey |last2=Rixner |first2=Scott |last3=Cox |first3=Alan |publisher=Rice University| access-date=2016-09-19 }}&lt;/ref&gt; Due to its widespread integration into enterprise-level infrastructure, monitoring HDFS performance at scale has become an increasingly important issue. Monitoring end-to-end performance requires tracking metrics from datanodes, namenodes, and the underlying operating system.&lt;ref&gt;{{cite web |url=https://www.datadoghq.com/blog/monitor-hadoop-metrics/#toc-hdfs-metrics2 |title=How to Collect Hadoop Performance Metrics |last1=Mouzakitis |first1=Evan| access-date=2016-10-24 |date=21 July 2016 }}&lt;/ref&gt; There are currently several monitoring platforms to track HDFS performance, including [[Hortonworks#Hortonworks Data Platform (HDP)|Hortonworks]], [[Cloudera]], and [[Datadog]].

====Other file systems====
Hadoop works directly with any distributed file system that can be mounted by the underlying operating system by simply using a &lt;code&gt;file://&lt;/code&gt; URL; however, this comes at a price – the loss of locality. To reduce network traffic, Hadoop needs to know which servers are closest to the data, information that Hadoop-specific file system bridges can provide.

In May 2011, the list of supported file systems bundled with Apache Hadoop were:

* HDFS: Hadoop's own rack-aware file system.&lt;ref&gt;{{cite web |url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Rack_Awareness |title=HDFS Users Guide&amp;nbsp;– Rack Awareness |publisher=Hadoop.apache.org |access-date=2013-10-17 }}&lt;/ref&gt; This is designed to scale to tens of petabytes of storage and runs on top of the file systems of the underlying [[operating system]]s.
* Apache Hadoop Ozone: HDFS-compatible object store targeting optimized for billions small files.
* [[FTP]] file system: This stores all its data on remotely accessible FTP servers.
* [[Amazon Simple Storage Service|Amazon S3 (Simple Storage Service)]] object storage: This is targeted at clusters hosted on the [[Amazon Elastic Compute Cloud]] server-on-demand infrastructure. There is no rack-awareness in this file system, as it is all remote.
* Windows Azure Storage Blobs (WASB) file system: This is an extension of HDFS that allows distributions of Hadoop to access data in Azure blob stores without moving the data permanently into the cluster.

A number of third-party file system bridges have also been written, none of which are currently in Hadoop distributions. However, some commercial distributions of Hadoop ship with an alternative file system as the default{{snd}}specifically IBM and [[MapR]].

* In 2009, [[IBM]] discussed running Hadoop over the [[IBM General Parallel File System]].&lt;ref&gt;{{cite web |url=http://www.usenix.org/events/hotcloud09/tech/full_papers/ananthanarayanan.pdf |title= Cloud analytics: Do we really need to reinvent the storage stack? |date=June 2009 |publisher=IBM }}&lt;/ref&gt; The source code was published in October 2009.&lt;ref&gt;{{cite web |url=https://issues.apache.org/jira/browse/HADOOP-6330 |title=HADOOP-6330: Integrating IBM General Parallel File System implementation of Hadoop Filesystem interface |date=2009-10-23 |publisher=IBM }}&lt;/ref&gt;
* In April 2010, Parascale published the source code to run Hadoop against the Parascale file system.&lt;ref&gt;{{cite web |url=https://issues.apache.org/jira/browse/HADOOP-6704 |title=HADOOP-6704: add support for Parascale filesystem |date=2010-04-14 |publisher=Parascale }}&lt;/ref&gt;
* In April 2010, Appistry released a Hadoop file system driver for use with its own CloudIQ Storage product.&lt;ref&gt;{{cite web |url=http://resources.appistry.com/news-and-events/press/06072010-appistry-cloudiq-storage-now-generally-available |title=HDFS with CloudIQ Storage |date=2010-07-06 |publisher=Appistry,Inc. |access-date=2013-12-10 |archive-url=https://web.archive.org/web/20140405044536/http://resources.appistry.com/news-and-events/press/06072010-appistry-cloudiq-storage-now-generally-available |archive-date=2014-04-05 |url-status=dead }}&lt;/ref&gt;
* In June 2010, [[Hewlett-Packard|HP]] discussed a location-aware [[IBRIX Fusion]] file system driver.&lt;ref&gt;{{cite web |url=http://www.slideshare.net/steve_l/high-availability-hadoop |title=High Availability Hadoop |date=2010-06-09 |publisher=HP }}&lt;/ref&gt;
* In May 2011, [[MapR|MapR Technologies Inc.]] announced the availability of an alternative file system for Hadoop, [[MapR FS]], which replaced the HDFS file system with a full random-access read/write file system.

===JobTracker and TaskTracker: the MapReduce engine===
{{Main|MapReduce}}

Atop the file systems comes the MapReduce Engine, which consists of one ''JobTracker'', to which client applications submit MapReduce jobs. The JobTracker pushes work to available ''TaskTracker'' nodes in the cluster, striving to keep the work as close to the data as possible. With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack. This reduces network traffic on the main backbone network. If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns a separate [[Java virtual machine]] (JVM) process to prevent the TaskTracker itself from failing if the running job crashes its JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by [[Jetty (web server)|Jetty]] and can be viewed from a web browser.

Known limitations of this approach are:

# The allocation of work to TaskTrackers is very simple. Every TaskTracker has a number of available ''slots'' (such as "4 slots"). Every active map or reduce task takes up one slot. The Job Tracker allocates work to the tracker nearest to the data with an available slot. There is no consideration of the current [[load (computing)|system load]] of the allocated machine, and hence its actual availability.
# If one TaskTracker is very slow, it can delay the entire MapReduce job{{snd}} especially towards the end, when everything can end up waiting for the slowest task. With speculative execution enabled, however, a single task can be executed on multiple slave nodes.

====Scheduling====
By default Hadoop uses [[FIFO (computing and electronics)|FIFO]] scheduling, and optionally 5 scheduling priorities to schedule jobs from a work queue.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/common/docs/current/commands_manual.html|title=Commands Guide|date=17 August 2011|access-date=11 December 2017|url-status=bot: unknown|archive-url=https://web.archive.org/web/20110817053520/http://hadoop.apache.org/common/docs/current/commands_manual.html#job|archive-date=17 August 2011}}&lt;/ref&gt; In version 0.19 the job scheduler was refactored out of the JobTracker, while adding the ability to use an alternate scheduler (such as the ''Fair scheduler'' or the ''Capacity scheduler'', described next).&lt;ref&gt;{{cite web |title=Refactor the scheduler out of the JobTracker |url=https://issues.apache.org/jira/browse/HADOOP-3412 |work=Hadoop Common |publisher=Apache Software Foundation |access-date=9 June 2012 }}&lt;/ref&gt;

=====Fair scheduler=====
The fair scheduler was developed by [[Facebook]].&lt;ref&gt;{{cite web |url=http://www.ibm.com/developerworks/library/os-hadoop-scheduling/ |title=Scheduling in Hadoop |first=M. Tim |last=Jones |date=6 December 2011 |website=ibm.com |publisher=[[IBM]] |access-date=20 November 2013 }}&lt;/ref&gt; The goal of the fair scheduler is to provide fast response times for small jobs and [[Quality of service]] (QoS) for production jobs. The fair scheduler has three basic concepts.&lt;ref&gt;{{cite web|url=https://svn.apache.org/repos/asf/hadoop/common/branches/MAPREDUCE-233/src/contrib/fairscheduler/designdoc/fair_scheduler_design_doc.pdf|title=Hadoop Fair Scheduler Design Document|website=apache.org|access-date=12 October 2017}}&lt;/ref&gt;
# Jobs are grouped into [[Pool (computer science)|pools]].
# Each pool is assigned a guaranteed minimum share.
# Excess capacity is split between jobs.

By default, jobs that are uncategorized go into a default pool. Pools have to specify the minimum number of map slots, reduce slots, as well as a limit on the number of running jobs.

=====Capacity scheduler=====
The capacity scheduler was developed by Yahoo. The capacity scheduler supports several features that are similar to those of the fair scheduler.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/docs/stable1/capacity_scheduler.html|title=CapacityScheduler Guide|website=Hadoop.apache.org|access-date=31 December 2015}}&lt;/ref&gt;

# Queues are allocated a fraction of the total resource capacity.
# Free resources are allocated to queues beyond their total capacity.
# Within a queue, a job with a high level of priority has access to the queue's resources.

There is no [[preemption (computing)|preemption]] once a job is running.

===Difference between Hadoop 1 and Hadoop 2 (YARN)===
The biggest difference between Hadoop 1 and Hadoop 2 is the addition of YARN (Yet Another Resource Negotiator), which replaced the MapReduce engine in the first version of Hadoop.
YARN strives to allocate resources to various applications effectively. It runs two dæmons, which take care of two different tasks: the ''resource manager'', which does job tracking and resource allocation to applications, the ''application master'', which monitors progress of the execution.

===Difference between Hadoop 2 and Hadoop 3===
There are important features provided by Hadoop 3. For example, while there is one single ''namenode'' in Hadoop 2, Hadoop 3 enables having multiple name nodes, which solves the single point of failure problem.

In Hadoop 3, there are containers working in principle of [[Docker (software)|Docker]], which reduces time spent on application development.

One of the biggest changes is that Hadoop 3 decreases storage overhead with [[Erasure code|erasure coding]].

Also, Hadoop 3 permits usage of GPU hardware within the cluster, which is a very substantial benefit to execute deep learning algorithms on a Hadoop cluster.&lt;ref&gt;{{cite web |url=https://it.hortonworks.com/blog/hadoop-3-adds-value-hadoop-2/ |title=How Apache Hadoop 3 Adds Value Over Apache Hadoop 2 |website=hortonworks.com |access-date=2018-06-11|date=7 February 2018 }}&lt;/ref&gt;

===Other applications===
The HDFS is not restricted to MapReduce jobs. It can be used for other applications, many of which are under development at Apache. The list includes the [[HBase]] database, the [[Apache Mahout]] [[machine learning]] system, and the [[Apache Hive]] [[Data Warehouse]] system. Hadoop can, in theory, be used for any sort of work that is batch-oriented rather than real-time, is very data-intensive, and benefits from parallel processing of data. It can also be used to complement a real-time system, such as [[lambda architecture]], Apache Storm, Flink and Spark Streaming.&lt;ref&gt;{{cite book |chapter= Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming |date=May 2016 |publisher=IEEE |doi=10.1109/IPDPSW.2016.138 |title=2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) |pages=1789–1792 |last1=Chintapalli |first1=Sanket |last2=Dagit |first2=Derek |last3=Evans |first3=Bobby |last4=Farivar |first4=Reza |last5=Graves |first5=Thomas |last6=Holderbaugh |first6=Mark |last7=Liu |first7=Zhuo |last8=Nusbaum |first8=Kyle |last9=Patil |first9=Kishorkumar |last10=Peng |first10=Boyang Jerry |last11=Poulosky |first11=Paul |isbn=978-1-5090-3682-0 }}&lt;/ref&gt;

{{As of|2009|10}}, commercial applications of Hadoop&lt;ref&gt;{{cite web |date=10 October 2009 |url=http://www.dbms2.com/2009/10/10/enterprises-using-hadoo/ |title="How 30+ enterprises are using Hadoop", in DBMS2 |publisher=Dbms2.com |access-date=2013-10-17 }}&lt;/ref&gt; included:-

* log and/or clickstream analysis of various kinds
* marketing analytics
* machine learning and/or sophisticated data mining
* image processing
* processing of XML messages
* web crawling and/or text processing
* general archiving, including of relational/tabular data, e.g. for compliance

==Prominent use cases==
On 19 February 2008, Yahoo! Inc. launched what they claimed was the world's largest Hadoop production application. The Yahoo! Search Webmap is a Hadoop application that runs on a Linux cluster with more than 10,000 [[Multi-core|cores]] and produced data that was used in every Yahoo! web search query.&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html|title=Yahoo! Launches World's Largest Hadoop Production Application|date=19 February 2008|work=Yahoo|access-date=31 December 2015|archive-url=https://web.archive.org/web/20160307081144/https://developer.yahoo.com/blogs/hadoop/yahoo-launches-world-largest-hadoop-production-application-398.html|archive-date=7 March 2016|url-status=dead}}&lt;/ref&gt; There are multiple Hadoop clusters at Yahoo! and no HDFS file systems or MapReduce jobs are split across multiple data centers. Every Hadoop cluster node bootstraps the Linux image, including the Hadoop distribution. Work that the clusters perform is known to include the index calculations for the Yahoo! search engine. In June 2009, Yahoo! made the source code of its Hadoop version available to the open-source community.&lt;ref&gt;{{cite web |url=http://developer.yahoo.com/hadoop/ |title=Hadoop and Distributed Computing at Yahoo! |publisher=Yahoo! |date=2011-04-20 |access-date=2013-10-17 }}&lt;/ref&gt;

In 2010, Facebook claimed that they had the largest Hadoop cluster in the world with 21 [[Petabyte|PB]] of storage.&lt;ref&gt;{{cite web |url=http://hadoopblog.blogspot.com/2010/05/facebook-has-worlds-largest-hadoop.html |title=HDFS: Facebook has the world's largest Hadoop cluster! |publisher=Hadoopblog.blogspot.com |date=2010-05-09 |access-date=2012-05-23 }}&lt;/ref&gt; In June 2012, they announced the data had grown to 100 PB&lt;ref&gt;{{cite web |url=http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920 |title=Under the Hood: Hadoop Distributed File system reliability with Namenode and Avatarnode |publisher=Facebook |access-date=2012-09-13 }}&lt;/ref&gt; and later that year they announced that the data was growing by roughly half a PB per day.&lt;ref&gt;{{cite web |url=https://www.facebook.com/notes/facebook-engineering/under-the-hood-scheduling-mapreduce-jobs-more-efficiently-with-corona/10151142560538920 |title=Under the Hood: Scheduling MapReduce jobs more efficiently with Corona |publisher=Facebook |access-date=2012-11-09 }}&lt;/ref&gt;

{{As of|2013}}, Hadoop adoption had become widespread: more than half of the Fortune 50 companies used Hadoop.&lt;ref&gt;{{cite press release &lt;!-- |author=Staff writer(s); no by-line.--&gt; |title=Altior's AltraSTAR – Hadoop Storage Accelerator and Optimizer Now Certified on CDH4 (Cloudera's Distribution Including Apache Hadoop Version 4) |url=http://www.prnewswire.com/news-releases/altiors-altrastar---hadoop-storage-accelerator-and-optimizer-now-certified-on-cdh4-clouderas-distribution-including-apache-hadoop-version-4-183906141.html |location=Eatontown, NJ |publisher=Altior Inc. |date=2012-12-18 |access-date=2013-10-30 }}&lt;/ref&gt;

==Hadoop hosting in the cloud==
Hadoop can be deployed in a traditional onsite datacenter as well as in [[Cloud computing|the cloud]].&lt;ref&gt;{{cite web|url=http://azure.microsoft.com/en-us/solutions/hadoop/|title=Hadoop - Microsoft Azure|website=azure.microsoft.com|access-date=11 December 2017}}&lt;/ref&gt; The cloud allows organizations to deploy Hadoop without the need to acquire hardware or specific setup expertise.&lt;ref&gt;{{cite web |url=http://azure.microsoft.com/en-us/solutions/hadoop/ |title=Hadoop |publisher=Azure.microsoft.com |access-date=2014-07-22 }}&lt;/ref&gt;

==Commercial support==
&lt;!--
Please don't go overboard in marketing here, as it will only be edited out. Use external citations rather than press releases, and be aware of Wikipedia's rules regarding conflict of interest and external links, WP:COI and WP:EL specifically
--&gt;
A number of companies offer commercial implementations or support for Hadoop.&lt;ref&gt;{{cite web |url=http://gigaom.com/cloud/why-we-need-more-hadoop-innovation/ |title=Why the Pace of Hadoop Innovation Has to Pick Up |publisher=Gigaom.com |date=2011-04-25 |access-date=2013-10-17 }}&lt;/ref&gt;

===Branding===
The Apache Software Foundation has stated that only software officially released by the Apache Hadoop Project can be called ''Apache Hadoop'' or ''Distributions of Apache Hadoop''.&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/Defining%20Hadoop |title=Defining Hadoop |publisher=Wiki.apache.org |date=2013-03-30 |access-date=2013-10-17 }}&lt;/ref&gt; The naming of products and derivative works from other vendors and the term "compatible" are somewhat controversial within the Hadoop developer community.&lt;ref&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/hadoop-general/201105.mbox/%3C4DC91392.2010308@apache.org%3E |title=Defining Hadoop Compatibility: revisited |publisher=Mail-archives.apache.org |date=2011-05-10 |access-date=2013-10-17 }}&lt;/ref&gt;

==Papers==
Some papers influenced the birth and growth of Hadoop and big data processing. Some of these are:
* Jeffrey Dean, Sanjay Ghemawat (2004) [https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/index.html MapReduce: Simplified Data Processing on Large Clusters], Google. This paper inspired Doug Cutting to develop an open-source implementation of the Map-Reduce framework. He named it Hadoop, after his son's toy elephant.
* Michael Franklin, Alon Halevy, David Maier (2005) [http://www.eecs.berkeley.edu/~franklin/Papers/dataspaceSR.pdf From Databases to Dataspaces: A New Abstraction for Information Management]. The authors highlight the need for storage systems to accept all data formats and to provide APIs for data access that evolve based on the storage system's understanding of the data.
* Fay Chang et al.&lt;!-- Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber --&gt; (2006) [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf Bigtable: A Distributed Storage System for Structured Data], Google.
* Robert Kallman et al.&lt;!-- Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alexander Rasin, Stanley Zdonik, Evan P. C. Jones, Samuel Madden, Michael Stonebraker, Yang Zhang, John Hugg, Daniel J. Abadi --&gt; (2008) [http://www.vldb.org/pvldb/vol1/1454211.pdf H-store: a high-performance, distributed main memory transaction processing system]

==See also==
{{Portal|Free and open-source software}}
* [[Apache Accumulo]] – Secure [[Bigtable]]&lt;ref&gt;{{cite web |url=https://accumulo.apache.org/1.4/user_manual/Security.html |title=Apache Accumulo User Manual: Security &lt;!-- |author=Staff writer(s); no by-line.--&gt; |website=apache.org |publisher=Apache Software Foundation |access-date=2014-12-03 }}&lt;/ref&gt;
* [[Apache Cassandra]], a column-oriented database that supports access from Hadoop
* [[Apache CouchDB]], a database that uses JSON for documents, JavaScript for MapReduce queries, and regular HTTP for an API
* Apache HCatalog, a table and storage management layer for Hadoop
* [[Big data]]
*[[Data Intensive Computing]]
* [[HPCC]] – [[LexisNexis]] Risk Solutions High Performance Computing Cluster
* [[Hypertable]] – HBase alternative
* [[Sector/Sphere]] – Open source distributed storage and processing
* [[Simple Linux Utility for Resource Management]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    = Chuck
| last1     = Lam
| date      = 28 July 2010
| title     = Hadoop in Action
| edition   = 1st
| publisher = [[Manning Publications]]
| page     = 325
| isbn      = 978-1-935-18219-1
| url       = https://books.google.com/books?id=iGq3PwAACAAJ
}}
*{{Cite book
 |first1       = Jason
 |last1        = Venner
 |date         = 22 June 2009
 |title        = Pro Hadoop
 |edition      = 1st
 |publisher    = [[Apress]]
 |page         = 440
 |isbn         = 978-1-430-21942-2
 |url          = http://www.apress.com/book/view/1430219424
 |access-date  = 3 July 2009
 |archive-url  = https://web.archive.org/web/20101205204120/http://apress.com/book/view/1430219424
 |archive-date = 5 December 2010
 |url-status     = dead
}}
*{{Cite book
| first1    = Tom
| last1     = White
| date      = 16 June 2009
| title     = Hadoop: The Definitive Guide
| edition   = 1st
| publisher = [[O'Reilly Media]]
| page      = 524
| isbn      = 978-0-596-52197-4
| url       = http://oreilly.com/catalog/9780596521974
}}
*{{Cite book
| first1    = Deepak
| last1     = Vohra
| date      = October 2016
| title     = Practical Hadoop Ecosystem: A Definitive Guide to Hadoop-Related Frameworks and Tools
| edition   = 1st
| publisher = [[Apress]]
| page      = 429
| isbn      = 978-1-4842-2199-0
| url       = https://www.apress.com/us/book/9781484221983
}}
*{{cite book |last=Wiktorski |first=Tomasz |date=January 2019 |title=Data-intensive Systems |url=https://www.springer.com/it/book/9783030046026 |location=Cham, Switzerland |publisher=Springer |isbn=978-3-030-04603-3}}

{{Refend}}

==External links==
* {{Official website}}

{{Apache Software Foundation}}
{{File systems}}
{{Authority control}}
{{Use dmy dates|date=June 2019}}

{{DEFAULTSORT:Hadoop}}
[[Category:Apache Software Foundation projects|Hadoop]]
[[Category:Big data products]]
[[Category:Distributed file systems]]
[[Category:Free software for cloud computing]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Hadoop|*]]
[[Category:Software using the Apache license]]</text>
      <sha1>7s67jqt2guthmk72dt9ap9h3gbvssca</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Spark</title>
    <ns>0</ns>
    <id>42164234</id>
    <revision>
      <id>998778125</id>
      <parentid>998558704</parentid>
      <timestamp>2021-01-07T00:24:41Z</timestamp>
      <contributor>
        <username>Certes</username>
        <id>5984052</id>
      </contributor>
      <minor/>
      <comment>improve link: C Sharp (programming language) (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28233" xml:space="preserve">{{Infobox software
| name                   = Apache Spark
| logo                   = [[File:Apache Spark logo.svg|frameless|250px|Spark Logo]]
| caption                = 
| author                 = [[Matei Zaharia]]
| developer              = [[Apache Spark]]
| status                 = Active
| released               = [https://github.com/apache/spark/releases/tag/v1.0.0 {{Start date and age|2014|05|26}}]
| latest release version = 3.0.1
| latest release date    = {{Start date and age|2020|10|02}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Microsoft Windows]], [[macOS]], [[Linux]]
| size                   = 
| programming language   = [[Scala (programming language)|Scala]]&lt;ref&gt;{{cite web |url=https://spark.apache.org/releases/spark-release-2-0-0.html |title=Spark Release 2.0.0 |quote=MLlib in R: SparkR now offers MLlib APIs [..] Python: PySpark now offers many more MLlib algorithms"}}&lt;/ref&gt;
| genre                  = Data analytics, [[machine learning]] algorithms
| license                = [[Apache License 2.0]]
| website                = {{Official URL}}
| repo                   = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;spark.git|Spark Repository}}
| language               = [[Scala (programming_language)|Scala]], [[Java (programming language)|Java]], [[SQL]], [[Python (programming language)|Python]], [[R (programming language)|R]], [[C Sharp (programming language)|C#]], [[F Sharp (programming language)|F#]]
}}
'''Apache Spark''' is an [[Open-source software|open-source]] [[Distributed computing|distributed]] general-purpose [[Cluster computing|cluster-computing]] [[Software framework|framework]]. Spark provides an [[application programming interface|interface]] for programming entire clusters with implicit [[data parallelism]] and [[fault tolerance]]. Originally developed at the [[UC Berkeley|University of California, Berkeley]]'s [[AMPLab]], the Spark [[codebase]] was later donated to the [[Apache Software Foundation]], which has maintained it since.

==Overview==
Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only [[multiset]] of data items distributed over a cluster of machines, that is maintained in a [[fault-tolerant computing|fault-tolerant]] way.&lt;ref name="hc10"&gt;{{cite conference |first1=Matei |last1=Zaharia |first2=Mosharaf |last2=Chowdhury |first3=Michael J. |last3=Franklin |first4=Scott |last4=Shenker |first5=Ion |last5=Stoica |title=Spark: Cluster Computing with Working Sets |conference=USENIX Workshop on Hot Topics in Cloud Computing (HotCloud) |url=https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Spark-Cluster-Computing-with-Working-Sets.pdf}}&lt;/ref&gt; The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary [[application programming interface]] (API), but as of Spark 2.x use of the Dataset API is encouraged&lt;ref&gt;{{cite web |url=https://spark.apache.org/docs/2.2.0/quick-start.html |title=Spark 2.2.0 Quick Start |author=&lt;!--Not stated--&gt; |date=2017-07-11 |website=apache.org |access-date=2017-10-19 |quote=we highly recommend you to switch to use Dataset, which has better performance than RDD}}&lt;/ref&gt; even though the RDD API is not [[deprecated]].&lt;ref&gt;{{cite web |url=https://spark.apache.org/docs/2.2.0/api/scala/index.html#deprecated-list |title=Spark 2.2.0 deprecation list |author=&lt;!--Not stated--&gt; |date=2017-07-11 |website=apache.org |access-date=2017-10-10}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html |title=A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets: When to use them and why |last=Damji |first=Jules |date=2016-07-14 |website=databricks.com |access-date=2017-10-19}}&lt;/ref&gt; The RDD technology still underlies the Dataset API.&lt;ref&gt;{{cite book |last=Chambers |first=Bill |date=2017-08-10 |title=Spark: The Definitive Guide |chapter-url=http://techbus.safaribooksonline.com/book/operating-systems-and-server-administration/apache/9781491912201/12dot-resilient-distributed-datasets-rdds/about_rdds_html |publisher=[[O'Reilly Media]] |chapter=12 |quote=virtually all Spark code you run, where DataFrames or Datasets, compiles down to an RDD }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.janbasktraining.com/blog/what-is-spark/ |title= What is Apache Spark? Spark Tutorial Guide for Beginner |website=janbasktraining.com |access-date=2018-04-13|date= 2018-04-13 }}&lt;/ref&gt;

Spark and its RDDs were developed in 2012 in response to limitations in the [[MapReduce]] cluster computing [[Programming paradigm|paradigm]], which forces a particular linear [[dataflow]] structure on distributed programs: MapReduce programs read input data from disk, [[Map (parallel pattern)|map]] a function across the data, [[Fold (higher-order function)|reduce]] the results of the map, and store reduction results on disk. Spark's RDDs function as a [[working set]] for distributed programs that offers a (deliberately) restricted form of distributed [[shared memory]].&lt;ref&gt;{{cite conference |first1=Matei |last1=Zaharia |first2=Mosharaf |last2=Chowdhury |first3=Tathagata |last3=Das |first4=Ankur |last4=Dave |first5=Justin |last5=Ma |first6=Murphy |last6=McCauley |first7=Michael |last7=J. |first8=Scott |last8=Shenker |first9=Ion |last9=Stoica |date=2010 |title=Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing |url=https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf |conference=USENIX Symp. Networked Systems Design and Implementation}}&lt;/ref&gt;

Spark facilitates the implementation of both [[iterative algorithm]]s, which visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated [[database]]-style querying of data. The [[latency (engineering)|latency]] of such applications may be reduced by several orders of magnitude compared to [[Apache Hadoop]] MapReduce implementation.{{r|hc10}}&lt;ref&gt;{{cite journal|first1=Reynold| last1=Xin| first2=Josh |last2=Rosen| first3=Matei| last3=Zaharia| first4=Michael| last4=Franklin| first5=Scott| last5=Shenker| first6=Ion| last6=Stoica|title=Shark: SQL and Rich Analytics at Scale| conference=SIGMOD 2013|date=June 2013| url=https://amplab.cs.berkeley.edu/wp-content/uploads/2013/02/shark_sigmod2013.pdf| bibcode=2012arXiv1211.6176X| arxiv=1211.6176}}&lt;/ref&gt;
Among the class of iterative algorithms are the training algorithms for [[machine learning]] systems, which formed the initial impetus for developing Apache Spark.&lt;ref&gt;{{cite web |title=4 reasons why Spark could jolt Hadoop into hyperdrive |first=Derrick |last=Harris |date=28 June 2014 |url=https://gigaom.com/2014/06/28/4-reasons-why-spark-could-jolt-hadoop-into-hyperdrive/ |website=[[Gigaom]]}}&lt;/ref&gt;

Apache Spark requires a [[cluster manager]] and a [[Clustered file system|distributed storage system]]. For cluster management, Spark supports standalone (native Spark cluster,  where you can launch a cluster either manually or use the launch scripts provided by the install package. It is also possible to run these daemons on a single machine for testing), [[Apache Hadoop|Hadoop YARN]], [[Apache Mesos]] or [[Kubernetes]]. &lt;ref&gt;{{cite web|url=https://spark.apache.org/docs/2.4.0/cluster-overview.html#cluster-manager-types|title=Cluster Mode Overview - Spark 2.4.0 Documentation - Cluster Manager Types|author=&lt;!--Staff writer(s); no by-line.--&gt;|date=2019-07-09|website=apache.org|publisher=Apache Foundation|access-date=2019-07-09}}&lt;/ref&gt; For distributed storage, Spark can interface with a wide variety, including [[Alluxio]], [[Apache Hadoop#Hadoop distributed file system|Hadoop Distributed File System (HDFS)]],&lt;ref&gt;[https://amplab.cs.berkeley.edu/software/ Figure showing Spark in relation to other open-source Software projects including Hadoop]&lt;/ref&gt; [[MapR#MapR converged data platform|MapR File System (MapR-FS)]],&lt;ref&gt;[http://doc.mapr.com/display/MapR/Ecosystem+Support+Matrix MapR ecosystem support matrix]&lt;/ref&gt; [[Apache Cassandra|Cassandra]],&lt;ref&gt;{{cite mailing list |url=http://mail-archives.apache.org/mod_mbox/cassandra-user/201409.mbox/%3CCABNXB2DE5Apmvn1nNg79+VdPCSZiCsGdt=ZB4s4OF_5JzS60iA@mail.gmail.com%3E |title=Re: cassandra + spark / pyspark |date=2014-09-10 |access-date=2014-11-21 |mailing-list=Cassandra User |last=Doan |first=DuyHai }}&lt;/ref&gt; [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Amazon S3]], [[Apache Kudu|Kudu]], [[Lustre (file system)|Lustre file system]],&lt;ref&gt;{{cite book|chapter=Characterization and Optimization of Memory-Resident MapReduce on HPC Systems|publisher=IEEE|date=May 2014|doi=10.1109/IPDPS.2014.87|isbn=978-1-4799-3800-1|title=2014 IEEE 28th International Parallel and Distributed Processing Symposium|last1=Wang|first1=Yandong|last2=Goldstone|first2=Robin|last3=Yu|first3=Weikuan|last4=Wang|first4=Teng|pages=799–808|s2cid=11157612}}&lt;/ref&gt; or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per [[CPU core]].

===Spark Core===
Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic [[I/O interface|I/O]] functionalities, exposed through an application programming interface (for [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Scala (programming language)|Scala]], [[.NET Core|.NET]]&lt;ref name=":1"&gt;{{Citation|title=dotnet/spark|date=2020-09-14|url=https://github.com/dotnet/spark|publisher=.NET Platform|access-date=2020-09-14}}&lt;/ref&gt; and [[R (programming language)|R]]) centered on the RDD [[Abstraction (computer science)|abstraction]] (the Java API is available for other JVM languages, but is also usable for some other non-JVM languages that can connect to the JVM, such as [[Julia (programming language)|Julia]]&lt;ref&gt;{{Cite web | url=https://github.com/dfdx/Spark.jl |title =GitHub - DFDX/Spark.jl: Julia binding for Apache Spark.|date = 2019-05-24}}&lt;/ref&gt;). This interface mirrors a [[functional programming|functional]]/[[higher-order programming|higher-order]] model of programming: a "driver" program invokes parallel operations such as map, [[Filter (computer science)|filter]] or reduce on an RDD by passing a function to Spark, which then schedules the function's execution in parallel on the cluster.{{r|hc10}} These operations, and additional ones such as [[Join (database)|joins]], take RDDs as input and produce new RDDs. RDDs are [[Immutable object|immutable]] and their operations are [[lazy evaluation|lazy]]; fault-tolerance is achieved by keeping track of the "lineage" of each RDD (the sequence of operations that produced it) so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, .NET, Java, or Scala objects.

Besides the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: ''broadcast variables'' reference read-only data that needs to be available on all nodes, while ''accumulators'' can be used to program reductions in an [[imperative programming|imperative]] style.{{r|hc10}}

A typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each {{mono|map}}, {{mono|flatMap}} (a variant of {{mono|map}}) and {{mono|reduceByKey}} takes an [[anonymous function]] that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.

&lt;syntaxhighlight lang="scala"&gt;
val conf = new SparkConf().setAppName("wiki_test") // create a spark config object
val sc = new SparkContext(conf) // Create a spark context
val data = sc.textFile("/path/to/somedir") // Read files from "somedir" into an RDD of (filename, content) pairs.
val tokens = data.flatMap(_.split(" ")) // Split each file into a list of tokens (words).
val wordFreq = tokens.map((_, 1)).reduceByKey(_ + _) // Add a count of one to each token, then sum the counts per word type.
wordFreq.sortBy(s =&gt; -s._2).map(x =&gt; (x._2, x._1)).top(10) // Get the top 10 words. Swap word and count to sort by count.
&lt;/syntaxhighlight&gt;

===Spark SQL===
Spark [[SQL]] is a component on top of Spark Core that introduced a data abstraction called DataFrames,{{efn|Called SchemaRDDs before Spark 1.3&lt;ref&gt;{{Cite web | url=https://spark.apache.org/releases/spark-release-1-3-0.html |title = Spark Release 1.3.0 &amp;#124; Apache Spark}}&lt;/ref&gt;}} which provides support for structured and [[semi-structured data]]. Spark SQL provides a [[domain-specific language]] (DSL) to manipulate DataFrames in [[Scala (programming language)|Scala]], [[Java (programming language)|Java]], [[Python (programming language)|Python]] or [[.NET Core|.NET]].&lt;ref name=":1" /&gt; It also provides SQL language support, with [[command-line interface]]s and [[Open Database Connectivity|ODBC]]/[[Java Database Connectivity|JDBC]] server. Although DataFrames lack the compile-time type-checking afforded by RDDs, as of Spark 2.0, the strongly typed DataSet is fully supported by Spark SQL as well.

&lt;syntaxhighlight lang="scala"&gt;
import org.apache.spark.sql.SparkSession

val url = "jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword" // URL for your database server.
val spark = SparkSession.builder().getOrCreate() // Create a Spark session object

val df = spark
  .read
  .format("jdbc")
  .option("url", url)
  .option("dbtable", "people")
  .load()

df.printSchema() // Looks the schema of this DataFrame.
val countsByAge = df.groupBy("age").count() // Counts people by age

//or alternatively via SQL:
//df.createOrReplaceTempView("people")
//val countsByAge = spark.sql("SELECT age, count(*) FROM people GROUP BY age")
&lt;/syntaxhighlight&gt;

===Spark Streaming===
Spark Streaming uses Spark Core's fast scheduling capability to perform [[Event stream processing|streaming analytics]]. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of [[lambda architecture]].&lt;ref&gt;{{Cite web|url=https://www.pluralsight.com/courses/spark-kafka-cassandra-applying-lambda-architecture|title=Applying the Lambda Architecture with Spark, Kafka, and Cassandra {{!}} Pluralsight|website=www.pluralsight.com|access-date=2016-11-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://blog.cloudera.com/blog/2014/08/building-lambda-architecture-with-spark-streaming/ |title=Building Lambda Architecture with Spark Streaming |last1=Shapira |first1=Gwen |date=29 August 2014 |website=cloudera.com |publisher=Cloudera |access-date=17 June 2016 |quote=re-use the same aggregates we wrote for our batch application on a real-time data stream |archive-url=https://web.archive.org/web/20160614022751/http://blog.cloudera.com/blog/2014/08/building-lambda-architecture-with-spark-streaming/ |archive-date=14 June 2016 |url-status=dead }}&lt;/ref&gt; However, this convenience comes with the penalty of latency equal to the mini-batch duration. Other streaming data engines that process event by event rather than in mini-batches include [[Storm (event processor)|Storm]] and the streaming component of [[Apache Flink|Flink]].&lt;ref&gt;{{cite book |chapter= Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming |date=May 2016 |publisher=IEEE |doi=10.1109/IPDPSW.2016.138 |title=2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) |pages=1789–1792 |last1=Chintapalli |first1=Sanket |last2=Dagit |first2=Derek |last3=Evans |first3=Bobby |last4=Farivar |first4=Reza |last5=Graves |first5=Thomas |last6=Holderbaugh |first6=Mark |last7=Liu |first7=Zhuo |last8=Nusbaum |first8=Kyle |last9=Patil |first9=Kishorkumar |last10=Peng |first10=Boyang Jerry |last11=Poulosky |first11=Paul |isbn=978-1-5090-3682-0 |s2cid=2180634 }}&lt;/ref&gt; Spark Streaming has support built-in to consume from [[Apache Kafka|Kafka]], [[Apache Flume|Flume]], [[Twitter#Implementation|Twitter]], [[ZeroMQ]], [[Amazon Web Services#Database|Kinesis]], and [[Network socket|TCP/IP sockets]].&lt;ref&gt;{{cite web |url=https://www.sigmoid.com/getting-data-into-spark-streaming/ |title=Getting Data into Spark Streaming |last1=Kharbanda |first1=Arush |date=17 March 2015 |website=sigmoid.com |publisher=Sigmoid (Sunnyvale, California IT product company) |access-date=7 July 2016 |archive-url=https://web.archive.org/web/20160815025917/https://www.sigmoid.com/getting-data-into-spark-streaming/ |archive-date=15 August 2016 |url-status=dead }}&lt;/ref&gt;

In Spark 2.x, a separate technology based on Datasets, called Structured Streaming, that has a higher-level interface is also provided to support streaming.&lt;ref&gt;{{cite web |url=https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html |title=Structured Streaming In Apache Spark: A new high-level API for streaming |last=Zaharia |first=Matei |date=2016-07-28 |website=databricks.com |access-date=2017-10-19}}&lt;/ref&gt;

Spark can be deployed in a traditional [[On-premises software|on-premises]] [[data center]] as well as in the [[Cloud computing|cloud]].

===MLlib Machine Learning Library===
Spark MLlib is a [[Distributed computing|distributed]] machine-learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by [[Apache Mahout]] (according to benchmarks done by the MLlib developers against the [[Linear regression|alternating least squares]] (ALS) implementations, and before Mahout itself gained a Spark interface), and [[Scale (computing)|scales]] better than [[Vowpal Wabbit]].&lt;ref&gt;{{cite web |url=http://www.slideshare.net/chaochen5496/mlllib-sparkmeetup8613finalreduced/68 |title=Spark Meetup: MLbase, Distributed Machine Learning with Spark |last1=Sparks |first1=Evan |last2=Talwalkar |first2=Ameet |date=2013-08-06 |website=slideshare.net |publisher=Spark User Meetup, San Francisco, California |access-date=10 February 2014}}&lt;/ref&gt; Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning [[Pipeline (software)|pipelines]], including:

* [[summary statistics]], [[Correlation and dependence|correlations]], [[stratified sampling]], [[hypothesis testing]], random data generation&lt;ref&gt;{{Cite web|title = MLlib {{!}} Apache Spark|url = http://spark.apache.org/mllib/|website = spark.apache.org|access-date = 2016-01-18}}&lt;/ref&gt;
* [[Statistical classification|classification]] and [[Regression analysis|regression]]: [[support vector machines]], [[logistic regression]], [[linear regression]], [[Naive Bayes classifier|naive Bayes classification]], [[Decision tree|Decision Tree]], [[Random forest|Random Forest]], [[Gradient boosting|Gradient-Boosted Tree]]
* [[collaborative filtering]] techniques including alternating least squares (ALS)
* [[Cluster analysis|cluster analysis methods]] including [[K-means clustering|k-means]], and [[latent Dirichlet allocation]] (LDA)
* [[dimensionality reduction|dimensionality reduction techniques]] such as [[singular value decomposition]] (SVD), and [[principal component analysis]] (PCA)
* [[feature extraction]] and [[Data transformation (statistics)|transformation]] functions
* [[optimization (mathematics)|optimization]] algorithms such as [[stochastic gradient descent]], [[limited-memory BFGS]] (L-BFGS)

===GraphX===
GraphX is a distributed [[Graph (abstract data type)|graph-processing]] framework on top of Apache Spark. Because it is based on RDDs, which are immutable, graphs are immutable and thus GraphX is unsuitable for graphs that need to be updated, let alone in a transactional manner like a [[graph database]].&lt;ref&gt;{{cite web |url=http://www.slideshare.net/SparkSummit/finding-graph-isomorphisms-in-graphx-and-graphframes/11 |title=Finding Graph Isomorphisms In GraphX And GraphFrames: Graph Processing vs. Graph Database |last1=Malak |first1=Michael |date=14 June 2016 |website=slideshare.net |publisher=sparksummit.org |access-date=11 July 2016}}&lt;/ref&gt; GraphX provides two separate APIs for implementation of massively parallel algorithms (such as [[PageRank]]): a [[Graph database#Distributed processing|Pregel]] abstraction, and a more general MapReduce-style API.&lt;ref&gt;{{cite book |last=Malak |first=Michael |date=1 July 2016 |title=Spark GraphX in Action |url=https://books.google.com/books?id=8XcPjwEACAAJ |publisher=Manning |page=89 |isbn=9781617292521 |quote=Pregel and its little sibling aggregateMessages() are the cornerstones of graph processing in GraphX. ... algorithms that require more flexibility for the terminating condition have to be implemented using aggregateMessages()}}&lt;/ref&gt; Unlike its predecessor Bagel, which was formally deprecated in Spark 1.6, GraphX has full support for property graphs (graphs where properties can be attached to edges and vertices).&lt;ref&gt;{{cite web |url=http://www.slideshare.net/SparkSummit/finding-graph-isomorphisms-in-graphx-and-graphframes/15 |title=Finding Graph Isomorphisms In GraphX And GraphFrames: Graph Processing vs. Graph Database |last1=Malak |first1=Michael |date=14 June 2016 |website=slideshare.net |publisher=sparksummit.org |access-date=11 July 2016}}&lt;/ref&gt;

GraphX can be viewed as being the Spark in-memory version of [[Apache Giraph]], which utilized Hadoop disk-based MapReduce.&lt;ref&gt;{{cite book |last=Malak |first=Michael |date=1 July 2016 |title=Spark GraphX in Action |url=https://books.google.com/books?id=8XcPjwEACAAJ |publisher=Manning |page=9 |isbn=9781617292521 |quote=Giraph is limited to slow Hadoop Map/Reduce}}&lt;/ref&gt;

Like Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.&lt;ref&gt;{{cite journal|first1=Joseph| last1=Gonzalez| first2=Reynold |last2=Xin| first3=Ankur| last3=Dave| first4=Daniel| last4=Crankshaw| first5=Michael| last5=Franklin| first6=Ion| last6=Stoica|title=GraphX: Graph Processing in a Distributed Dataflow Framework| conference=OSDI 2014|date=Oct 2014| url=https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf}}&lt;/ref&gt;

===Language Support===

Apache Spark has built-in support for Scala, Java, R, and Python with 3rd party support for the .net languages,&lt;ref&gt;[https://dotnet.microsoft.com/apps/data/spark]&lt;/ref&gt; Julia,&lt;ref&gt;[https://github.com/dfdx/Spark.jl]&lt;/ref&gt; and more.

==History==
Spark was initially started by [[Matei Zaharia]] at UC Berkeley's AMPLab in 2009, and open sourced in 2010 under a [[BSD licenses|BSD license]].&lt;ref name=":0"&gt;{{Cite news|url=https://www.computerweekly.com/feature/Apache-Spark-speeds-up-big-data-decision-making|title=Apache Spark speeds up big data decision-making|last=Clark|first=Lindsay|work=ComputerWeekly.com|access-date=2018-05-16|language=en-GB}}&lt;/ref&gt;

In 2013, the project was donated to the Apache Software Foundation and switched its license to [[Apache License|Apache 2.0]]. In February 2014, Spark became a [[Apache Software Foundation#Projects|Top-Level Apache Project]].&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |access-date=4 March 2014}}&lt;/ref&gt;

In November 2014, Spark founder M. Zaharia's company [[Databricks]] set a new world record in large scale sorting using Spark.&lt;ref&gt;[http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html Spark officially sets a new record in large-scale sorting]&lt;/ref&gt;&lt;ref name=":0" /&gt;

Spark had in excess of 1000 contributors in 2015,&lt;ref&gt;[https://www.openhub.net/p/apache-spark Open HUB Spark development activity]&lt;/ref&gt; making it one of the most active projects in the Apache Software Foundation&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |access-date=4 March 2014}}&lt;/ref&gt; and one of the most active open source [[big data]] projects.

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date
|-
| {{Version|o|0.5}}
| 2012-06-12
| 0.5.1
| 2012-10-07
|-
| {{Version|o|0.6}}
| 2012-10-14
| 0.6.2
| 2013-02-07
|-
| {{Version|o|0.7}}
| 2013-02-27
| 0.7.3
| 2013-07-16
|-
| {{Version|o|0.8}}
| 2013-09-25
| 0.8.1
| 2013-12-19
|-
| {{Version|o|0.9}}
| 2014-02-02
| 0.9.2
| 2014-07-23
|-
| {{Version|o|1.0}}
| 2014-05-26
| 1.0.2
| 2014-08-05
|-
| {{Version|o|1.1}}
| 2014-09-11
| 1.1.1
| 2014-11-26
|-
| {{Version|o|1.2}}
| 2014-12-18
| 1.2.2
| 2015-04-17
|-
| {{Version|o|1.3}}
| 2015-03-13
| 1.3.1
| 2015-04-17
|-
| {{Version|o|1.4}}
| 2015-06-11
| 1.4.1
| 2015-07-15
|-
| {{Version|o|1.5}}
| 2015-09-09
| 1.5.2
| 2015-11-09
|-
| {{Version|o|1.6}}
| 2016-01-04
| 1.6.3
| 2016-11-07
|-
| {{Version|o|2.0}}
| 2016-07-26
| 2.0.2
| 2016-11-14
|-
| {{Version|o|2.1}}
| 2016-12-28
| 2.1.3
| 2018-06-26
|-
| {{Version|o|2.2}}
| 2017-07-11
| 2.2.3
| 2019-01-11
|-
| {{Version|co|2.3}}
| 2018-02-28
| 2.3.4
| 2019-09-09
|-
| {{Version|co|2.4 LTS|show=}}
| 2018-11-02
| 2.4.7
| 2020-10-12&lt;ref&gt;{{cite web|author=&lt;!--Not stated--&gt;|title=Spark News|url=https://spark.apache.org/news/spark-2-4-7-released.html|website=apache.org}}&lt;/ref&gt;
|-
| {{Version|c|3.0}}
| 2020-06-18
| 3.0.1
| 2020-10-02&lt;ref&gt;{{cite web|author=&lt;!--Not stated--&gt;|title=Spark News|url=https://spark.apache.org/news/spark-3-0-1-released.html|website=apache.org}}&lt;/ref&gt;
|-
| colspan="4" | &lt;small&gt;{{Version |l |show=111110}}&lt;/small&gt;
|}
&lt;!-- o=Old version; co=Older version, still supported; c=Latest version; p=Latest preview version (same as "Planned-Future"?) --&gt;

===Developers===

Apache Mahout is developed by a community. The project is managed by a group called the "Project Management Committee" (PMC). The current PMC is Aaron Davidson,  Andy Konwinski,  Andrew Or,  Ankur Dave,  Robert Joseph Evans,  DB Tsai,  Dongjoon Hyun,  Felix Cheung,  Hyukjin Kwon,  Haoyuan Li,  Ram Sriharsha,  [[Holden_Karau]],  Herman van Hövell,  Imran Rashid,  Jason Dai,  Joseph Kurata Bradley,  Joseph E. Gonzalez,  Josh Rosen,  Jerry Shao,  Kay Ousterhout,  Cheng Lian,  Xiao Li,  Mark Hamstra,  Michael Armbrust,  [[Matei_Zaharia]],  Xiangrui Meng,  Nicholas Pentreath,  Mosharaf Chowdhury,  Mridul Muralidharan,  Prashant Sharma,  Patrick Wendell,  Reynold Xin,  Ryan LeCompte,  Shane Huang,  Shivaram Venkataraman,  Sean McNamara,  Sean R. Owen,  Stephen Haberman,  Tathagata Das,  Thomas Graves,  Thomas Dudziak,  Takuya Ueshin,  Marcelo Masiero Vanzin,  Wenchen Fan,  Charles Reiss,  Andrew Xia,  Yin Huai,  Yanbo Liang,  Shixiong Zhu.&lt;ref&gt;https://projects.apache.org/committee.html?spark&lt;/ref&gt;

==See also==
* [[List of concurrent and parallel programming languages#APIs/frameworks|List of concurrent and parallel programming APIs/Frameworks]]

==Notes==
{{Notelist}}

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}

{{Apache Software Foundation}}
{{Parallel computing}}

{{DEFAULTSORT:Spark}}
[[Category:Apache Software Foundation projects|Spark]]
[[Category:Big data products]]
[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Free software programmed in Scala]]
[[Category:Hadoop]]
[[Category:Java platform]]
[[Category:Software using the Apache license]]
[[Category:University of California, Berkeley]]
[[Category:Articles with example Scala code]]</text>
      <sha1>m82tssftksz29s0bsl4zsqqtee5iem1</sha1>
    </revision>
  </page>
  <page>
    <title>Cuneiform (programming language)</title>
    <ns>0</ns>
    <id>51797637</id>
    <revision>
      <id>996789284</id>
      <parentid>981925969</parentid>
      <timestamp>2020-12-28T16:51:18Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <comment>Add: pmid, s2cid. | You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | Suggested by Neko-chan | [[Category:Cross-platform free software]] | via #UCB_Category 61/378</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18658" xml:space="preserve">{{ Infobox programming language
| name                   = Cuneiform
| logo                   = G18225.png
| screenshot             = Cf screenshot.jpg
| caption                = Screenshot of the Cuneiform editor and command line shell
| paradigm               = [[functional programming|functional]], [[Scientific workflow system|scientific workflow]]
| designer               = Jörgen Brandt
| founder                =
| status                 = Active
| latest release version = 3.0.4
| latest release date    = {{release date|2018|11|19}}
| latest preview version =
| latest preview date    =
| typing                 = simple types
| implementations        =
| dialects               =
| influenced_by          = [[Swift (parallel scripting language)|Swift]]
| influenced             =
| operating system       = [[Linux]], [[Mac OS]]
| programming language   = [[Erlang (programming language)|Erlang]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://cuneiform-lang.org/}}
| file_ext               = .cfl
| year                   = 2013
}}

'''Cuneiform''' is an [[open source software|open-source]] [[Scientific workflow system|workflow language]]
for large-scale scientific data analysis.&lt;ref&gt;https://github.com/joergen7/cuneiform&lt;/ref&gt;&lt;ref&gt;{{Cite journal 
| last1 = Brandt | first1 = Jörgen 
| last2 = Bux    | first2 = Marc N. 
| last3 = Leser  | first3 = Ulf
| title = Cuneiform: A functional language for large scale scientific data analysis
| journal = Proceedings of the Workshops of the EDBT/ICDT
| volume = 1330
| pages = 17–26
| year = 2015
| url = http://ceur-ws.org/Vol-1330/paper-03.pdf
}}&lt;/ref&gt;
It is a [[Type system#STATIC|statically typed]] [[Functional programming|functional programming language]] promoting [[parallel computing]]. It features a versatile [[foreign function interface]] allowing users to integrate software from many external programming languages. At the organizational level Cuneiform provides facilities like [[Conditional (computer programming)|conditional branching]] and [[Recursion|general recursion]] making it [[Turing completeness|Turing-complete]]. In this, Cuneiform is the attempt to close the gap between scientific workflow systems like [[Apache Taverna|Taverna]], [[KNIME]], or [[Galaxy (computational biology)|Galaxy]] and large-scale data analysis programming models like [[MapReduce]] or [[Pig (programming tool)|Pig Latin]] while offering the generality of a functional programming language.

Cuneiform is implemented in distributed [[Erlang (programming language)|Erlang]]. If run in distributed mode it drives a [[POSIX]]-compliant distributed file system like [[Gluster]] or [[Ceph (software)#CephFS|Ceph]] (or a [[Filesystem in Userspace|FUSE]] integration of some other file system, e.g., [[Apache Hadoop#HDFS|HDFS]]). Alternatively, Cuneiform scripts can be executed on top of [[HTCondor]] or [[Apache Hadoop|Hadoop]].&lt;ref&gt;{{cite web|title=Scalable Multi-Language Data Analysis on Beam: The Cuneiform Experience by Jörgen Brandt|url=http://beta.erlangcentral.org/videos/scalable-multi-language-data-analysis-on-beam-the-cuneiform-experience-by-jorgen-brandt/#.WBLlE2hNzIU|website=Erlang Central|accessdate=28 October 2016|archive-url=https://web.archive.org/web/20161002222350/http://beta.erlangcentral.org/videos/scalable-multi-language-data-analysis-on-beam-the-cuneiform-experience-by-jorgen-brandt/#.WBLlE2hNzIU|archive-date=2 October 2016|url-status=dead}}&lt;/ref&gt;&lt;ref&gt;
{{Cite journal 
| last1 = Bux        | first1 = Marc
| last2 = Brandt     | first2 = Jörgen
| last3 = Lipka      | first3 = Carsten
| last4 = Hakimzadeh | first4 = Kamal
| last5 = Dowling    | first5 = Jim
| last6 = Leser      | first6 = Ulf
| title = SAASFEE: scalable scientific workflow execution engine
| journal = Proceedings of the VLDB Endowment
| volume = 8
| number = 12
| pages = 1892–1895
| year = 2015
| url = http://www.vldb.org/pvldb/vol8/p1892-bux.pdf
| doi = 10.14778/2824032.2824094
}}&lt;/ref&gt;&lt;ref&gt;
{{Cite journal 
| last1 = Bessani     | first1 = Alysson
| last2 = Brandt      | first2 = Jörgen
| last3 = Bux         | first3 = Marc
| last4 = Cogo        | first4 = Vinicius
| last5 = Dimitrova   | first5 = Lora
| last6 = Dowling     | first6 = Jim
| last7 = Gholami     | first7 = Ali
| last8 = Hakimzadeh  | first8 = Kamal
| last9 = Hummel      | first9 = Michael
| last10 = Ismail     | first10 = Mahmoud
| last11 = Laure      | first11 = Erwin
| last12 = Leser      | first12 = Ulf
| last13 = Litton     | first13 = Jan-Eric
| last14 = Martinez   | first14 = Roxanna
| last15 = Niazi      | first15 = Salman
| last16 = Reichel    | first16 = Jane
| last17 = Zimmermann | first17 = Karin 
| title =  Biobankcloud: a platform for the secure storage, sharing, and processing of large biomedical data sets
| journal = The First International Workshop on Data Management and Analytics for Medicine and Healthcare (DMAH 2015)
| volume =
| number =
| pages =
| year = 2015
| url = http://www.di.fc.ul.pt/~bessani/publications/dmah15-bbc.pdf
}}
&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Scalable Multi-Language Data Analysis on Beam: The Cuneiform Experience|url=http://www.erlang-factory.com/euc2016/jorgen-brandt|website=Erlang-factory.com|accessdate=28 October 2016}}&lt;/ref&gt;

Cuneiform is influenced by the work of Peter Kelly who proposes functional programming as a model for scientific workflow execution.&lt;ref&gt;{{cite journal
| last1 = Kelly      | first1 = Peter M.
| last2 = Coddington | first2 = Paul D.
| last3 = Wendelborn | first3 = Andrew L.
| year = 2009
| title = Lambda calculus as a workflow model
| journal = Concurrency and Computation: Practice and Experience
| volume = 21
| issue = 16
| pages = 1999–2017
| doi = 10.1002/cpe.1448| s2cid = 10833434
}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
| title = Workflows and extensions to the Kepler scientific workflow system to support environmental sensor data access and analysis
| last1  = Barseghian  | first1  = Derik
| last2  = Altintas    | first2  = Ilkay
| last3  = Jones       | first3  = Matthew B.
| last4  = Crawl       | first4  = Daniel
| last5  = Potter      | first5  = Nathan
| last6  = Gallagher   | first6  = James
| last7  = Cornillon   | first7  = Peter
| last8  = Schildhauer | first8  = Mark
| last9  = Borer       | first9  = Elizabeth T.
| last10 = Seabloom    | first10 = Eric W.
| journal = Ecological Informatics
| volume = 5
| number = 1
| pages = 42–50
| year = 2010
| doi  = 10.1016/j.ecoinf.2009.08.008  | url  = https://escholarship.org/content/qt2q46n1tp/qt2q46n1tp.pdf?t=nivnuu
}}
&lt;/ref&gt;
In this, Cuneiform is distinct from related workflow languages based on [[dataflow programming]] like [[Swift (parallel scripting language)|Swift]].&lt;ref&gt;
{{cite journal
| title = Nextflow enables reproducible computational workflows
| last1 = Di Tommaso | first1 = Paolo
| last2 = Chatzou    | first2 = Maria
| last3 = Floden     | first3 = Evan W
| last4 = Barja      | first4 = Pablo Prieto
| last5 = Palumbo    | first5 = Emilio
| last6 = Notredame  | first6 = Cedric
| journal = Nature Biotechnology
| volume = 35
| number = 4
| pages = 316–319
| year = 2017
| doi = 10.1038/nbt.3820 | pmid = 28398311 | s2cid = 9690740 }}
&lt;/ref&gt;

==External software integration==

External tools and libraries (e.g., [[R (programming language)|R]] or [[Python (programming language)|Python]] libraries) are integrated via a [[foreign function interface]]. In this it resembles, e.g., [[KNIME]] which allows the use of external software through snippet nodes, or [[Apache Taverna|Taverna]] which offers [[BeanShell]] services for integrating [[Java (programming language)|Java]] software. By defining a task in a foreign language it is possible to use the API of an external tool or library. This way, tools can be integrated directly without the need of writing a wrapper or reimplementing the tool.&lt;ref&gt;{{cite web|title=A Functional Workflow Language Implementation in Erlang|url=http://www.erlang-factory.com/static/upload/media/1448992381831050cuneiformberlinefl2015.pdf|accessdate=28 October 2016}}&lt;/ref&gt;

Currently supported foreign programming languages are:
* [[Bash (Unix shell)|Bash]]
* [[Elixir (programming language)|Elixir]]
* [[Erlang (programming language)|Erlang]]
* [[Java (programming language)|Java]]
* [[JavaScript]]
* [[MATLAB]]
* [[GNU Octave]]
* [[Perl]]
* [[Python (programming language)|Python]]
* [[R (programming language)|R]]
* [[Racket (programming language)|Racket]]

Foreign language support for [[AWK]] and [[gnuplot]] are planned additions.

==Type System==

Cuneiform provides a simple, statically checked type system.&lt;ref&gt;
{{ cite journal
| title = Computation semantics of the functional scientific workflow language Cuneiform
| last1 = Brandt | first1 = Jörgen
| last2 = Reisig | first2 = Wolfgang
| last3 = Leser  | first3 = Ulf
| journal = Journal of Functional Programming
| volume = 27
| year = 2017
| doi = 10.1017/S0956796817000119 }}
&lt;/ref&gt; While Cuneiform provides lists as [[compound data type]]s it omits traditional list accessors (head and tail) to avoid the possibility of runtime errors which might arise when accessing the empty list. Instead lists are accessed in an all-or-nothing fashion by only mapping or folding over them. Additionally, Cuneiform omits (at the organizational level) arithmetics which excludes the possibility of division by zero. The omission of any partially defined operation allows to guarantee that runtime errors can arise exclusively in foreign code.

===Base data types===

As base data types Cuneiform provides Booleans, strings, and files. Herein, files are used to exchange data in arbitrary format between foreign functions.

===Records and pattern matching===

Cuneiform provides [[Record_(computer_science)|record]]s (structs) as compound data types. The example below shows the definition of a variable &lt;code&gt;r&lt;/code&gt; being a record with two fields &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a2&lt;/code&gt;, the first being a string and the second being a Boolean.

&lt;pre&gt;
let r : &lt;a1 : Str, a2 : Bool&gt; =
  &lt;a1 = "my string", a2 = true&gt;;
&lt;/pre&gt;

Records can be accessed either via projection or via pattern matching. The example below extracts the two fields &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a2&lt;/code&gt; from the record &lt;code&gt;r&lt;/code&gt;.

&lt;pre&gt;
let a1 : Str = ( r|a1 );

let &lt;a2 = a2 : Bool&gt; = r;
&lt;/pre&gt;

===Lists and list processing===

Furthermore, Cuneiform provides lists as compound data types. The example below shows the definition of a variable &lt;code&gt;xs&lt;/code&gt; being a file list with three elements.

&lt;pre&gt;
let xs : [File] =
  ['a.txt', 'b.txt', 'c.txt' : File];
&lt;/pre&gt;

Lists can be processed with the for and fold operators. Herein, the for operator can be given multiple lists to consume list element-wise (similar to &lt;code&gt;for/list&lt;/code&gt; in [[Racket (programming language)|Racket]], &lt;code&gt;mapcar&lt;/code&gt; in [[Common Lisp]] or &lt;code&gt;zipwith&lt;/code&gt; in [[Erlang (programming language)|Erlang]]).

The example below shows how to map over a single list, the result being a file list.

&lt;pre&gt;
for x &lt;- xs do
  process-one( arg1 = x )
  : File
end;
&lt;/pre&gt;

The example below shows how to zip two lists the result also being a file list.

&lt;pre&gt;
for x &lt;- xs, y &lt;- ys do
  process-two( arg1 = x, arg2 = y )
  : File
end;
&lt;/pre&gt;

Finally, lists can be aggregated by using the fold operator. The following example sums up the elements of a list.

&lt;pre&gt;
  fold acc = 0, x &lt;- xs do
    add( a = acc, b = x )
  end;
&lt;/pre&gt;

==Parallel execution==

Cuneiform is a purely functional language, i.e., it does not support [[Reference (computer science)|mutable references]]. In the consequence, it can use subterm-independence to divide a program into parallelizable portions. The Cuneiform scheduler distributes these portions to worker nodes. In addition, Cuneiform uses a [[Evaluation strategy#Call by name|Call-by-Name evaluation strategy]] to compute values only if they contribute to the computation result. Finally, foreign function applications are [[Memoization|memoized]] to speed up computations that contain previously derived results.

For example, the following Cuneiform program allows the applications of &lt;code&gt;f&lt;/code&gt; and &lt;code&gt;g&lt;/code&gt; to run in parallel while &lt;code&gt;h&lt;/code&gt; is dependent and can be started only when both &lt;code&gt;f&lt;/code&gt; and &lt;code&gt;g&lt;/code&gt; are finished.

&lt;pre&gt;
let output-of-f : File = f();
let output-of-g : File = g();

h( f = output-of-f, g = output-of-g );
&lt;/pre&gt;

The following Cuneiform program creates three parallel applications of the function &lt;code&gt;f&lt;/code&gt; by mapping &lt;code&gt;f&lt;/code&gt; over a three-element list:

&lt;pre&gt;
let xs : [File] =
  ['a.txt', 'b.txt', 'c.txt' : File];

for x &lt;- xs do
  f( x = x )
  : File
end;
&lt;/pre&gt;

Similarly, the applications of &lt;code&gt;f&lt;/code&gt; and &lt;code&gt;g&lt;/code&gt; are independent in the construction of the record &lt;code&gt;r&lt;/code&gt; and can, thus, be run in parallel:

&lt;pre&gt;
let r : &lt;a : File, b : File&gt; =
  &lt;a = f(), b = g()&gt;;
&lt;/pre&gt;

==Examples==

A hello-world script:
&lt;pre&gt;
def greet( person : Str ) -&gt; &lt;out : Str&gt;
in Bash *{
  out="Hello $person"
}*

( greet( person = "world" )|out );
&lt;/pre&gt;
This script defines a task &lt;code&gt;greet&lt;/code&gt; in [[Bash (Unix shell)|Bash]] which prepends &lt;code&gt;"Hello "&lt;/code&gt; to its string argument &lt;code&gt;person&lt;/code&gt;.
The function produces a record with a single string field &lt;code&gt;out&lt;/code&gt;.
Applying &lt;code&gt;greet&lt;/code&gt;, binding the argument &lt;code&gt;person&lt;/code&gt; to the string &lt;code&gt;"world"&lt;/code&gt; produces the record &lt;code&gt;&lt;out = "Hello world"&gt;&lt;/code&gt;. Projecting this record to its field &lt;code&gt;out&lt;/code&gt; evaluates the string &lt;code&gt;"Hello world"&lt;/code&gt;.

Command line tools can be integrated by defining a task in [[Bash (Unix shell)|Bash]]:
&lt;pre&gt;
def samtoolsSort( bam : File ) -&gt; &lt;sorted : File&gt;
in Bash *{
  sorted=sorted.bam
  samtools sort -m 2G $bam -o $sorted
}*
&lt;/pre&gt;
In this example a task &lt;code&gt;samtoolsSort&lt;/code&gt; is defined.
It calls the tool [[SAMtools]], consuming an input file, in BAM format, and producing a sorted output file, also in BAM format.

==Release history==

{| class="wikitable"
|-
! Version !! Appearance !! Implementation Language !! Distribution Platform !! Foreign Languages
|-
! 1.0.0
| May 2014
| [[Java (programming language)|Java]]
| [[Apache Hadoop]]
| Bash, Common Lisp, GNU Octave, Perl, Python, R, Scala
|-
! 2.0.x
| Mar. 2015
| [[Java (programming language)|Java]]
| [[HTCondor]], [[Apache Hadoop]]
| Bash, BeanShell, Common Lisp, MATLAB, GNU Octave, Perl, Python, R, Scala
|-
! 2.2.x
| Apr. 2016
| [[Erlang (programming language)|Erlang]]
| [[HTCondor]], [[Apache Hadoop]]
| Bash, Perl, Python, R
|-
! 3.0.x
| Feb. 2018
| [[Erlang (programming language)|Erlang]]
| Distributed Erlang
| Bash, Erlang, Java, MATLAB, GNU Octave, Perl, Python, R, Racket
|}

In April 2016, Cuneiform's implementation language switched from [[Java (programming language)|Java]] to [[Erlang (programming language)|Erlang]] and, in February 2018, its major distributed execution platform changed from a Hadoop to distributed Erlang. Additionally, from 2015 to 2018 [[HTCondor]] had been maintained as an alternative execution platform.

Cuneiform's surface syntax was revised twice, as reflected in the major version number.

===Version 1===

In its first draft published in May 2014, Cuneiform was closely related to [[Make (software)|Make]] in that it constructed a static data dependency graph which the interpreter traversed during execution. The major difference to later versions was the lack of conditionals, recursion, or static type checking. Files were distinguished from strings by juxtaposing single-quoted string values with a tilde &lt;code&gt;~&lt;/code&gt;. The script's query expression was introduced with the &lt;code&gt;target&lt;/code&gt; keyword. Bash was the default foreign language. Function application had to be performed using an &lt;code&gt;apply&lt;/code&gt; form that took &lt;code&gt;task&lt;/code&gt; as its first keyword argument. One year later, this surface syntax was replaced by a streamlined but similar version.

The following example script downloads a reference genome from an FTP server.

&lt;pre&gt;
declare download-ref-genome;

deftask download-fa( fa : ~path ~id ) *{
    wget $path/$id.fa.gz
    gunzip $id.fa.gz
    mv $id.fa $fa
}*

ref-genome-path = ~'ftp://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes';
ref-genome-id = ~'chr22';

ref-genome = apply(
    task : download-fa
    path : ref-genome-path
    id : ref-genome-id
);

target ref-genome;
&lt;/pre&gt;

===Version 2===

[[File:Cf screenshot.jpg|thumb|Swing-based editor and REPL for Cuneiform 2.0.3]]
The second draft of the Cuneiform surface syntax, first published in March 2015, remained in use for three years outlasting the transition from Java to Erlang as Cuneiform's implementation language. Evaluation differs from earlier approaches in that the interpreter reduces a query expression instead of traversing a static graph. During the time the surface syntax remained in use the interpreter was formalized and simplified which resulted in a first specification of Cuneiform's semantics. The syntax featured conditionals. However, Booleans were encoded as lists, recycling the empty list as Boolean false and the non-empty list as Boolean true. Recursion was added later as a byproduct of formalization. However, static type checking was introduced only in Version 3.

The following script decompresses a zipped file and splits it into evenly sized partitions.

&lt;pre&gt;
deftask unzip( &lt;out( File )&gt; : zip( File ) ) in bash *{
  unzip -d dir $zip
  out=`ls dir | awk '{print "dir/" $0}'`
}*

deftask split( &lt;out( File )&gt; : file( File ) ) in bash *{
  split -l 1024 $file txt
  out=txt*
}*

sotu = "sotu/stateoftheunion1790-2014.txt.zip";
fileLst = split( file: unzip( zip: sotu ) );

fileLst;
&lt;/pre&gt;


===Version 3===

The current version of Cuneiform's surface syntax, in comparison to earlier drafts, is an attempt to close the gap to mainstream functional programming languages. It features a simple, statically checked typesystem and introduces records in addition to lists as a second type of compound data structure. Booleans are a separate base data type.

The following script untars a file resulting in a file list.

&lt;pre&gt;
def untar( tar : File ) -&gt; &lt;fileLst : [File]&gt;
in Bash *{
  tar xf $tar
  fileLst=`tar tf $tar`
}*

let hg38Tar : File =
  'hg38/hg38.tar';

let &lt;fileLst = faLst : [File]&gt; =
  untar( tar = hg38Tar );

faLst;
&lt;/pre&gt;

==References==
{{Reflist|30em}}

[[Category:Programming languages]]
[[Category:Workflow languages]]
[[Category:Functional languages]]
[[Category:Scripting languages]]
[[Category:Linux programming tools]]
[[Category:Hadoop]]
[[Category:Statically typed programming languages]]
[[Category:Cross-platform free software]]</text>
      <sha1>juu9hofb7nr4cjcudwprpe4rqt06eub</sha1>
    </revision>
  </page>
  <page>
    <title>Kyvos</title>
    <ns>0</ns>
    <id>58423370</id>
    <revision>
      <id>994204485</id>
      <parentid>991694629</parentid>
      <timestamp>2020-12-14T16:14:20Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 19 templates: del empty params (11×); del |url-status= (3×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10045" xml:space="preserve">{{Infobox Software
| name                   = Kyvos
| logo                   = Kyvos_Logo.svg
| screenshot             = 
| caption                = 
| author                 = 
| developer              = Kyvos Insights
| released               = June 30, 2015
| latest release version = Kyvos 2020.2
| latest release date    = May 2020
| latest preview version = 
| latest preview date    = 
| operating system       = 
| platform               = [[Amazon Web Services]], [[Microsoft Azure]], [[Google Cloud Platform]], [[Cloudera]], [[Hortonworks]], [[MapR]], [[Apache Hadoop]]
| language               = 
| status                 = 
| genre                  = [[Business_intelligence_software#Cloud-hosted_business_intelligence_software|Cloud Business Intelligence]], [[Big data|Big Data Analytics]]
| license                = [[Proprietary software|Proprietary]]
| website                = {{URL|kyvosinsights.com}}
}}
'''Kyvos''' is a [[business intelligence]] acceleration platform for cloud and [[big data]] platforms developed by an American [[privately held company]] named '''Kyvos Insights'''. The company, headquartered in [[Los Gatos, California]], was founded by [[Praveen Kankariya]], CEO of [[Impetus Technologies]]. The software provides [[Online analytical processing|OLAP]]-based [[multidimensional analysis]] on big data and cloud platforms and was launched officially in June 2015.&lt;ref name="Whiting 2018"&gt;{{Cite news|url=https://www.crn.com/news/applications-os/300077320/startup-kyvos-insights-exits-stealth-offers-olap-for-hadoop-software.htm|title=Startup Kyvos Insights Exits Stealth, Offers OLAP For Hadoop Software|last=Whiting|first=Rick|date=June 30, 2015|work=CRN Magazine|access-date=September 7, 2018}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://adtmag.com/articles/2015/06/30/kyvos-olap.aspx|title=Kyvos Emerges from Stealth with OLAP on Hadoop|last=Ramel|first=David|date=June 30, 2015|work=ADTmag|access-date=September 7, 2018}}&lt;/ref&gt;  In December the same year, the company was listed among the 10 Coolest Big Data Startups of 2015 by [[CRN (magazine)|CRN Magazine]].&lt;ref&gt;{{Cite news|url=https://www.crn.com/slide-shows/applications-os/300079062/the-10-coolest-big-data-startups-of-2015.htm/10|title=The 10 Coolest Big Data Startups Of 2015|last=Whiting|first=Rick|date=December 8, 2015|work=CRN Magazine|access-date=September 7, 2018}}&lt;/ref&gt;

== Technology ==
The software uses [[Online analytical processing|OLAP]] technology to enable [[business intelligence]] on the cloud and big data platforms.&lt;ref&gt;{{Cite news|url=https://www.datanami.com/2015/06/30/kyvos-debuts-olap-for-hadoop/|title=Kyvos Debuts OLAP for Hadoop|last=Woodie|first=Alex|date=June 30, 2015|work=Datanami|access-date=September 7, 2018}}&lt;/ref&gt; In a report published by [[Forrester Research]] in 2016,&lt;ref name="Evelson 2018"&gt;{{Cite news|url=https://www.forrester.com/report/The+Forrester+Wave+Native+Hadoop+BI+Platforms+Q3+2016/-/E-RES129500|title=The Forrester Wave™: Native Hadoop BI Platforms, Q3 2016|last=Evelson|first=Boris|date=September 13, 2016|work=Forrester Research|access-date=September 12, 2018}}&lt;/ref&gt; where they evaluated several native [[Apache Hadoop|Hadoop]] business intelligence (BI) platforms on 22 parameters, Kyvos was referred to as a platform that gave new life to OLAP by bringing it to Hadoop. As per the report, Kyvos enables analysis on Hadoop based on OLAP schemas, aggregations, and predefined drill-down paths. It pre-calculates aggregates at multiple levels of dimensional hierarchies to improve query response times as compared to SQL-on-Hadoop platforms. Users can analyze data through the Kyvos visualization tool or by using other BI platforms.&lt;ref name="Evelson 2018" /&gt;

Kyvos was originally built for Hadoop and later on added support for Cloud platforms such as [[Amazon Web Services]] (AWS), [[Google Cloud Platform|Google Cloud]] &lt;ref name="Rowe 2018"&gt;{{Cite news|url=https://www.destinationcrm.com/Articles/ReadArticle.aspx?ArticleID=117535|title=Kyvos Insights Now Supports Google Cloud|last=Rowe|first=Sam Del|date=April 12, 2017|work=Destination CRM|access-date=September 7, 2018}}&lt;/ref&gt; and [[Microsoft Azure]].&lt;ref&gt;{{Cite news|url=https://www.zdnet.com/article/kyvos-insights-embraces-microsoft-azure-hdinsight/|title=Kyvos Insights embraces Microsoft Azure, HDInsight|last=Brust|first=Andrew|date=March 22, 2016|work=ZD Net|access-date=September 7, 2018}}&lt;/ref&gt; Initially, it supported only MDX queries and integrated with [[data visualization]] tools such as [[Microsoft Excel|Excel]] and [[Tableau Software|Tableau]].&lt;ref&gt;{{Cite news|url=https://www.computerworld.com/article/2942859/big-data/kyvos-serves-up-hadoop-on-cubes.html|title=Kyvos serves up Hadoop on cubes|last=Jackson|first=Joab|date=July 1, 2015|work=Computer World|access-date=September 7, 2018}}&lt;/ref&gt;  In 2017, Kyvos 4.0 added support for SQL connectivity extending integration to other BI tools such as [[BusinessObjects|Business Objects]], [[Cognos]],  [[MicroStrategy]], [[Power BI]] and [[Qlik]].&lt;ref name="Ramel 2018" /&gt; 

In late 2018, Version 5 of the software was built specifically for the cloud with elastic OLAP to provide a cloud native way to scale up and down for changing data workloads.&lt;ref&gt;{{Cite news|url=https://www.datanami.com/this-just-in/kyvos-insights-announces-the-availability-of-kyvos-version-5/|title=Kyvos Insights Announces the Availability of Kyvos Version 5|date=November 15, 2018|work=Datanami|access-date=November 20, 2018}}&lt;/ref&gt; &lt;ref&gt;{{Cite news|url=https://dzone.com/articles/kyvos-releases-version-5|title=Kyvos Releases Version 5|last=Ferrell|first=Lauren|date=November 16, 2018|work=DZone|access-date=November 20, 2018}}&lt;/ref&gt; 

With its 2020.2 release, Kyvos added support for [[Snowflake Inc.|Snowflake]] data warehouse.&lt;ref name=":0"&gt;{{Cite news|date=May 29, 2020|title=Kyvos Announces Snowflake Integration Enabling Multidimensional Analytics on the Cloud|work=Datanami|url=https://www.datanami.com/this-just-in/kyvos-announces-snowflake-integration-enabling-multidimensional-analytics-on-the-cloud/|access-date=June 24, 2020}}&lt;/ref&gt; The product was also made available on [[Microsoft Azure]] marketplace&lt;ref name=":1"&gt;{{Cite news|date=April 28, 2020|title=Kyvos BI Acceleration Platform is Now Available on Azure Marketplace|work=Business Insider|url=https://markets.businessinsider.com/news/stocks/kyvos-bi-acceleration-platform-is-now-available-on-azure-marketplace-1029138759|access-date=June 24, 2020}}&lt;/ref&gt; and [[Amazon Web Services]] marketplace.&lt;ref name=":2"&gt;{{Cite news|date=May 22, 2020|title=Kyvos BI Acceleration Platform Now Available on AWS Marketplace|work=Datanami|url=https://www.datanami.com/this-just-in/kyvos-bi-acceleration-platform-now-available-on-aws-marketplace/|access-date=June 24, 2020}}&lt;/ref&gt;

== Major releases ==

* First release in June 2015.&lt;ref name="Whiting 2018" /&gt;
* Kyvos 2.0 released in June 2016 with support for Amazon Web Services and additional BI tools.&lt;ref name="Gutierrez 2018"&gt;{{Cite news|url=https://insidebigdata.com/2016/06/26/kyvos-insights-delivers-major-new-version-of-big-data-analytics-solution-for-hadoop/|title=Kyvos Insights Delivers Major New Version of Big Data Analytics Solution for Hadoop|last=Gutierrez|first=Daniel|date=June 26, 2016|access-date=September 7, 2018}}&lt;/ref&gt;
* Kyvos 4.0 released in August 2017 with support for SQL queries,&lt;ref name="Ramel 2018"&gt;{{Cite news|url=https://adtmag.com/articles/2017/08/15/kyvos-4-0.aspx|title=Kyvos Self-Service Big Data Platform Boosts SQL Support|last=Ramel|first=David|date=August 15, 2017|work=ADT Mag|access-date=September 7, 2018}}&lt;/ref&gt; enhanced performance, and support for concurrent users.&lt;ref&gt;{{Cite news|url=http://www.dataversity.net/kyvos-4-0-establishes-breakthrough-levels-scale-performance/|title=Kyvos 4.0 Establishes Breakthrough Levels of Scale and Performance|last=Guess|first=A.R.|date=August 16, 2017|work=DATAVERSITY|access-date=September 7, 2018}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://www.crn.com/news/applications-os/300090711/kyvos-expands-scalability-data-security-capabilities-of-its-business-analytics-platform.htm|title=Kyvos Expands Scalability, Data Security Capabilities Of Its Business Analytics Platform|last=Whiting|first=Rick|date=August 17, 2017|work=CRN Magazine|access-date=September 7, 2018}}&lt;/ref&gt;
*Kyvos 5 released in November 2018 with elastic OLAP for native cloud support and data profiling features.&lt;ref&gt;{{Cite news|url=http://www.dbta.com/Editorial/News-Flashes/Kyvos-Version-5-Delivers-New-Enhancements-to-Scale-Growing-Workloads-128596.aspx|title=Kyvos Version 5 Delivers New Enhancements to Scale Growing Workloads|date=November 16, 2018|work=Database Trends and Applications|access-date=November 20, 2018}}&lt;/ref&gt;
*Kyvos 2020.2 released in May 2020 with support for Snowflake cloud data warehouse&lt;ref name=":0" /&gt;  and general availability on Azure&lt;ref name=":1" /&gt; and AWS marketplace.&lt;ref name=":2" /&gt;

== Awards ==
* In June 2018, Kyvos technology won the TDWI's Best Practices Award in the Emerging Technologies and Methods category for a customer implementation.&lt;ref&gt;{{Cite web|url=https://tdwi.org/articles/2018/06/27/tdwi-announces-2018-best-practices-awards-winners.aspx|title=TDWI Announces 2018 Best Practices Awards Winners|date=June 26, 2018|website=TDWI|access-date=September 13, 2018}}&lt;/ref&gt;

== See also ==
*[[Impetus Technologies]]
*[[Comparison of OLAP servers]]

== References ==
{{reflist}}

== External links ==
* {{Official website|https://www.kyvosinsights.com/}}

[[Category:Big data companies]]
[[Category:Big data products]]
[[Category:Companies based in Santa Clara County, California]]
[[Category:Privately held companies based in California]]
[[Category:Online analytical processing]]
[[Category:Business intelligence companies]]
[[Category:Hadoop]]
[[Category:Companies based in Silicon Valley]]
[[Category:Los Gatos, California]]
[[Category:2015 establishments in California]]
[[Category:Business services companies established in 2015]]
[[Category:American companies established in 2015]]</text>
      <sha1>m7isujrs8ilvfsdgmkd4ae83jbv64j0</sha1>
    </revision>
  </page>
  <page>
    <title>Presto (SQL query engine)</title>
    <ns>0</ns>
    <id>48384890</id>
    <revision>
      <id>1000917853</id>
      <parentid>999790084</parentid>
      <timestamp>2021-01-17T10:39:16Z</timestamp>
      <contributor>
        <ip>82.166.199.42</ip>
      </contributor>
      <comment>/* External links */ [[Category:Facebook software]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6255" xml:space="preserve">{{weasel|date=December 2020}}{{Infobox software
| name = Presto
| author = Martin Traverso, Dain Sundstrom, David Phillips, Eric Hwang
| released = {{Start date and age|10 November 2013}}
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| standard = [[SQL]]
| genre = [[Data warehouse]]
| license = [[Apache License]] 2.0
| website = {{URL|https://prestodb.io}} &lt;br&gt; {{URL|https://trino.io}}
}}

'''Presto''' is a high performance, distributed [[SQL]] query engine for big data. Its architecture allows users to query a variety of data sources such as [[Apache Hadoop|Hadoop]], [[Amazon S3|AWS S3]], [[Alluxio]], [[MySQL]], [[Apache Cassandra|Cassandra]], [[Apache Kafka|Kafka]], and [[MongoDB]]. One can even query data from multiple data sources within a single query. Presto is community driven [[open-source software]] released under the [[Apache License]]. 

== History ==
Presto was originally designed and developed at [[Facebook]] for their data analysts to run interactive queries on its large data warehouse in [[Apache Hadoop]]. Before Presto, the data analysts at Facebook relied on [[Apache Hive]] for running SQL analytics on their multi petabyte data warehouse. Hive was deemed too slow&lt;ref name="face2013" /&gt; for Facebook's scale and Presto was invented to fill the gap to run fast queries. Original development started in 2012 and deployed at Facebook later that year. In November 2013, Facebook announced its release as open source 2013.&lt;ref name="face2013"&gt;{{Cite news|url=http://www.computerworld.com/article/2485668/business-intelligence/facebook-goes-open-source-with-query-engine-for-big-data.html|title=Facebook goes open source with query engine for big data|author=Joab Jackson|date=November 6, 2013|work=Computer World|access-date=April 26, 2017}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://gigaom.com/2013/06/06/facebook-unveils-presto-engine-for-querying-250-pb-data-warehouse/|title=Facebook unveils Presto engine for querying 250 PB data warehouse|author=Jordan Novet|date=June 6, 2013|work=Giga Om|access-date=April 26, 2017}}&lt;/ref&gt;  In 2014, [[Netflix]] disclosed they used Presto on 10 [[Petabyte|petabytes]] of data stored in the [[Amazon S3|Amazon Simple Storage Service]] (S3).&lt;ref&gt;{{Cite news|url=http://techblog.netflix.com/2014/10/using-presto-in-our-big-data-platform.html|title=Using Presto in our Big Data Platform on AWS|authors=Eva Tse, Zhenxiao Luo, Nezih Yigitbasi|date=October 7, 2014|work=Netflix technical blog|access-date=April 26, 2017}}&lt;/ref&gt; 

In January 2019, the Presto Software Foundation was announced. The foundation is a not-for-profit organization dedicated to the advancement of the Presto open source distributed SQL query engine.&lt;ref&gt;{{Cite web|url=https://www.prweb.com/releases/presto_software_foundation_launches_to_advance_presto_open_source_community/prweb16070792.htm|title=Presto Software Foundation Launches to Advance Presto Open Source Community|website=PRWeb|access-date=2019-02-01}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://thenewstack.io/prestos-new-foundation-signals-growth-for-the-big-data-sql-engine/|title=Presto's New Foundation Signals Growth for the Big Data SQL Engine|date=2019-01-31|website=The New Stack|language=en-US|access-date=2019-02-01}}&lt;/ref&gt; At the same time, Presto development forked: PrestoDB maintained by Facebook and PrestoSQL maintained by the Presto Software Foundation with some cross pollination of code.

In September 2019, Facebook donated PrestoDB to the [[Linux Foundation]] establishing the Presto Foundation.&lt;ref&gt;{{Cite web|url=https://www.linuxfoundation.org/press-release/2019/09/facebook-uber-twitter-and-alibaba-form-presto-foundation-to-tackle-distributed-data-processing-at-scale/|title=Facebook, Uber, Twitter and Alibaba form Presto Foundation to Tackle Distributed Data Processing at Scale|access-date=2019-11-12}}&lt;/ref&gt; Neither the creators of Presto, nor the top contributors and committers, were invited to join this foundation.&lt;ref&gt;{{Cite news|url=https://github.com/trinodb/trino/issues/380#issuecomment-557691046|title=What's the relationship between prestosql and prestodb?|date=2019-11-22}}&lt;/ref&gt; 

In December 2020, PrestoSQL was rebranded as Trino.&lt;ref&gt;{{Cite web|url=https://trino.io/blog/2020/12/27/announcing-trino.html|title=We’re rebranding PrestoSQL as Trino|date=2020-12-27}}&lt;/ref&gt;

== Architecture ==
Presto’s architecture is very similar to a classic [[database management system]] using [[cluster computing]] ([[massively parallel|MPP]]). It can be visualized as one coordinator node working in sync with multiple worker nodes. Clients submit SQL statements that get parsed and planned following which parallel tasks are scheduled to workers. Workers jointly process rows from the data sources and produce results that are returned to the client. Compared to the original [[Apache Hive]] execution model which used the Hadoop [[MapReduce]] mechanism on each query, Presto does not write intermediate results to disk resulting in a significant speed improvement. Presto is written in the [[Java (programming language)|Java programming language]].

A single Presto query can combine data from multiple sources. Presto offers connectors to data sources including files in [[Alluxio]], [[Apache Hadoop#HDFS|Hadoop Distributed File System]], [[Amazon S3]], [[MySQL]], [[PostgreSQL]], [[Microsoft SQL Server]], [[Amazon Redshift]], [[Apache Kudu]], [[Apache Phoenix]], [[Apache Kafka]], [[Apache Cassandra]], [[Apache Accumulo]], [[MongoDB]] and [[Redis]]. Unlike other Hadoop distribution-specific tools, such as [[Apache Impala]], Presto can work with any flavor of Hadoop or without it. Presto supports separation of compute and storage and may be deployed both on premises and in the [[Cloud computing|cloud]].

==See also==
* [[Big data]]
* [[Data Intensive Computing]]
* [[Apache Drill]]

== References ==
{{Reflist}}&lt;br/&gt;

== External links ==

* [https://trino.io/foundation.html Trino Software Foundation (formerly Presto Software Foundation)]
* [https://github.com/prestodb/foundation Presto Foundation] (under the [[Linux Foundation]]) 

[[Category:SQL]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Cloud platforms]]
[[Category:Facebook software]]</text>
      <sha1>k4e02xovk1gsvw2r8rif10ylrnj8z1s</sha1>
    </revision>
  </page>
  <page>
    <title>WANdisco</title>
    <ns>0</ns>
    <id>23764543</id>
    <revision>
      <id>993401482</id>
      <parentid>959390793</parentid>
      <timestamp>2020-12-10T12:31:03Z</timestamp>
      <contributor>
        <username>AshMo90</username>
        <id>40602470</id>
      </contributor>
      <comment>/* History */ ref supplied</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13505" xml:space="preserve">{{Infobox company
| name             = WANdisco, plc.
| logo             = WANdisco logo.svg
| type             = [[Public limited company]]
| traded_as        = {{AIM|WAND}} 
| founder          = [[David James Richards|David Richards]], Dr. Yeturu Aahlad&lt;ref&gt;{{cite web |url=https://www.wandisco.com/about-us |title=WANdisco About Us |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt;
| key_people       = David Richards ([[Chairman]] &amp; [[Chief executive officer|CEO]])&lt;br&gt;Erik Miller([[Chief financial officer|CFO]])&lt;br&gt;Yeturu Aahlad ([[Entrepreneur|Inventor]])
| industry         = [[Big data]]&lt;br /&gt;[[Cloud Computing]]&lt;br /&gt;[[Configuration management]]&lt;br /&gt;[[Software]]
| products         = WANdisco Fusion &lt;br /&gt;SVN MultiSite Plus &lt;br /&gt;Git MultiSite &lt;br /&gt;Gerrit MultiSite &lt;br /&gt;Access Control Plus
| num_employees    = circa 180 &lt;ref&gt;{{cite web |url=https://www.linkedin.com/company/wandisco |title=WANdisco LinkedIn |access-date=2018-11-12}}&lt;/ref&gt;
| foundation       = [[San Ramon, California|San Ramon]], [[California]], U.S. ({{Start date|2005}})
| location_city    = [[Sheffield]], [[United Kingdom|U.K.]]/[[San Ramon, California|San Ramon]], [[United States|U.S.]]
| location_country = 
| homepage         =  {{URL |www.wandisco.com}}
}}

'''WANdisco, plc.''', dually headquartered in [[Sheffield]], England and [[San Ramon, California]] in the US, is a public [[software company]] specialized in the area of [[distributed computing]]. It has development offices in [[San Ramon, California]]; [[Sheffield]], England; and [[Belfast]], Northern Ireland.  WANdisco is a corporate contributor to [[Apache Hadoop|Hadoop]], [[Apache Subversion|Subversion]] and other [[open source]] projects.

==History==
The name WANdisco is an acronym for [[wide area network]] [[distributed computing]]. Initially offering a [[Replication (computer science)|replication]] solution for distributed teams using the [[Concurrent Versions System]] (CVS), this was expanded to include [[Apache Subversion]] with SVN MultiSite Plus in 2006, [[Git (software)|Git]] with Git MultiSite in 2013 and [[Gerrit (software)|Gerrit]] with Gerrit MultiSite in 2014.{{Citation needed|date=May 2020}}

In 2012, WANdisco acquired AltoStor, and entered the Big Data market with its Non-Stop Hadoop product.&lt;ref&gt;{{Cite web|title=Big Data Consolidation: WANdisco Buys AltoStor For $5.1M To Beef Up Its Apache Hadoop Cred|url=https://social.techcrunch.com/2012/11/19/big-data-consolidation-wandisco-buys-altostor-to-beef-up-apache-hadoop-cred/|access-date=2020-12-10|website=TechCrunch|language=en-US}}&lt;/ref&gt; AltoStor's founders, Dr. Konstantin Shvachko and Jagane Sundar, joined WANdisco as part of the acquisition, and helped develop the company's next generation Hadoop product released in 2015, WANdisco Fusion.{{Citation needed|date=May 2020}}

==Technology==
WANdisco's Distributed Coordination Engine (DConE) is the shared component for WANdisco [[Cluster (computing)|clustering]] products.&lt;ref&gt;{{cite journal |title=The Distributed Coordination Engine (DConE) |work=White paper |url=https://www.wandisco.com/assets/bltdd38b4158b687dd8/WANdisco_DConE_White_Paper.pdf |year=2014 |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt; The DConE system allows multiple instances of the same application to operate on independent hardware without sharing any resources. All of the application servers are kept in synchronisation by DConE regardless of whether the servers are on the same LAN or globally separated and accessible only over a [[wide area network]] (WAN).

WANdisco's replication technology was the work of Yeturu Aahlad, who had previously worked for Sun, Netscape and IBM, and was involved in developing the [[CORBA|CORBA Framework]].&lt;ref&gt;{{cite journal |title=Methods and apparatus for managing deactivation and shutdown of a server |publisher=[[United States Patent and Trademark Office|US Patent Office]] |work=White paper |url=https://patents.google.com/patent/US5907675A/en |date=1995-03-22 |access-date=2018-11-12}}&lt;/ref&gt; Aahlad theorized a model for effective [[Replication (computer science)|Active replication]] over a WAN. In the development of DConE, WANdisco has taken the [[Paxos (computer science)|Paxos]] algorithm as a baseline and added innovations relevant to mission-critical, high transaction volume distributed environments.{{Citation needed|date=May 2020}}

WANdisco provides replication products for  [[Concurrent Versions System|CVS]], [[Apache Subversion]], [[Git (software)|Git]], [[Gerrit (software)|Gerrit]], [[Apache Hadoop]], [[Amazon Web Services]], [[Microsoft Azure]], [[Google Cloud Platform]]. In addition, the company offers support, consultancy and training services.

The company's website lists companies such as [[Arm Holdings|ARM]], [[Avaya]], [[Bally Technologies]], [[Barclays]], [[BlackRock]], [[Robert Bosch GmbH|Bosch]], [[Cisco Systems|Cisco]], [[Dell EMC]], [[Disney]], [[Fujitsu]], [[General Electric]], [[Honda]], [[Juniper Networks]] and [[Pitney Bowes]].&lt;ref&gt;{{cite web |url=https://www.wandisco.com/customers |title=WANdisco Customers |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt;

===IBM OEM===
In April 2016, WANdisco announced that [[IBM]] had signed a deal to [[Original equipment manufacturer|OEM]] WANdisco Fusion.&lt;ref&gt;{{cite web |url=https://www.theregister.co.uk/2016/04/29/ibm_oems_wandisco_replication_product/ |title=IBM goes to WANdisco |author=Chris Mellor |date=2016-04-29 |publisher=[[The Register|The Register/Situation Publishing]] |access-date=2018-11-12}}&lt;/ref&gt;  The deal allows IBM to rebrand Fusion as "IBM Big Replicate" and plays an important role in the IBM Big Data and Cloud Computing strategy, including movement of data between [[on-premises software]] and Cloud.&lt;ref&gt;{{cite web |url=http://www-03.ibm.com/software/products/en/ibm-big-replicate |title=IBM Big Replicate |publisher=[[IBM]] |access-date=2018-11-12}}&lt;/ref&gt;

===Blockchain===
In July 2018, WANdisco announced that it had filed a new [[patent]] in [[Blockchain]].&lt;ref&gt;{{cite press release |url= https://wandisco.com/news-events/press-releases/wandisco-announces-filing-new-blockchain-patent-and-vp-engineering |title=WANdisco announces filing of new blockchain patent and VP of Engineering |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=2018-07-25 |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt; The company claims that the patent "enables effective permissioned blockchain transactions with an underlying algorithmic mechanism. This mechanism enables throughput to be achieved that is orders of magnitude higher than public blockchains."

===Defunct products===
In 2011 WANdisco announced uberSVN, a deployment of Apache Subversion which included a web based management console and the ability to add additional [[application lifecycle management]] features.&lt;ref&gt;{{cite web |url=http://www.drdobbs.com/parallel/wandisco-do-si-dos-apache-subversion-wit/229402130 |title=WANdisco Do-Si-Dos Apache Subversion With uberSVN |author=Adrian Bridgwater |date=2011-04-22 |publisher=Dr. Dobb's |access-date=2018-11-12}}&lt;/ref&gt; The uberSVN download was available through mid-2013.&lt;ref&gt;{{cite web |title=uberSVN: The Open ALM Platform for Subversion |url= http://www.wandisco.com/ubersvn |publisher=WANdisco, Inc. |url-status=dead |archive-url= https://web.archive.org/web/20130717170753/http://www.wandisco.com/ubersvn |archive-date=2013-07-17 |access-date=2018-11-12 }}&lt;/ref&gt;

==Open source contributions==
In September 2013 WANdisco announced it is an official sponsor of the UC Berkeley [[AMPLab]], a five-year collaborative effort at the [[University of California, Berkeley]]&lt;!-- where Spark, Shark and Mesos have been developed  what are those?? --&gt;.&lt;ref&gt;{{cite press release |url=https://www.wandisco.com/news-events/press-releases/wandisco-announces-official-sponsorship-of-uc-berkeley-amplab |title=WANdisco Announces Official Sponsorship of UC Berkeley AMPLab |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=2013-09-05 |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt;

===Hadoop===
WANdisco has one [[Apache Hadoop]] committer on staff: Jagane Sundar.
In February 2013 WANdisco released a free distribution of Hadoop containing additional components developed by WANdisco.&lt;ref&gt;{{cite web |url=https://siliconangle.com/2013/02/11/wandisco-releases-free-hadoop-distro/ |title=WANdisco Releases Free Hadoop Distro |author=Maria Deutscher |date=2013-02-11 |publisher=SiliconANGLE Media Inc. |access-date=2018-11-12}}&lt;/ref&gt;

===Subversion===
WANdisco was involved in the [[Apache Subversion]] open source project from 2008&lt;ref name=apache-svn-wandisco /&gt; through 2015. They employed several contributors to work on the Subversion project during that time.&lt;ref&gt;{{cite press release |url=https://www.wandisco.com/news-events/press-releases/wandisco-sponsors-the-apache-software-foundation-for-a-second-year-and-welcomes-core-subversion-developers-to-its-team |title=WANdisco sponsors the Apache Software Foundation for a second year and welcomes core Subversion developers to its team |author=&lt;!--Staff writer(s); no by-line.--&gt;  |date=2012-07-26 |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt;

====Server and client binaries====
WANdisco provides Subversion binary downloads for [[Windows]], [[CentOS]], [[Debian]], [[Oracle Linux]], [[RHEL]], [[SUSE Linux]], [[Ubuntu (operating system)|Ubuntu]], [[OS X (operating system)|Mac OS X]] and [[Solaris (operating system)|Solaris]] via its website.&lt;ref&gt;{{cite web |url=https://www.wandisco.com/resource-library |title=WANdisco Resource Library |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt; These binaries use the default package management system for each Linux distribution.

====Project announcements====
In December 2010, WANdisco announced its intention to develop some features for the Subversion project,&lt;ref&gt;{{cite press release  |url=http://www.wandisco.com/news/press-releases/wandisco-shakes-software-change-management-overhaul-subversion |title=WANdisco Shakes Up Software Change Management With Overhaul of Subversion |date=2010-12-20 |place=San Ramon, CA |publisher=WANdisco, Inc. |url-status=dead |archive-url=https://web.archive.org/web/20160421071437/http:/www.wandisco.com/press-releases/wandisco-shakes-software-change-management-overhaul-subversion |archive-date=2016-04-21 |access-date=2018-11-12}}&lt;/ref&gt; specifically aimed at improving branching and merging functionality.

The [[Apache Foundation]] and some Subversion developers said  the announcement contained unfounded claims and insinuations about community involvement and the lack of development on these features.&lt;ref&gt;{{Citation |url=http://blog.red-bean.com/sussman/?p=475 |title=WANdisco, ur doin it rong |author=Ben Collins-Sussman |date=2011-01-03 |access-date=2018-11-12 |quote=He’s insulted two-thirds of the active developers (and embarrassed his own employees) by declaring them to be {{sic|nolink=y|incompetant}} stewards. There’s no simpler way to garner hate and come off like an ass than to say 'everyone move aside and let me fix this'}}&lt;/ref&gt; According to Apache, these features were already being worked on at the time.&lt;ref name=apache-svn-wandisco&gt;{{Citation |url=https://news.slashdot.org/story/11/01/04/0248206/Apache-Subversion-To-WANdisco-Inc-Get-Real |title=Apache Subversion To WANdisco, Inc: Get Real |journal=Slashdot.org |publisher=[[Slashdot|SlashdotMedia]] |date=2011-01-04 |access-date=2018-11-12}}&lt;/ref&gt;&lt;ref&gt;{{Citation |url=https://blogs.apache.org/foundation/entry/apache_subversion_to_wandisco_1 |title=Apache Subversion to WANdisco: +1 on the code contributions, −1 on the attitude |journal=The Apache Software Foundation Blog |date=2011-01-03 |author=Apache Software Foundation |authorlink=Apache Software Foundation |access-date=2018-11-12 |quote=WANdisco's false implication that it is in some kind of steering position in Subversion development discredits the efforts of other contributors and companies.}}&lt;/ref&gt; David Richards from WANdisco clarified this position to the Subversion community&lt;ref&gt;{{Citation |url=http://blogs.wandisco.com/2011/01/04/110/ |title=Subversion Politics |author=David Richards |journal=WANdisco Blog |date=2011-01-04 |publisher=WANdisco, Inc. |url-status=dead |archive-url=https://web.archive.org/web/20150610121138/http:/blogs.wandisco.com/2011/01/04/110/ |archive-date=2015-06-10 |access-date=2018-11-12}}&lt;/ref&gt; and followed up by announcing WANdisco's sponsorship and ongoing support for the work of the Apache Software Foundation.&lt;ref&gt;{{cite press release |url=https://www.wandisco.com/news-events/press-releases/wandisco-sponsors-apache-software-foundation |title=WANdisco sponsors Apache Software Foundation |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=2011-02-01 |publisher=WANdisco, Inc. |access-date=2018-11-12}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* [https://www.wandisco.com/ WANdisco web site]

{{DEFAULTSORT:Wandisco}}
[[Category:Software companies of England]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Companies based in San Ramon, California]]
[[Category:Software companies established in 2005]]
[[Category:2005 establishments in California]]
[[Category:Companies listed on the Alternative Investment Market]]
[[Category:Companies based in Sheffield]]
[[Category:Big data companies]]
[[Category:Cloud computing providers]]
[[Category:Hadoop]]
[[Category:2012 initial public offerings]]
[[Category:Software companies of the United States]]</text>
      <sha1>r7itik9gkeerab717icszn5hw5k204l</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ZooKeeper</title>
    <ns>0</ns>
    <id>26039352</id>
    <revision>
      <id>995902632</id>
      <parentid>993361735</parentid>
      <timestamp>2020-12-23T14:13:19Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 12 templates: hyphenate params (1×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7772" xml:space="preserve">{{Infobox software
| name = Apache ZooKeeper
| logo = [[File:Apache ZooKeeper Logo.svg|250px|Apache ZooKeeper Logo]]
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| latest release version = 3.6.2
| latest release date = {{Start date and age|2020|09|09}}&lt;ref name="releases"&gt;{{cite web|url=https://zookeeper.apache.org/releases.html|title=Apache ZooKeeper - Releases|access-date=10 December 2020}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| platform = 
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;zookeeper.git|ZooKeeper Repository}}
| programming language = [[Java (programming language)|Java]]
| genre = [[Distributed computing]]
| license = [[Apache License 2.0]]
| website = {{URL|https://zookeeper.apache.org}}
}}
'''Apache ZooKeeper''' is a software project of the [[Apache Software Foundation]]. It is essentially a [[service (systems architecture)|service]] for [[distributed computing|distributed systems]] offering a [[hierarchical database model|hierarchical]] [[Key-value database|key-value store]], which is used to provide a distributed [[configuration management|configuration service]], [[synchronization (computer science)|synchronization service]], and [[directory service|naming registry]] for large distributed systems (see ''[[#Use cases|Use cases]]'').&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/ZOOKEEPER/|title=Index - Apache ZooKeeper - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-08-26}}&lt;/ref&gt; ZooKeeper was a sub-project of [[Hadoop]] but is now a [[Apache Software Foundation#Projects|top-level Apache project]] in its own right.

==Overview==
ZooKeeper's [[software architecture|architecture]] supports [[High-availability cluster|high availability]] through [[redundancy (engineering)|redundant services]]. The clients can thus ask another ZooKeeper leader if the first fails to answer. ZooKeeper nodes store their data in a hierarchical name space, much like a file system or a [[Tree (data structure)|tree]] data structure. Clients can read from and write to the nodes and in this way have a shared configuration service. ZooKeeper can be viewed as an [[atomic broadcast]] system, through which updates are [[total order|totally ordered]]. The ZooKeeper Atomic Broadcast (ZAB) protocol is the core of the system.&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/ZOOKEEPER/ProjectDescription|title=Zookeeper Overview}}&lt;/ref&gt;

ZooKeeper is used by companies including [[Yelp]], [[Rackspace]], [[Yahoo!]],&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/ZooKeeper/PoweredBy |title=ZooKeeper/Powered By |access-date=2012-01-25 |archive-url=https://web.archive.org/web/20131209063307/http://wiki.apache.org/hadoop/ZooKeeper/PoweredBy |archive-date=2013-12-09 |url-status=dead }}&lt;/ref&gt; [[Odnoklassniki]], [[Reddit]],&lt;ref&gt;{{cite web|url=https://www.reddit.com/r/announcements/comments/4y0m56/why_reddit_was_down_on_aug_11/|title=Why Reddit was down on Aug 11}}&lt;/ref&gt; [[NetApp]] [[SolidFire]],&lt;ref&gt;{{Cite news|url=https://newsroom.netapp.com/blogs/5-big-daas-challenges-and-how-to-overcome-them/|title=5 Big DaaS Challenges and How to Overcome Them {{!}} NetApp Newsroom|date=2016-06-20|work=NetApp Newsroom|access-date=2017-05-24|language=en-US}}{{Dead link|date=October 2019 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; [[Facebook]],&lt;ref&gt;{{Cite news|url=https://code.fb.com/data-infrastructure/location-aware-distribution-configuring-servers-at-scale/|title=Location-Aware Distribution: Configuring servers at scale|date=2018-07-19|work=Facebook Code|access-date=2018-07-20|language=en-US}}&lt;/ref&gt; [[Twitter]]&lt;ref&gt;{{Cite news|url=https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/zookeeper-at-twitter.html|title=ZooKeeper at Twitter|date=2018-10-11|work=Twitter Engineering Blog|access-date=2018-12-08|language=en-US}}&lt;/ref&gt; and [[eBay]]  as well as [[Open-source software|open source]] [[enterprise search]] systems like [[Solr]].&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/solr/SolrCloud|title=SolrCloud}}&lt;/ref&gt;

ZooKeeper is modeled after Google's Chubby lock service&lt;ref&gt;{{Cite journal|last=Burrows|first=Mike|date=2006|title=The Chubby lock service for loosely-coupled distributed systems|url=https://research.google/pubs/pub27897/|journal=7th USENIX Symposium on Operating Systems Design and Implementation (OSDI)}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://research.google/pubs/pub33002/|title=Paxos Made Live - An Engineering Perspective (2006 Invited Talk)|last=Chandra|first=Tushar Deepak|last2=Griesemer|first2=Robert|date=2007|website=Google Research|language=en|access-date=2020-03-03|last3=Redstone|first3=Joshua}}&lt;/ref&gt; and was originally developed at Yahoo! for streamlining the processes running on big-data clusters by storing the status in local log files on the ZooKeeper servers. These servers communicate with the client machines to provide them the information. ZooKeeper was developed in order to fix the bugs that occurred while deploying distributed big-data applications. 

Some of the prime features of Apache ZooKeeper are:
* Reliable System: This system is very reliable as it keeps working even if a node fails.
* Simple Architecture: The architecture of ZooKeeper is quite simple as there is a shared hierarchical namespace which helps coordinating the processes.
* Fast Processing: ZooKeeper is especially fast in "read-dominant" workloads (i.e. workloads in which reads are much more common than writes).
* Scalable: The performance of ZooKeeper can be improved by adding nodes.

==Architecture==
Some common terminologies regarding the ZooKeeper architecture:
* Node: The systems installed on the cluster
* ZNode: The nodes where the status is updated by other nodes in cluster
* Client applications: The tools that interact with the distributed applications
* Server applications: Allows the client applications to interact using a common interface

The services in the cluster are replicated and stored on a set of servers (called an "ensemble"), each of which maintains an in-memory database containing the entire data tree of state as well as a transaction log and snapshots stored persistently. Multiple client applications can connect to a server, and each client maintains a TCP connection through which it sends requests and heartbeats and receives responses and watch events for monitoring.&lt;ref&gt;{{cite web|url=https://zookeeper.apache.org/doc/current/zookeeperOver.html|title= Zookeeper}}&lt;/ref&gt;

==Use cases==
Typical use cases for ZooKeeper are: 
* [[Name service|Naming service]]
* [[Configuration management]]
* [[Synchronization (computer science)|Data Synchronization]]
* [[Leader election]]
* [[Message queue]]
* [[Notification system]]

== Client libraries ==
In addition to the client libraries included with the ZooKeeper distribution, a number of third-party libraries such as Apache Curator and Kazoo are available that make using ZooKeeper easier, add additional functionality, additional programming languages, etc.

==Apache projects using ZooKeeper==
* [[Apache Hadoop]]
* [[Apache Accumulo]]
* [[Apache HBase]]
* [[Apache Hive]]
* [[Apache Kafka]]
* [[Apache Solr]]
* [[Apache Spark]]
* [[Apache NiFi]]
* [[Apache Druid]]
* [[Apache Helix]]

==See also==
{{Portal|Computer programming|Free and open-source software}}
* [[Hadoop]]

==References==
{{Reflist}}

==External links==
*{{Official website|https://zookeeper.apache.org}}

{{Apache Software Foundation}}
{{Java (Sun)}}

{{DEFAULTSORT:Zookeeper}}
[[Category:Apache Software Foundation projects|ZooKeeper]]
[[Category:Configuration management]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Hadoop]]</text>
      <sha1>efuoewrxb5ymiywhxf08y4o3gj0bllm</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Beam</title>
    <ns>0</ns>
    <id>51237252</id>
    <revision>
      <id>1000550509</id>
      <parentid>999175203</parentid>
      <timestamp>2021-01-15T16:08:31Z</timestamp>
      <contributor>
        <ip>66.65.132.230</ip>
      </contributor>
      <comment>/* Timeline */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7360" xml:space="preserve">{{Advert|date=January 2020}}
{{Infobox software
| name = Apache Beam
| logo = Beam-logo-full-color-name-right-200-autocrop.png
| caption = Beam logo
| author = [[Google]]
| developer = [[Apache Software Foundation]]
| released = {{Start date and age|2016|06|15}}
| latest release version = 2.26.0
| latest release date = {{Start date and age|2020|12|11}}&lt;ref&gt;{{citation|url=https://beam.apache.org/blog/beam-2.26.0/|title=Apache Beam 2.26.0|access-date=11 December 2020}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| size = 
| repo = {{URL|https://github.com/apache/beam|Beam Repository}}
| programming language = [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Go_(programming_language)|Go]]
| genre = 
| license = [[Apache License 2.0]]
| website = {{URL|https://beam.apache.org}}
}}
'''Apache Beam''' is an [[Open-source software|open source]] unified programming model to define and execute data processing [[Pipeline (software)|pipelines]], including [[Extract, transform, load|ETL]], [[Batch processing|batch]] and [[Stream processing|stream]] (continuous) processing.&lt;ref name="Woodie2016"&gt;{{cite web|last1=Woodie|first1=Alex|title=Apache Beam's Ambitious Goal: Unify Big Data Development|url=https://www.datanami.com/2016/04/22/apache-beam-emerges-ambitious-goal-unify-big-data-development/|website=Datanami|access-date=4 August 2016|date=22 April 2016}}&lt;/ref&gt; Beam Pipelines are defined using one of the provided [[Software development kit|SDKs]] and executed in one of the Beam’s supported ''runners'' ([[distributed processing]] back-ends) including [[Apache Flink]], [[Apache Samza]], [[Apache Spark]], and [[Google Cloud Dataflow]].&lt;ref name="google.com"&gt;{{cite web|url=https://cloud.google.com/dataflow/|title=Cloud Dataflow - Batch &amp; Stream Data Processing}}&lt;/ref&gt;

==History==
Apache Beam&lt;ref name="google.com"/&gt; is one implementation of the Dataflow model paper.&lt;ref name="Akidau2015"&gt;{{cite journal|last1=Akidau|first1=Tyler|last2=Schmidt|first2=Eric|last3=Whittle|first3=Sam|last4=Bradshaw|first4=Robert|last5=Chambers|first5=Craig|last6=Chernyak|first6=Slava|last7=Fernández-Moctezuma|first7=Rafael J.|last8=Lax|first8=Reuven|last9=McVeety|first9=Sam|last10=Mills|first10=Daniel|last11=Perry|first11=Frances|title=The dataflow model|journal=Proceedings of the VLDB Endowment|date=1 August 2015|volume=8|issue=12|pages=1792–1803|doi=10.14778/2824032.2824076|url=http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf|access-date=4 August 2016}}&lt;/ref&gt; The Dataflow model is based on previous work on distributed processing abstractions at Google, in particular on FlumeJava&lt;ref name="Chambers2010"&gt;{{cite journal|last1=Chambers |first1=Craig |last2=Raniwala |first2=Ashish |last3=Perry |first3=Frances |last4=Adams |first4=Stephen |last5=Henry |first5=Robert R. |last6=Bradshaw |first6=Robert |last7=Weizenbaum |first7=Nathan |title=FlumeJava: Easy, Efficient Data-parallel Pipelines |journal=Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation |date=1 January 2010 |pages=363–375 |doi=10.1145/1806596.1806638 |url=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf |archive-url=https://web.archive.org/web/20160923141630/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35650.pdf |url-status=dead |archive-date=23 September 2016 |access-date=4 August 2016 |publisher=ACM |s2cid=14888571 }}&lt;/ref&gt; and Millwheel.&lt;ref name="Akidau2013"&gt;{{cite journal|last1=Akidau |first1=Tyler |last2=Whittle |first2=Sam |last3=Balikov |first3=Alex |last4=Bekiroğlu |first4=Kaya |last5=Chernyak |first5=Slava |last6=Haberman |first6=Josh |last7=Lax |first7=Reuven |last8=McVeety |first8=Sam |last9=Mills |first9=Daniel |last10=Nordstrom |first10=Paul |title=MillWheel |journal=Proceedings of the VLDB Endowment |date=27 August 2013 |volume=6 |issue=11 |pages=1033–1044 |doi=10.14778/2536222.2536229 |url=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf |archive-url=https://web.archive.org/web/20160201091359/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41378.pdf |url-status=dead |archive-date=1 February 2016 |access-date=4 August 2016 }}&lt;/ref&gt;&lt;ref name="Pointer2016"&gt;{{cite web|last1=Pointer|first1=Ian|title=Apache Beam wants to be uber-API for big data|url=http://www.infoworld.com/article/3056172/application-development/apache-beam-wants-to-be-uber-api-for-big-data.html|publisher=InfoWorld|access-date=4 August 2016}}&lt;/ref&gt;

Google released an open SDK implementation of the Dataflow model in 2014 and an environment to execute Dataflows locally (non-distributed) as well as in the [[Google Cloud Platform]] service.

In 2016 Google donated the core SDK as well as the implementation of a local runner, and a set of IOs (data connectors) to access [[Google Cloud Platform]] data services to the [[Apache Software Foundation]]. Other companies and members of the community have contributed runners for existing distributed execution platforms, as well as new IOs to integrate the Beam Runners with existing Databases, Key-Value stores and Message systems. Additionally new [[Domain-specific language|DSLs]] have been proposed to support specific domain needs on top of the Beam Model{{Citation needed|date=June 2019}}.

===Timeline===

{| class="wikitable"
|-
! Version
! Release date
|-
| {{Version|c|2.27.0}}
| 2021-01-08
|-
| {{Version|o|2.26.0}}
| 2020-12-11
|-
| {{Version|o|2.25.0}}
| 2020-10-23
|-
| {{Version|o|2.24.0}}
| 2020-09-18
|-
| {{Version|o|2.23.0}}
| 2020-07-29
|-
| {{Version|o|2.22.0}}
| 2020-06-08
|-
| {{Version|o|2.21.0}}
| 2020-05-27
|-
| {{Version|o|2.20.0}}
| 2020-04-15
|-
| {{Version|o|2.19.0}}
| 2020-02-04
|-
| {{Version|o|2.18.0}}
| 2020-01-23
|-
| {{Version|o|2.17.0}}
| 2020-01-06
|-
| {{Version|o|2.16.0}}
| 2019-10-07
|-
| {{Version|o|2.15.0}}
| 2019-08-22
|-
| {{Version|o|2.14.0}}
| 2019-08-01
|-
| {{Version|o|2.13.0}}
| 2019-05-22
|-
| {{Version|o|2.12.0}}
| 2019-04-25
|-
| {{Version|o|2.11.0}}
| 2019-02-26
|-
| {{Version|o|2.10.0}}
| 2019-02-01
|-
| {{Version|o|2.9.0}}
| 2018-12-13
|-
| {{Version|o|2.8.0}}
| 2018-10-29
|-
| {{Version|o|2.7.0 (LTS)}}
| 2018-10-03
|-
| {{Version|o|2.6.0}}
| 2018-08-08
|-
| {{Version|o|2.5.0}}
| 2018-06-26
|-
| {{Version|o|2.4.0}}
| 2018-03-20
|-
| {{Version|o|2.3.0}}
| 2018-01-30
|-
| {{Version|o|2.2.0}}
| 2017-12-02
|-
| {{Version|o|2.1.0}}
| 2017-08-23
|-
| {{Version|o|2.0.0}}
| 2017-05-17
|-
| {{Version|o|0.6.0}}
| 2017-03-11
|-
| {{Version|o|0.5.0}}
| 2017-02-02
|-
| {{Version|o|0.4.0}}
| 2016-12-29
|-
| {{Version|o|0.3.0}}
| 2016-10-31
|-
| {{Version|o|0.2.0}}
| 2016-08-08
|-
| {{Version|o|0.1.0}}
| 2016-06-15
|-
| colspan="2" | &lt;small&gt;{{Version |l |show=111100}}&lt;/small&gt;
|}

==See also==
*[[List of Apache Software Foundation projects]]

==References==
{{Reflist|30em}}

{{Apache Software Foundation}}
{{Google FOSS}}

{{DEFAULTSORT:Beam}}
[[Category:Apache Software Foundation]]
[[Category:Apache Software Foundation projects]]
[[Category:Big data products]]
[[Category:Cluster computing]]
[[Category:Distributed stream processing]]
[[Category:Google software]]
[[Category:Hadoop]]
[[Category:Java platform]]
[[Category:Free software programmed in Java (programming language)]]</text>
      <sha1>6yzk6kkle5rqmuch1x1u9rjnhoxpx31</sha1>
    </revision>
  </page>
  <page>
    <title>Apache CarbonData</title>
    <ns>0</ns>
    <id>61082569</id>
    <revision>
      <id>972213774</id>
      <parentid>915146012</parentid>
      <timestamp>2020-08-10T20:55:13Z</timestamp>
      <contributor>
        <username>Ghettoblaster</username>
        <id>6603820</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2645" xml:space="preserve">{{Primary sources|date=June 2019}}
{{Infobox software
| name = Apache CarbonData
| logo = [[File:Apache_CarbonData_Logo.svg|250px|Apache CarbonData Logo]]
| screenshot = 
| caption = Apache CarbonData
| developer = 
| released = {{Start date and age|2013|df=yes}} &lt;ref name="auto"&gt;{{Cite web|url=http://www.globenewswire.com/news-release/2017/05/01/974937/0/en/The-Apache-Software-Foundation-Announces-Apache-CarbonData-as-a-Top-Level-Project.html|title=The Apache Software Foundation Announces Apache® CarbonData™ as a Top-Level Project|first=The Apache Software|last=Foundation|date=May 1, 2017|website=GlobeNewswire News Room}}&lt;/ref&gt;
| latest release version = 1.6.0
| latest release date = {{Start date and age|2019|8|28|df=yes}}&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/CARBONDATA/Releases|title=Releases - CarbonData - Apache Software Foundation|website=cwiki.apache.org}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| programming language = 
| genre = [[Database management system]]
| license = [[Apache License 2.0]]
| website = {{Official URL}}
}}
'''Apache CarbonData''' is a [[free and open-source]] [[Column-oriented DBMS|column-oriented]] data storage format of the [[Apache Hadoop]] ecosystem. It is similar to the other columnar-storage file formats available in [[Apache Hadoop|Hadoop]] namely [[RCFile]] and [[Apache ORC|ORC]]. It is compatible with most of the data processing frameworks in the [[Hadoop]] environment. It provides efficient [[data compression]] and [[encoding]] schemes with enhanced performance to handle complex data in bulk.

==History==
CarbonData was first developed at Huawei in 2013. The project was submitted to the Apache Incubator in June 2016, and had its first official release two months later. The project won top honors in the BlackDuck 2016 Open Source Rookies of the Year's Big Data category. Since May 1, 2017, Apache CarbonData is a top-level Apache Software Foundation (ASF)-sponsored project.&lt;ref name="auto"/&gt;

==See also==
{{Portal|Free and open-source software}}
* [[Pig (programming tool)]]
* [[Apache Hive]]
* [[Apache Impala]]
* [[Apache Drill]]
* [[Apache Kudu]]
* [[Apache Spark]]
* [[Apache Thrift]]
* [[Apache Parquet]]
* [[Presto (SQL query engine)]]

==References==
{{Reflist}}

==External links==
* {{Official website}}

{{Apache Software Foundation}}

{{DEFAULTSORT:CarbonData}}
[[Category:2015 software]]
[[Category:Apache Software Foundation projects|CarbonData]]
[[Category:Cloud computing]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>6j9lil1gp55kdik09s4x06momfdkgii</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Giraph</title>
    <ns>0</ns>
    <id>37752641</id>
    <revision>
      <id>1000512627</id>
      <parentid>972217023</parentid>
      <timestamp>2021-01-15T11:49:43Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 3 templates: hyphenate params (3×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2267" xml:space="preserve">{{Infobox software
| name = Apache Giraph
| logo = [[File:Apache_Giraph_Logo.svg|200px|Apache Giraph Logo]]
| screenshot = 
| caption = 
| collapsible = yes
| developer = [[Apache Software Foundation]]
| latest release version = 1.2.0
| latest release date = {{Start date and age|2016|10|20|df=yes}}
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| size = 
| programming language = [[Java (programming language)|Java]]
| genre = [[Graph (computer science)|Graph processing]]
| license = [[Apache License 2.0]]
| website = {{URL|http://giraph.apache.org}}
}}
'''Apache Giraph''' is an [[Apache Software Foundation|Apache]] project to perform [[Graph (computer science)|graph processing]] on [[big data]]. Giraph utilizes [[Apache Hadoop]]'s MapReduce implementation to process graphs. [[Facebook]] used Giraph with some performance improvements to analyze one trillion edges using 200 machines in 4 minutes.&lt;ref&gt;{{cite web|last=Ching|first=Avery|title=Scaling Apache Giraph to a trillion edges|url=http://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920|publisher=Facebook|access-date=8 February 2014|date=August 14, 2013}}&lt;/ref&gt; Giraph is based on a paper published by Google about its own graph processing system called Pregel.&lt;ref&gt;{{cite news|last=Jackson|first=Joab|title=Facebook's Graph Search puts Apache Giraph on the map|url=http://www.pcworld.com/article/2046680/facebooks-graph-search-puts-apache-giraph-on-the-map.html|access-date=8 February 2014|newspaper=[[PC World]]|date=Aug 14, 2013}}&lt;/ref&gt; It can be compared to other Big Graph processing libraries such as Cassovary.&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Facebook’s trillion-edge, Hadoop-based and open source graph-processing engine|url=http://gigaom.com/2013/08/14/facebooks-trillion-edge-hadoop-based-graph-processing-engine/|publisher=[[Gigaom]]|access-date=8 February 2014|date=Aug 14, 2013}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* {{Official website|http://giraph.apache.org/}}

{{Apache Software Foundation}}

{{DEFAULTSORT:Giraph}}
[[Category:Apache Software Foundation projects|Giraph]]
[[Category:Hadoop]]
[[Category:Data mining and machine learning software]]</text>
      <sha1>f2vrg8i3h68ckrcasmkb1auild8qaim</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Hive</title>
    <ns>0</ns>
    <id>30248516</id>
    <revision>
      <id>998424465</id>
      <parentid>998340701</parentid>
      <timestamp>2021-01-05T09:14:09Z</timestamp>
      <contributor>
        <ip>2601:646:C900:7CF0:A5F3:F6AF:3DAD:D2E1</ip>
      </contributor>
      <comment>/* Comparison with traditional databases */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="19661" xml:space="preserve">{{Advert|date=October 2019}}
{{Infobox software
| name = Apache Hive
| logo = [[File:Apache Hive logo.svg|150px|Apache Hive]]
| screenshot = 
| caption = Apache Hive
| author = [[Facebook]]
| developer = [https://hive.apache.org/people.html Contributors]
| latest release version = 3.1.2
| latest release date = {{Start date and age|2019|08|26}}&lt;ref&gt;{{cite web|url=https://hive.apache.org/downloads.html#26-august-2019-release-312-available|title=26 August 2019: release 3.1.2 available|access-date=28 August 2019}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[Data warehouse]]
| license = [[Apache License 2.0]]
| website = {{URL|https://hive.apache.org}}
| released = {{Start date and age|2010|10|01}}&lt;ref&gt;[https://github.com/apache/hive/releases/tag/release-1.0.0]&lt;/ref&gt;
| repo = {{URL|https://github.com/apache/hive}}
| language = SQL
}}
'''Apache Hive''' is a [[data warehouse]] software project built on top of [[Apache Hadoop]] for providing data query and analysis.&lt;ref&gt;{{cite book |last=Venner |first=Jason |title=Pro Hadoop |url=https://archive.org/details/prohadoop0000venn |url-access=registration |publisher=[[Apress]] |year=2009 |isbn=978-1-4302-1942-2}}&lt;/ref&gt; Hive gives an [[SQL]]-like [[Interface (computing)|interface]] to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the [[MapReduce]] Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries ([[#HiveQL|HiveQL]]) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids portability of SQL-based applications to Hadoop.&lt;ref name=":3"&gt;{{Cite book|url=https://www.safaribooksonline.com/library/view/programming-hive/9781449326944/|title=Programming Hive [Book]}}&lt;/ref&gt; While initially developed by [[Facebook]], Apache Hive is used and developed by other companies such as [[Netflix]] and the [[Financial Industry Regulatory Authority]] (FINRA).&lt;ref&gt;[http://www.slideshare.net/evamtse/hive-user-group-presentation-from-netflix-3182010-3483386 Use Case Study of Hive/Hadoop ]&lt;/ref&gt;&lt;ref&gt;{{YouTube|id=Idu9OKnAOis|title=OSCON Data 2011, Adrian Cockcroft, "Data Flow at Netflix"}}&lt;/ref&gt; Amazon maintains a software fork of Apache Hive included in [[Apache Hadoop#On Amazon Elastic MapReduce|Amazon Elastic MapReduce]] on [[Amazon Web Services]].&lt;ref&gt;[http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf Amazon Elastic MapReduce Developer Guide]&lt;/ref&gt;

==Features==
Apache Hive supports analysis of large datasets stored in Hadoop's [[HDFS]] and compatible file systems such as [[Amazon S3]] filesystem and [[Alluxio]]. It provides a [[SQL]]-like query language called HiveQL&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual HiveQL Language Manual]&lt;/ref&gt; with schema on read and transparently converts queries to [[MapReduce]], Apache Tez&lt;ref&gt;[http://tez.apache.org/ Apache Tez]&lt;/ref&gt; and [[Apache Spark|Spark]] jobs. All three execution engines can run in [[Hadoop]]'s resource negotiator, YARN (Yet Another Resource Negotiator). To accelerate queries, it provided indexes, but this feature was removed in version 3.0 &lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing#LanguageManualIndexing-IndexingIsRemovedsince3.0 Hive Language Manual&lt;/ref&gt;
Other features of Hive include:
* Different storage types such as plain text, [[RCFile]], [[HBase]], ORC, and others.
* Metadata storage in a [[relational database management system]], significantly reducing the time to perform semantic checks during query execution.
* Operating on compressed data stored into the Hadoop ecosystem using algorithms including [[DEFLATE]], [[Burrows–Wheeler transform|BWT]], [[snappy (compression)|snappy]], etc.
* Built-in [[user-defined function]]s (UDFs) to manipulate dates, strings, and other data-mining tools. Hive supports extending the UDF set to handle use-cases not supported by built-in functions.
* SQL-like queries (HiveQL), which are implicitly converted into MapReduce or Tez, or Spark jobs.
By default, Hive stores metadata in an embedded [[Apache Derby]] database, and other client/server databases like [[MySQL]] can optionally be used.&lt;ref&gt;{{cite book |last=Lam |first=Chuck |title=Hadoop in Action |publisher=[[Manning Publications]] |year=2010 |isbn=978-1-935182-19-1}}&lt;/ref&gt;

The first four file formats supported in Hive were plain text,&lt;ref&gt;[http://www.semantikoz.com/blog/optimising-hadoop-big-data-text-hive/ Optimising Hadoop and Big Data with Text and HiveOptimising Hadoop and Big Data with Text and Hive]&lt;/ref&gt; sequence file, optimized row columnar (ORC) format&lt;ref&gt;{{Cite web |title= ORC Language Manual |url= https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC |work= Hive project wiki |access-date= April 24, 2017 }}&lt;/ref&gt; and [[RCFile]].&lt;ref name=":0"&gt;{{Cite web |url=http://www.sfbayacm.org/wp/wp-content/uploads/2010/01/sig_2010_v21.pdf |title=Facebook's Petabyte Scale Data Warehouse using Hive and Hadoop |access-date=2011-09-09 |archive-url=https://web.archive.org/web/20110728063630/http://www.sfbayacm.org/wp/wp-content/uploads/2010/01/sig_2010_v21.pdf |archive-date=2011-07-28 |url-status=dead }}&lt;/ref&gt; [[Apache Parquet]] can be read via plugin in versions later than 0.10 and natively starting at 0.13.&lt;ref&gt;{{cite web|title=Parquet|url=https://cwiki.apache.org/confluence/display/Hive/Parquet|access-date=2 February 2015|archive-url=https://web.archive.org/web/20150202145641/https://cwiki.apache.org/confluence/display/Hive/Parquet|archive-date=2 February 2015|date=18 Dec 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Massie|first1=Matt|title=A Powerful Big Data Trio: Spark, Parquet and Avro|url=http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/|website=zenfractal.com|access-date=2 February 2015|archive-url=https://web.archive.org/web/20150202145026/http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/|archive-date=2 February 2015|date=21 August 2013}}&lt;/ref&gt; Additional Hive plugins support querying of the [[Bitcoin]] [[Blockchain (database)|Blockchain]].&lt;ref&gt;{{cite web|last1=Franke|first1=Jörn|title=Hive &amp; Bitcoin: Analytics on Blockchain data with SQL|url=https://snippetessay.wordpress.com/2016/04/28/hive-bitcoin-analytics-on-blockchain-data-with-sql/|date=2016-04-28}}&lt;/ref&gt;

== Architecture ==
{{Prose|section|date=October 2016}}
Major components of the Hive architecture are:
&lt;!-- Deleted image removed: [[File:Hive architecture.png|thumb|300x300px|Hive Architecture&lt;ref&gt;{{Cite journal|last=Thusoo|first=Ashish|last2=Sarma|first2=Joydeep Sen|last3=Jain|first3=Namit|last4=Shao|first4=Zheng|last5=Chakka|first5=Prasad|last6=Anthony|first6=Suresh|last7=Liu|first7=Hao|last8=Wyckoff|first8=Pete|last9=Murthy|first9=Raghotham|date=2009-08-01|title=Hive: A Warehousing Solution over a Map-reduce Framework|url=https://dx.doi.org/10.14778/1687553.1687609|journal=Proc. VLDB Endow.|volume=2|issue=2|pages=1626–1629|doi=10.14778/1687553.1687609|issn=2150-8097}}&lt;/ref&gt;]] --&gt;
* Metastore: Stores metadata for each of the tables such as their schema and location.  It also includes the partition metadata which helps the driver to track the progress of various data sets distributed over the cluster.&lt;ref name=":1"&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/Design|title=Design - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt; The data is stored in a traditional [[RDBMS]] format. The metadata helps the driver to keep track of the data and it is crucial. Hence, a backup server regularly replicates the data which can be retrieved in case of data loss.
* Driver: Acts like a controller which receives the HiveQL statements. It starts the execution of the statement by creating sessions, and monitors the life cycle and progress of the execution. It stores the necessary metadata generated during the execution of a HiveQL statement. The driver also acts as a collection point of data or query results obtained after the Reduce operation.&lt;ref name=":0" /&gt;
* Compiler: Performs compilation of the HiveQL query, which converts the query to an execution plan.  This plan contains the tasks and steps needed to be performed by the [[Apache Hadoop|Hadoop]] [[MapReduce]] to get the output as translated by the query. The compiler converts the query to an [[abstract syntax tree]] (AST). After checking for compatibility and compile time errors, it converts the AST to a [[directed acyclic graph]] (DAG).&lt;ref&gt;{{Cite web|url=http://c2.com/cgi/wiki?AbstractSyntaxTree|title=Abstract Syntax Tree|website=c2.com|access-date=2016-09-12}}&lt;/ref&gt; The DAG divides operators to MapReduce stages and tasks based on the input query and data.&lt;ref name=":1" /&gt;
* Optimizer: Performs various transformations on the execution plan to get an optimized DAG.  Transformations can be aggregated together, such as converting a pipeline of joins to a single join, for better performance.&lt;ref name=":2"&gt;{{Cite journal|last=Dokeroglu|first=Tansel|last2=Ozal|first2=Serkan|last3=Bayir|first3=Murat Ali|last4=Cinar|first4=Muhammet Serkan|last5=Cosar|first5=Ahmet|date=2014-07-29|title=Improving the performance of Hadoop Hive by sharing scan and computation tasks|journal=Journal of Cloud Computing|language=en|volume=3|issue=1|pages=1–11|doi=10.1186/s13677-014-0012-6|doi-access=free}}&lt;/ref&gt; It can also split the tasks, such as applying a transformation on data before a reduce operation, to provide better performance and scalability. However, the logic of transformation used for optimization used can be modified or pipelined using another optimizer.&lt;ref name=":0" /&gt;
* Executor: After compilation and optimization, the executor executes the tasks. It interacts with the job tracker of Hadoop to schedule tasks to be run. It takes care of pipelining the tasks by making sure that a task with dependency gets executed only if all other prerequisites are run.&lt;ref name=":2" /&gt;
* CLI, UI, and [[Apache Thrift|Thrift Server]]: A  [[command-line interface]] (CLI) provides a [[user interface]] for an external user to interact with Hive by submitting queries, instructions and monitoring the process status. Thrift server allows external clients to interact with Hive over a network, similar to the [[Jdbc|JDBC]] or [[Odbc|ODBC]] protocols.&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/HiveServer|title=HiveServer - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt;

==HiveQL==
While based on SQL, HiveQL does not strictly follow the full [[SQL-92]] standard. HiveQL offers extensions not in SQL, including ''multitable inserts'' and ''create table as select''. 
HiveQL lacked support for [[database transaction|transactions]] and [[materialized view]]s, and only limited subquery support.&lt;ref name=":4"&gt;{{cite book |last=White |first=Tom |title=Hadoop: The Definitive Guide |url=https://archive.org/details/hadoopdefinitive0000whit |url-access=registration |publisher=[[O'Reilly Media]] |year=2010 |isbn=978-1-4493-8973-4}}&lt;/ref&gt;&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual Hive Language Manual]&lt;/ref&gt; Support for insert, update, and delete with full [[ACID]] functionality was made available with release 0.14.&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions ACID and Transactions in Hive]&lt;/ref&gt;

Internally, a [[compiler]] translates HiveQL statements into a [[directed acyclic graph]] of [[MapReduce]], Tez, or [[Apache Spark|Spark]] jobs, which are submitted to Hadoop for execution.&lt;ref&gt;{{Cite web |url=http://www.vldb.org/pvldb/2/vldb09-938.pdf |title=Hive A Warehousing Solution Over a MapReduce Framework |access-date=2011-09-03 |archive-url=https://web.archive.org/web/20131008161608/http://www.vldb.org/pvldb/2/vldb09-938.pdf |archive-date=2013-10-08 |url-status=dead }}&lt;/ref&gt;

=== Example ===
The word count program counts the number of times each word occurs in the input. The word count can be written in HiveQL as:&lt;ref name=":3" /&gt;

&lt;syntaxhighlight lang="sql" line&gt;
DROP TABLE IF EXISTS docs;
CREATE TABLE docs (line STRING);
LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
 (SELECT explode(split(line, '\s')) AS word FROM docs) temp
GROUP BY word
ORDER BY word;
&lt;/syntaxhighlight&gt;
A brief explanation of each of the statements is as follows:
&lt;syntaxhighlight lang="sql" line&gt;
DROP TABLE IF EXISTS docs;
CREATE TABLE docs (line STRING);
&lt;/syntaxhighlight&gt;
Checks if table &lt;code&gt;docs&lt;/code&gt; exists and drops it if it does. Creates a new table called &lt;code&gt;docs&lt;/code&gt; with a single column of type &lt;code&gt;STRING&lt;/code&gt; called &lt;code&gt;line&lt;/code&gt;.
&lt;syntaxhighlight lang="sql" line start="3"&gt;
LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;
&lt;/syntaxhighlight&gt;
Loads the specified file or directory (In this case “input_file”) into the table. &lt;code&gt;OVERWRITE&lt;/code&gt; specifies that the target table to which the data is being loaded into is to be re-written; Otherwise the data would be appended.
&lt;syntaxhighlight lang="sql" line start="4" highlight="6"&gt;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
(SELECT explode(split(line, '\s')) AS word FROM docs) temp
GROUP BY word
ORDER BY word;
&lt;/syntaxhighlight&gt;
The query {{code|CREATE TABLE word_counts AS SELECT word, count(1) AS count}} creates a table called &lt;code&gt;word_counts&lt;/code&gt; with two columns: &lt;code&gt;word&lt;/code&gt; and  &lt;code&gt;count&lt;/code&gt;. This query draws its input from the inner query {{code|lang=sql|(SELECT explode(split(line, '\s')) AS word FROM docs) temp"}}. This query serves to split the input words into different rows of a temporary table aliased as &lt;code&gt;temp&lt;/code&gt;. The {{code|lang=sql|GROUP BY WORD}} groups the results based on their keys. This results in the &lt;code&gt;count&lt;/code&gt; column holding the number of occurrences for each word of the &lt;code&gt;word&lt;/code&gt; column. The {{code|lang=sql|ORDER BY WORDS}} sorts the words alphabetically.

== Comparison with traditional databases ==
The storage and querying operations of Hive closely resemble those of traditional databases. While Hive is a SQL dialect, there are a lot of differences in structure and working of Hive in comparison to relational databases. The differences are mainly because Hive is built on top of the [[Apache Hadoop|Hadoop]] ecosystem, and has to comply with the restrictions of Hadoop and [[MapReduce]].

A schema is applied to a table in traditional databases. In such traditional databases, the table typically enforces the schema when the data is loaded into the table. This enables the database to make sure that the data entered follows the representation of the table as specified by the table definition. This design is called ''schema on write''. In comparison, Hive does not verify the data against the table schema on write. Instead, it subsequently does run time checks when the data is read. This model is called ''schema on read''.&lt;ref name=":4" /&gt; The two approaches have their own advantages and drawbacks. Checking data against table schema during the load time adds extra overhead, which is why traditional databases take a longer time to load data. Quality checks are performed against the data at the load time to ensure that the data is not corrupt. Early detection of corrupt data ensures early exception handling. Since the tables are forced to match the schema after/during the data load, it has better query time performance. Hive, on the other hand, can load data dynamically without any schema check, ensuring a fast initial load, but with the drawback of comparatively slower performance at query time. Hive does have an advantage when the schema is not available at the load time, but is instead generated later dynamically.&lt;ref name=":4" /&gt;

Transactions are key operations in traditional databases. As any typical [[Relational database management system|RDBMS]], Hive supports all four properties of transactions ([[ACID]]): [[Atomicity (database systems)|Atomicity]], [[Consistency (database systems)|Consistency]], [[Isolation (database systems)|Isolation]], and [[Durability (database systems)|Durability]]. Transactions in Hive were introduced in Hive 0.13 but were only limited to the partition level.&lt;ref&gt;{{Cite web|url=http://datametica.com/introduction-to-hive-transactions/|title=Introduction to Hive transactions|website=datametica.com|access-date=2016-09-12|archive-url=https://web.archive.org/web/20160903210039/http://datametica.com/introduction-to-hive-transactions|archive-date=2016-09-03|url-status=dead}}&lt;/ref&gt;  Recent version of Hive 0.14 had these functions fully added to support complete [[ACID]] properties. Hive 0.14 and later provides different row level transactions such as ''INSERT, DELETE and UPDATE''.&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-NewConfigurationParametersforTransactions|title=Hive Transactions - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt; Enabling ''INSERT, UPDATE, DELETE'' transactions require setting appropriate values for configuration properties such as &lt;code&gt;hive.support.concurrency&lt;/code&gt;, &lt;code&gt;hive.enforce.bucketing&lt;/code&gt;, and &lt;code&gt;hive.exec.dynamic.partition.mode&lt;/code&gt;.&lt;ref&gt;{{Cite web|url=https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.txn.manager|title=Configuration Properties - Apache Hive - Apache Software Foundation|website=cwiki.apache.org|access-date=2016-09-12}}&lt;/ref&gt;

==Security==
Hive v0.7.0 added integration with Hadoop security. Hadoop began using [[Kerberos (protocol)|Kerberos]] authorization support to provide security. Kerberos allows for mutual authentication between client and server. In this system, the client's request for a ticket is passed along with the request. The previous versions of Hadoop had several issues such as users being able to spoof their username by setting the &lt;code&gt;hadoop.job.ugi&lt;/code&gt; property and also MapReduce operations being run under the same user: hadoop or mapred. With Hive v0.7.0's integration with Hadoop security, these issues have largely been fixed. TaskTracker jobs are run by the user who launched it and the username can no longer be spoofed by setting the &lt;code&gt;hadoop.job.ugi&lt;/code&gt; property. Permissions for newly created files in Hive are dictated by the [[Apache Hadoop|HDFS]]. The Hadoop distributed file system authorization model uses three entities: user, group and others with three permissions: read, write and execute. The default permissions for newly created files can be set by changing the umask value for the Hive configuration variable &lt;code&gt;hive.files.umask.value&lt;/code&gt;.&lt;ref name=":3" /&gt;

==See also==
* [[Pig (programming tool)|Apache Pig]]
* [[Sqoop]]
* [[Apache Impala]]
* [[Apache Drill]]
* [[Apache Flume]]
* [[HBase|Apache HBase]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}

{{Apache Software Foundation}}
{{Facebook navbox}}

{{DEFAULTSORT:Hive}}
[[Category:2015 software]]
[[Category:Apache Software Foundation projects|Hive]]
[[Category:Cloud computing]]
[[Category:Facebook software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>ly3osuj7fry8ya21182o35e1522vacq</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Trafodion</title>
    <ns>0</ns>
    <id>43330944</id>
    <revision>
      <id>1000544771</id>
      <parentid>972223930</parentid>
      <timestamp>2021-01-15T15:35:24Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 6 templates: hyphenate params (10×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5036" xml:space="preserve">{{Infobox software
| name = Trafodion
| logo = [[File:Apache Trafodion Logo.svg|250px|Apache Trafodion Logo]]
| developer = [[Apache Software Foundation]]
| latest release version = 2.3.0
| latest release date = {{Start date and age|2019|02|28}}
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;trafodion.git|Trafodion Repository}}
| programming language = [[C++]], [[Java (programming language)|Java]]
| operating system = [[Linux]]
| genre = SQL-on-Hadoop [[Relational database management system|DBMS]]
| license = [[Apache License 2.0]]
| website = {{URL|http://trafodion.apache.org}}
}}
'''Apache Trafodion''' is an [[Open-source software|open-source]] Top-Level Project at the [[Apache Software Foundation]]. It was originally developed by the [[information technology]] division of [[Hewlett-Packard|Hewlett-Packard Company]] and [[HP Labs]] to provide the [[SQL]] query language on [[Apache HBase]] targeting [[big data]] transactional or operational workloads.&lt;ref name="trafodion-wiki"&gt;{{cite web|title=Trafodion: Transactional SQL-on-HBase |url=https://wiki.trafodion.org/wiki/index.php/Main_Page |date=June 9, 2014 |access-date=July 17, 2014 |url-status=dead |archive-url=https://web.archive.org/web/20140725065043/https://wiki.trafodion.org/wiki/index.php/Main_Page |archive-date=July 25, 2014 }}&lt;/ref&gt; The project was named after the Welsh word for transactions.&lt;ref name="trafodion-wiki" /&gt;

==Features==
Trafodion is a [[relational database management system]] that runs on [[Apache Hadoop]],&lt;ref name="trafodion-features"&gt;{{cite web|title=Trafodion: First Release Features (Release 0.8.0) |url=https://wiki.trafodion.org/wiki/index.php/Release_0.8.0_Features |date=May 29, 2014 |access-date=October 21, 2015 }}{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; providing support for transactional or operational workloads in a [[big data]] environment.&lt;ref name="trafodion-wiki" /&gt; The following is a list of key features:

* [[SQL|ANSI SQL]] language support&lt;ref name="trafodion-features" /&gt;
* [[Java Database Connectivity|JDBC]] and [[Open Database Connectivity]] (ODBC) connectivity for Linux and Windows clients&lt;ref name="trafodion-features" /&gt;
* Distributed [[ACID]] transaction protection across multiple statements, tables, and rows&lt;ref name="trafodion-features" /&gt;
* Compile-time and run-time optimizations for real-time operational workloads&lt;ref name="trafodion-features" /&gt;
* Support for large data sets using a parallel-aware query optimizer and a parallel data-flow execution engine&lt;ref name="trafodion-features" /&gt;

Transaction management features include:

* Begin, commit, and rollback work syntax, including SET TRANSACTION&lt;ref name="trafodion-features" /&gt;
* READ COMMITTED transactional isolation level&lt;ref name="trafodion-features" /&gt;
* Multiple SQL processes participating in the same transaction concurrently&lt;ref name="trafodion-features" /&gt;
* Recovery after region server, transaction manager, or node failure&lt;ref name="trafodion-features" /&gt;
* Support for region splits and balancing&lt;ref name="trafodion-features" /&gt;

==History==
Trafodion was launched by HP as an [[Open-source software|open-source]] project on June 10, 2014.&lt;ref name="introducing-trafodion"&gt;{{cite web | title=Introducing Trafodion - HP Enterprise Business Community | url=http://h30499.www3.hp.com/t5/Information-Faster-Blog/Introducing-Trafodion/ba-p/6512958#.U73FLvldW0I | date=June 17, 2014 | access-date=July 17, 2014}}&lt;/ref&gt;

A version of Trafodion was released on January 29, 2015.&lt;ref name="Trafodion-1.0.0"&gt;{{cite web|title=Release of Trafodion 1.0.0 |url=https://wiki.trafodion.org/wiki/index.php/Event:2015/01/29_Release_of_Trafodion_1.0.0 |date=January 30, 2015 |access-date=February 23, 2015 |url-status=dead |archive-url=https://web.archive.org/web/20150224040747/https://wiki.trafodion.org/wiki/index.php/Event%3A2015/01/29_Release_of_Trafodion_1.0.0 |archive-date=February 24, 2015 }}&lt;/ref&gt;

Trafodion became an Apache Incubation Project in May 2015.&lt;ref name="Apache-Incubation-ProjList"&gt;{{cite web | title=Apache Incubation Project List |
url=http://incubator.apache.org/projects/ | access-date=October 19, 2015}}&lt;/ref&gt;

Trafodion graduated from the Apache Incubator to become a Top-Level Project at the Apache Software Foundation in January 2018.&lt;ref name="Apache-Foundation-Trafodion-TLP"&gt;{{cite web | title=The Apache Software Foundation Announces Apache Trafodion as a Top-Level Project |
url=https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces27 | access-date=February 13, 2018}}&lt;/ref&gt;

==See also==
* [[Big data]]
* [[Apache Hadoop|Hadoop]]
* [[Apache HBase|HBase]]
* [[NewSQL]]

==References==
{{Reflist}}

==External links==
* [http://trafodion.apache.org Apache Trafodion website]

{{Apache Software Foundation}}

{{DEFAULTSORT:Trafodion}}
[[Category:Apache Software Foundation projects|Trafodion]]
[[Category:Bigtable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:Structured storage]]</text>
      <sha1>ggmhyp8aso8jkyflwd4tkk9o5gbqvla</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Accumulo</title>
    <ns>0</ns>
    <id>34571412</id>
    <revision>
      <id>915416995</id>
      <parentid>915147324</parentid>
      <timestamp>2019-09-13T04:16:34Z</timestamp>
      <contributor>
        <username>BrownHairedGirl</username>
        <id>754619</id>
      </contributor>
      <minor/>
      <comment>replace link to deleted [[Portal:Java (programming language)]] with [[Portal:Computer programming]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6363" xml:space="preserve">{{Infobox software
| name = Apache Accumulo
| logo = [[File:Apache_Accumulo_Logo.svg|250px|Apache Accumulo Logo]]
| screenshot = 
| caption = 
| developer = [[Apache Software Foundation]]
| discontinued = No
| latest release version = 2.0.0
| latest release date = {{Start date and age|2019|08|02}}&lt;ref&gt;{{cite web|url=https://github.com/apache/accumulo/releases/tag/rel%2F2.0.0|title=Release rel/2.0.0 · apache/accumulo · GitHub|access-date=5 August 2019}}&lt;/ref&gt;
| latest preview version = &lt;!--2.0.0-alpha-2--&gt;
| latest preview date = {{Start date and age|2019|01|31}}&lt;ref&gt;{{cite web|url=https://accumulo.apache.org/release/accumulo-2.0.0-alpha-2/|title=Apache Accumulo 2.0.0-alpha-2|access-date=4 July 2019}}&lt;/ref&gt;
| repo = {{URL|https://github.com/apache/accumulo|Accumulo Repository}}
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| license = [[Apache License 2.0]]
| website = {{URL|//accumulo.apache.org}}
}}
'''Apache Accumulo''' is a highly scalable sorted, distributed key-value store based on [[Google]]'s [[Bigtable]].&lt;ref&gt;[http://accumulo.apache.org/ Apache Accumulo]. Accumulo.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;  It is a system built on top of [[Apache Hadoop]], [[Apache ZooKeeper]], and [[Apache Thrift]]. Written in [[Java (programming language)|Java]], Accumulo has cell-level [[Apache Accumulo#Cell-Level Security|access label]]s and [[server-side programming]] mechanisms. According to  [[DB-Engines ranking]], Accumulo is the third most popular [[NoSQL]] [[wide column store]] behind [[Apache Cassandra]] and [[HBase]] and the 67th most popular database engine of any type (complete) as of 2018.&lt;ref&gt;[http://db-engines.com/en/ranking/wide+column+store DB-Engines Ranking - popularity ranking of wide column stores]. Db-engines.com. Retrieved on 2018-04-10. [https://web.archive.org/web/20180410051147/http://db-engines.com/en/ranking/wide+column+store Archived 2018-04-10].&lt;/ref&gt;

==History==
Accumulo was created in 2008 by the US [[National Security Agency]] and contributed to the [[Apache Foundation]] as an incubator project in September 2011.&lt;ref name="informationweek1"&gt;[http://www.informationweek.com/news/government/enterprise-apps/231600835 NSA Submits Open Source, Secure Database To Apache - Government]. Informationweek.com (2011-09-06). Retrieved on 2013-09-18.&lt;/ref&gt;

On March 21, 2012, Accumulo graduated from incubation at Apache, making it a top-level project.&lt;ref&gt;[http://incubator.apache.org/projects/accumulo.html Accumulo Incubation Status - Apache Incubator]. Incubator.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;

===Controversy===
In June 2012 the US [[Senate Armed Services Committee]] (SASC) released the Draft 2012 Department of Defense (DoD) Authorization Bill, which included references to Apache Accumulo.  In the draft bill SASC required DoD to evaluate whether Apache Accumulo could achieve commercial viability before implementing it throughout DoD.&lt;ref&gt;Metz, Cade. (2012-12-19) [https://www.wired.com/wiredenterprise/2012/07/nsa-accumulo-google-bigtable/ NSA Mimics Google, Pisses Off Senate | Wired Enterprise]. Wired.com. Retrieved on 2013-09-18.&lt;/ref&gt; Specific criteria were not included in the draft language, but the establishment of commercial entities supporting Apache Accumulo could be considered a success factor.&lt;ref&gt;[http://www.fiercegovernmentit.com/story/sasc-accumulo-language-pro-open-source-say-proponents/2012-06-14 SASC Accumulo language pro-open source, say proponents]. FierceGovernmentIT (2012-06-14). Retrieved on 2013-09-18.&lt;/ref&gt;

==Main features==

===Cell-level security===
Apache Accumulo extends the [[Bigtable#Design|Bigtable data model]], adding a new element to the key called [http://accumulo.apache.org/1.4/user_manual/Security.html Column Visibility]. This element stores a logical combination of security labels that must be satisfied at query time in order for the key and value to be returned as part of a user request. This allows data of varying security requirements to be stored in the same table, and allows users to see only those keys and values for which they are authorized.&lt;ref name="informationweek1"/&gt;

===Server-side programming===
In addition to Cell-Level Security, Apache Accumulo provides a server-side programming mechanism called Iterators that allows users to perform additional processing at the Tablet Server. The range of operations that can be applied is equivalent to those that can be implemented within a [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/mapreduce-osdi04.pdf MapReduce Combiner function], which produces an aggregate value for several key-value pairs.

===User key ordering===
Apache Accumulo orders entries in order of user keys, and exposes an iterator over a key range. This allows locality of reference not available from some other distributed stores (including Cassandra and Voldemort that order by hash of the user key).

==Papers==
* 2011 [http://www.pdl.cmu.edu/PDL-FTP/Storage/socc2011.pdf YCSB++: Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores] by Carnegie Mellon University and the National Security Agency.
* 2012 [https://www.mit.edu/~kepner/pubs/ByunKepner_2012_BigData_Paper.pdf Driving Big Data With Big Compute] by MIT Lincoln Laboratory.
* 2013 [https://www.mit.edu/~kepner/pubs/D4Mschema_HPEC2013_Paper.pdf  D4M 2.0 Schema:A General Purpose High Performance Schema for the Accumulo Database] by MIT Lincoln Laboratory.
* 2013 [https://geomesa.github.io/assets/outreach/SpatioTemporalIndexing_IEEEcopyright.pdf Spatio-temporal Indexing in Non-relational Distributed Databases] by CCRi

==See also==
{{Portal|Computer programming|Free and open-source software}}
* [[Bigtable]]
* [[Apache Cassandra]]
* [[Column-oriented DBMS]]
* [[Hypertable]]
* [[HBase]]
* [[Hadoop]]
* [[sqrrl]]

==References==
{{Reflist}}

==External links==
* {{Official website|//accumulo.apache.org}}

{{Apache Software Foundation}}

{{DEFAULTSORT:Accumulo}}
[[Category:Apache Software Foundation]]
[[Category:Apache Software Foundation projects]]
[[Category:Bigtable implementations]]
[[Category:Distributed computing architecture]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:Hadoop]]
[[Category:NoSQL products]]
[[Category:NoSQL]]</text>
      <sha1>iqs2dyw94z05yaul25d3bel9gb1tdi5</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Kudu</title>
    <ns>0</ns>
    <id>54098260</id>
    <revision>
      <id>988315520</id>
      <parentid>972218679</parentid>
      <timestamp>2020-11-12T12:36:40Z</timestamp>
      <contributor>
        <username>R1pp3r.j4ck</username>
        <id>9752826</id>
      </contributor>
      <comment>update latest stable release</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4004" xml:space="preserve">{{Infobox software
| name = Apache Kudu
| logo = [[File:Apache_Kudu_Logo.svg|200px|Apache Kudu Logo]]
| screenshot = 
| caption = Apache Kudu
| developer = {{URL|https://kudu.apache.org/committers.html|Apache Kudu Committers and PMC Members}}
| discontinued = No
| latest release version = 1.13.0
| latest release date = {{Start date and age|2020|09|17|df=yes}}&lt;ref&gt;{{cite web|title=Apache Kudu - Releases|url=https://kudu.apache.org/releases/|access-date=12 November 2020|quote=Kudu 1.13.0 was released on September 17, 2020.}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Linux]], [[macOS]]
| repo = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;kudu.git|Kudu Repository}}
| programming language = [[C++]]
| genre = [[Database management system]]
| license = [[Apache License 2.0]]&lt;ref name="kudufaqprojectstatus"&gt;{{Cite web|url=https://kudu.apache.org/faq.html#project-status|title=Project Status|date=2017-05-21|language=en-US|access-date=2017-05-21|archive-url=https://web.archive.org/web/20170521110002/https://kudu.apache.org/faq.html|archive-date=2017-05-21|url-status=live|quote=Is Kudu open source? Yes, Kudu is open source and licensed under the Apache Software License, version 2.0. Apache Kudu is a top level project (TLP) under the umbrella of the Apache Software Foundation.}}&lt;/ref&gt;
}}
'''Apache Kudu''' is a [[free and open source]] [[Column-oriented DBMS|column-oriented]] data store of the [[Apache Hadoop]] ecosystem. It is compatible with most of the data processing frameworks in the [[Hadoop]] environment. It provides completeness to Hadoop's storage layer to enable fast analytics on fast data.&lt;ref&gt;https://kudu.apache.org/&lt;/ref&gt;

The [[Open-source software|open source]] project to build Apache Kudu began as internal project at [[Cloudera]].&lt;ref&gt;{{Cite web|url=https://kudu.apache.org/faq.html#project-status|title=Why was Kudu developed internally at Cloudera before its release?|date=2017-05-21|language=en-US|access-date=2017-05-21}}&lt;/ref&gt; The first version Apache Kudu 1.0 was released 19 September 2016.&lt;ref&gt;{{Cite web|url=https://kudu.apache.org/releases/|title=Apache Kudu releases|date=2017-05-21|language=en-US|access-date=2017-05-21|archive-url=https://web.archive.org/web/20170521105214/https://kudu.apache.org/releases/|archive-date=2017-05-21|url-status=live|quote=Kudu 1.0.0 was released on September 19, 2016. It is the first release not considered “beta”. […] Kudu 0.5.0 (beta) was released on Sep 28, 2015. It was the first public version of Kudu.}}&lt;/ref&gt;

== Comparison with other storage engines ==
Kudu was designed and optimized for OLAP workloads. Like HBase, it is a real-time store that supports key-indexed record lookup and mutation.&lt;ref name="kudufaqprojectmotivation"&gt;{{Cite web|url=https://kudu.apache.org/faq.html#project-motivation|title=Why build a new storage engine? Why not just improve Apache HBase to increase its scan speed?|date=2017-05-21|language=en-US|access-date=2017-05-21|archive-url=https://web.archive.org/web/20170521110002/https://kudu.apache.org/faq.html|archive-date=2017-05-21|url-status=live}}&lt;/ref&gt; Kudu differs from HBase since Kudu's datamodel is a more traditional relational model, while HBase is schemaless. Kudu's "on-disk representation is truly columnar and follows an entirely different storage design than HBase/[[Bigtable]]".&lt;ref name="kudufaqprojectmotivation"/&gt;

==See also==
{{Portal|Free and open-source software}}
* [[List of column-oriented DBMSes]]
* [[Apache HBase]]
* [[Apache Hive]]
* [[Apache Impala]]
* [[Apache Parquet]]
* [[Apache Drill]]
* [[Apache Spark]]
* [[Apache Thrift]]
* [[ClickHouse]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website|//kudu.apache.org/}}

{{Apache Software Foundation}}

{{DEFAULTSORT:Kudu}}
[[Category:Apache Software Foundation projects|Kudu]]
[[Category:2016 software]]
[[Category:Cloud computing]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>i8tcusj20vr82a8mxrbol41d7q4rg4q</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Mahout</title>
    <ns>0</ns>
    <id>18706674</id>
    <revision>
      <id>999087360</id>
      <parentid>998555553</parentid>
      <timestamp>2021-01-08T12:21:17Z</timestamp>
      <contributor>
        <username>Kj cheetham</username>
        <id>6972236</id>
      </contributor>
      <minor/>
      <comment>Minor formatting</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7652" xml:space="preserve">{{Infobox software
| name = Apache Mahout
| logo = [[File:Apache_Mahout_Logo.png|250px|Apache Mahout Logo]]
| screenshot = 
| caption = 
| collapsible = yes
| developer = [[Apache Software Foundation]]
| latest release version = 14.1
| latest release date = {{Start date and age|2020|10|07|df=yes}}&lt;ref&gt;{{Cite web|url=https://mahout.apache.org/|title=Apache Mahout: Scalable machine learning and data mining|access-date=6 March 2019}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| size = 
| repo = {{URL|https://gitbox.apache.org/repos/asf/mahout.git|Mahout Repository}}
| programming language = [[Java (programming language)|Java]], [[Scala (programming language)|Scala]]
| genre = [[Machine Learning]]
| license = [[Apache License 2.0]]
| website = {{URL|//mahout.apache.org}}
| released = {{Start date and age|2009|04|7|df=yes}}&lt;ref&gt;{{Cite web|url=http://mail-archives.apache.org/mod_mbox/www-announce/200904.mbox/%3C7EDF8CB8-388C-4A44-974E-6977E7AEB396@apache.org%3E|title=Apache Mahout: First release 0.1 released}}&lt;/ref&gt;
| discontinued = No
}}
'''Apache Mahout''' is a project of the [[Apache Software Foundation]] to produce [[free software|free]] implementations of [[distributed computing|distributed]] or otherwise [[Scalability|scalable]] [[machine learning]] algorithms focused primarily on [[linear algebra]]. In the past, many of the implementations use the [[Apache Hadoop]] platform, however today it is primarily focused on [[Apache Spark]].&lt;ref&gt;{{cite web |url= http://www.ibm.com/developerworks/java/library/j-mahout/ |title=Introducing Apache Mahout |work=ibm.com |year=2011  |access-date=13 September 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.infoq.com/news/2009/04/mahout |title=InfoQ: Apache Mahout: Highly Scalable Machine Learning Algorithms |work=infoq.com |year=2011 |access-date=13 September 2011}}&lt;/ref&gt; Mahout also provides Java/Scala libraries for common maths operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.&lt;ref&gt;{{cite web |url= http://mahout.apache.org/users/basics/algorithms.html |title=Algorithms - Apache Mahout - Apache Software Foundation|work=cwiki.apache.org |year=2011 |access-date=13 September 2011}}&lt;/ref&gt;

== Features == 

=== Samsara ===
Apache Mahout-Samsara refers to a Scala domain specific language (DSL) that allows users to use R-Like syntax as opposed to traditional Scala-like syntax. This allows user to express algorithms concisely and clearly.


&lt;syntaxhighlight lang="scala"&gt;
val G = B %*% B.t - C - C.t + (ksi dot ksi) * (s_q cross s_q)
&lt;/syntaxhighlight&gt;


=== Backend Agnostic ===
Apache Mahout’s code abstracts the domain specific language from the engine where the code is run. While active development is done with the Apache Spark engine, users are free to implement any engine they choose- H2O and Apache Flink have been implemented in the past and examples exist in the code base. 

=== GPU/CPU accelerators ===
The JVM has notoriously slow computation, to solve this set back “native solvers” were added which moves in-core, and by extension, distributed BLAS operations out of the JVM, offloading to off-heap or GPU memory for processing via multiple CPUs and/or CPU cores, or GPUs when built against the ViennaCL library.&lt;ref&gt;{{Cite web
| url = http://viennacl.sourceforge.net/| title = ViennaCL }}&lt;/ref&gt;  {{ cite web |url=https://on-demand.gputechconf.com/gtc/2017/video/s7572-extending-mahout-samsara-linear-algebra-dsl-to-support-gpu-clusters.mp4|title=Extending Mahout Samsara to GPU Clusters }}.  ViennaCL is a highly optimized C++ library with BLAS operations implemented in OpenMP, and OpenCL.  As of release 14.1, the OpenMP build considered to be stable, leaving the OpenCL build is still in its experimental POC phase.

=== Recommenders === 
Apache Mahout features implementations of Alternating Least Squares, Co-Occurrence, and Correlated Co-Occurrence, a unique-to-Mahout recommender algorithm that extends co-occurrence to be used on multiple dimensions of data.

== History ==

=== Transition from Map Reduce to Apache Spark ===

While Mahout's core algorithms for [[Cluster analysis|clustering]], classification and batch based collaborative filtering were implemented on top of Apache Hadoop using the [[MapReduce|map/reduce]] paradigm, it did not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster were also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop.

Starting with the release 0.10.0, the project shifted its focus to building a backend-independent programming environment, code named "Samsara".&lt;ref&gt;{{Cite web
| url = http://mahout.apache.org/users/environment/in-core-reference.html
| title = Mahout-Samsara's In-Core Linear Algebra DSL Reference
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
| url = http://mahout.apache.org/users/environment/out-of-core-reference.html
| title = Mahout-Samsara's Distributed Linear Algebra DSL Reference
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
| url = http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html
| title = Mahout 0.10.x: first Mahout release as a programming environment
| website = www.weatheringthroughtechdays.com
| access-date = 2016-02-29
| archive-url = https://web.archive.org/web/20161009224405/http://www.weatheringthroughtechdays.com/2015/04/mahout-010x-first-mahout-release-as.html
| archive-date = 9 October 2016
| url-status = dead
}}&lt;/ref&gt; The environment consists of an algebraic backend-independent optimizer and an algebraic Scala DSL unifying in-memory and distributed algebraic operators. Supported algebraic platforms are [[Apache Spark]], H2O, and [[Apache Flink]].{{citation needed|date=August 2019}} Support for [[MapReduce]] algorithms started being gradually phased out in 2014.&lt;ref&gt;{{Cite web
| url = https://issues.apache.org/jira/browse/MAHOUT-1510
| title = MAHOUT-1510 ("Good-bye MapReduce")
}}&lt;/ref&gt;

=== Release History ===
{| class="wikitable"
|+ Release History
|-
! Version !! Release Date !! Notes
|-
| 0.1 || 2009-04-07  ||  
|-
| 0.2 || 2009-11-18  ||
|-
| 0.3 ||  2010-03-17 ||
|-
| 0.4 ||  2010-10-31 ||
|-
| 0.5 ||  2011-05-27 ||
|-
| 0.6 || 2012-02-06 ||
|-
| 0.7 || 2012-05-16 ||
|-
| 0.8 || 2013-07-25 ||
|-
| 0.9 || 2014-02-01 ||
|-
| 0.10.0 || 2015-04-11 || Samsara DSL
|-
| 0.10.1 || 2015-05-31 ||
|-
| 0.10.2 || 2015-08-06 ||
|-
| 0.11.0 || 2015-08-07 ||
|-
| 0.11.1 || 2015-11-06 ||
|-
| 0.11.2 || 2016-03-11 ||
|-
| 0.12.0 || 2016-04-11 || Added Apache Flink engine
|-
| 0.12.1 || 2016-05-19  ||
|-
| 0.12.2 || 2016-06-13 ||
|-
| 0.13.0 || 2018-05-04  ||
|-
| 0.14.0 || 2019-03-07 || Source only (no binaries)
|-
| 14.1 || 2020-10-07  ||  
|}

=== Developers ===

Apache Mahout is developed by a community. The project is managed by a group called the "Project Management Committee" (PMC). The current PMC is Andrew Musselman,  Andrew Palumbo,  Drew Farris,  Isabel Drost-Fromm,  Jake Mannix,  Pat Ferrel,  Paritosh Ranjan,  Trevor Grant,  Robin Anil,  Sebastian Schelter,  Stevo Slavić.&lt;ref&gt;https://projects.apache.org/committee.html?mahout&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{Official website}}

{{Apache Software Foundation}}
{{Use dmy dates|date=June 2019}}

{{DEFAULTSORT:Mahout}}
[[Category:Apache Software Foundation projects|Mahout]]
[[Category:Hadoop]]
[[Category:Data mining and machine learning software]]
[[Category:Big data products]]
[[Category:2009 software]]</text>
      <sha1>5djpfzfwky571cvocz63kjx0j5xgrfj</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Oozie</title>
    <ns>0</ns>
    <id>6728387</id>
    <revision>
      <id>997108029</id>
      <parentid>979432450</parentid>
      <timestamp>2020-12-30T01:27:20Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 2 templates: hyphenate params (2×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2973" xml:space="preserve">{{multiple issues|
{{notability|Products|date=January 2013}}
{{primary sources|date=January 2013}}
}}
{{Infobox software
| name = Apache Oozie
| logo = [[File:Apache_Oozie_Logo.svg|250px|Apache Oozie Logo]]
| logo caption = 
| screenshot = &lt;!-- Image name is enough --&gt;
| caption = 
| screenshot alt = 
| collapsible = 
| author = 
| developer = [[Apache Software Foundation]]
| released = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued = 
| latest release version = 5.2.0
| latest release date = {{Start date and age|2019|12|05|df=yes}}&lt;ref&gt;{{cite web|url=https://www-us.apache.org/dist/oozie/|access-date=28 May 2020|title=Index of /dist/oozie}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| repo = {{URL|https://gitbox.apache.org/repos/asf/oozie.git|Oozie Repository}}
| programming language = [[Java (programming language)|Java]],&lt;ref&gt;{{cite web|url=https://github.com/apache/oozie/tree/master/core/src/main/java/org/apache/oozie|title=apache/oozie - core/src/main/java/org/apache/oozie|access-date=28 May 2020}}&lt;/ref&gt; [[JavaScript]]
| operating system = [[Cross-platform]]
| platform = [[Java virtual machine]]
| size = 
| language = 
| language count = &lt;!-- Number only --&gt;
| language footnote = 
| genre = 
| license = [[Apache License 2.0]]
| alexa = 
| website = {{URL|//oozie.apache.org/}}
| standard = 
| AsOf = 
}}
'''Apache Oozie''' is a server-based [[workflow]] [[Scheduling (computing)|scheduling]] system to manage [[Apache Hadoop|Hadoop]] jobs.

Workflows in Oozie are defined as a collection of control flow and action [[Vertex (graph theory)|nodes]] in a [[directed acyclic graph]]. Control flow nodes define the beginning and the end of a workflow (start, end, and failure nodes) as well as a mechanism to control the workflow execution path (decision, fork, and join nodes). Action nodes are the mechanism by which a workflow triggers the execution of a computation/processing task. Oozie provides support for different types of actions including Hadoop [[MapReduce]], Hadoop distributed file system operations, [[Pig (programming tool)|Pig]], [[Secure Shell|SSH]], and [[email]]. Oozie can also be extended to support additional types of actions.

Oozie workflows can be parameterised using variables such as &lt;code&gt;${inputDir}&lt;/code&gt; within the workflow definition. When submitting a workflow job, values for the parameters must be provided. If properly parameterized (using different output directories), several identical workflow jobs can run concurrently.

Oozie is implemented as a Java [[web application]] that runs in a [[Java servlet]] container and is distributed under the [[Apache License]] 2.0.

==References==
{{Reflist}}

==External links==
* {{Official website|//oozie.apache.org/}}

{{Apache Software Foundation}}

{{DEFAULTSORT:Oozie}}
[[Category:Apache Software Foundation projects|Oozie]]
[[Category:Hadoop]]
[[Category:Workflow applications]]</text>
      <sha1>h2ss9j9qlgbr7m4gcbkujsih8y0r1td</sha1>
    </revision>
  </page>
  <page>
    <title>Apache ORC</title>
    <ns>0</ns>
    <id>59536460</id>
    <revision>
      <id>1000031228</id>
      <parentid>972220627</parentid>
      <timestamp>2021-01-13T05:04:53Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>/* top */Expanded [[Template:Notability]] and [[WP:AWB/GF|general fixes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2812" xml:space="preserve">{{multiple issues|
{{notability|Products|date=February 2019}}
{{third-party|date=February 2019}}
}}
{{Infobox software
| name = Apache ORC
| logo = [[File:Apache_ORC_Logo.svg|200px|Apache ORC]]
| screenshot = 
| caption = Apache ORC
| developer = 
| released = {{Start date and age|2013|02|20|df=yes}}&lt;ref&gt;{{cite web|title=The Stinger Initiative: Making Apache Hive 100 Times Faster|url=https://hortonworks.com/blog/100x-faster-hive/ |access-date= Jan 1, 2019 }}&lt;/ref&gt;
| latest release version = 1.6.0
| latest release date = {{Start date and age|2019|09|03|df=yes}}&lt;ref&gt;{{cite web|title=Releases|url=https://orc.apache.org/docs/releases.html}}&lt;/ref&gt;
| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| repo = {{URL|https://github.com/apache/orc|ORC Repository}}
| programming language = 
| genre = [[Database management system]]
| license = [[Apache License 2.0]]
| website = {{URL|https://orc.apache.org/}}
}}
'''Apache ORC''' (Optimized Row Columnar) is a [[free and open-source]] [[Column-oriented DBMS|column-oriented]] data storage format of the [[Apache Hadoop]] ecosystem. It is similar to the other columnar-storage file formats available in the [[Apache Hadoop|Hadoop]] ecosystem such as [[RCFile]] and [[Apache Parquet|Parquet]]. It is compatible with most of the data processing frameworks in the [[Hadoop]] environment.

In  February 2013, the Optimized Row Columnar (ORC) file format was announced by [[Hortonworks]] in collaboration with [[Facebook]].&lt;ref&gt;{{Cite news |title= The Stinger Initiative: Making Apache Hive 100 Times Faster |work= Hortonworks blog |date= February 20, 2013 |author= Alan Gates |url= https://hortonworks.com/blog/100x-faster-hive/  |access-date= Dec 31, 2018 }}&lt;/ref&gt;
A month later, the [[Apache Parquet]] format was announced, developed by [[Cloudera]] and [[Twitter]].&lt;ref&gt;{{Cite web |title= Introducing Parquet: Efficient Columnar Storage for Apache Hadoop |date= March 13, 2013 |author= Justin Kestelyn |work= Cloudera blog |url= http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/ |access-date= May 4, 2017 |archive-url= https://web.archive.org/web/20160919221247/http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/ |archive-date= September 19, 2016 |url-status= dead }}&lt;/ref&gt;

==See also==
{{Portal|Free and open-source software}}
* [[Apache Hive]]
* [[Apache NiFi]]
* [[Pig (programming tool)]]
* [[Apache Spark]]
* [[Presto (SQL query engine)]]

==References==
{{Reflist}}

{{Apache Software Foundation}}

{{DEFAULTSORT:ORC}}
[[Category:2013 software]]
[[Category:Apache Software Foundation projects|ORC]]
[[Category:Cloud computing]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>3hrhe149mhjmp5cmey9j5hndz56z73d</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Parquet</title>
    <ns>0</ns>
    <id>51579024</id>
    <revision>
      <id>993520562</id>
      <parentid>985768412</parentid>
      <timestamp>2020-12-11T01:32:04Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 11 templates: del empty params (6×); hyphenate params (3×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7860" xml:space="preserve">{{Primary sources|date=October 2016}}
{{Infobox software
| name = Apache Parquet
| logo = [[File:Apache_Parquet_Logo.svg|250px|Apache Parquet]]
| screenshot = 
| caption = Apache Parquet
| developer = 
| released = {{Start date and age|2013|03|13|df=yes}} &lt;!-- https://web.archive.org/web/20130504133255/http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/ --&gt;
| latest release version = 2.8.0
| latest release date = {{Start date and age|2020|01|13|df=yes}}&lt;ref&gt;{{cite web|url=https://github.com/apache/parquet-format/releases|title=Github releases|access-date=26 March 2020}}&lt;/ref&gt;
&lt;!-- This is a comment block.
     Before the version was referring to Parquet-MR, an implementation of the Parquet format:
| latest release version = 1.11.0&lt;ref&gt;{{cite web|title=Github releases|url=https://github.com/apache/parquet-mr/releases}}&lt;/ref&gt;
| latest release date = {{Start date and age|2019|03|19|df=yes}} 
--&gt;| latest preview version = 
| latest preview date = 
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]] (reference implementation)&lt;ref&gt;{{cite web|url=https://github.com/apache/parquet-mr|title=Parquet-MR source code|access-date=2 July 2019}}&lt;/ref&gt;
| genre = [[Column-oriented DBMS]]
| license = [[Apache License 2.0]]
| website = {{URL|https://parquet.apache.org}}
}}
'''Apache Parquet''' is a [[free and open-source]] [[Column-oriented DBMS|column-oriented]] data storage format of the [[Apache Hadoop]] ecosystem. It is similar to the other columnar-storage file formats available in [[Apache Hadoop|Hadoop]] namely [[RCFile]] and [[Apache ORC|ORC]]. It is compatible with most of the data processing frameworks in the [[Hadoop]] environment. It provides efficient [[data compression]] and [[encoding]] schemes with enhanced performance to handle complex data in bulk.

==History==
The [[Open-source software|open-source]] project to build Apache Parquet began as a joint effort between [[Twitter]]&lt;ref&gt;{{cite web|url=https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop|title=Release Date}}&lt;/ref&gt; and [[Cloudera]].&lt;ref&gt;{{Cite web|url=http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/|archive-url=https://web.archive.org/web/20130504133255/http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/|url-status=dead|archive-date=2013-05-04|title=Introducing Parquet: Efficient Columnar Storage for Apache Hadoop - Cloudera Engineering Blog|date=2013-03-13|language=en-US|access-date=2018-10-22}}&lt;/ref&gt; Parquet was designed as an improvement upon the Trevni columnar storage format created by Hadoop creator [[Doug Cutting]]. The first version&amp;mdash;Apache Parquet 1.0&amp;mdash;was released in July 2013. Since April 27, 2015, Apache Parquet is a top-level Apache Software Foundation (ASF)-sponsored project.&lt;ref&gt;http://www.infoworld.com/article/2915565/big-data/apache-parquet-paves-the-way-towards-better-hadoop-data-storage.html&lt;/ref&gt;&lt;ref&gt;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75&lt;/ref&gt;

==Features==
Apache Parquet is implemented using the record-shredding and assembly algorithm,&lt;ref&gt;{{cite web|title=The striping and assembly algorithms from the Google-inspired Dremel paper|url=https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper|website=github|access-date=13 November 2017}}&lt;/ref&gt; which accommodates the complex [[data structures]] that can be used to store the data.&lt;ref name=":1"&gt;{{cite web|url=https://parquet.apache.org/documentation/latest/|title=Apache Parquet Documentation}}&lt;/ref&gt; The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:&lt;ref&gt;{{cite web|title=Apache Parquet Cloudera|url=http://www.cloudera.com/documentation/enterprise/5-5-x/topics/cdh_ig_parquet.html}}&lt;/ref&gt;

* Column-wise compression is efficient and saves storage space
* Compression techniques specific to a type can be applied as the column values tend to be of the same type
* Queries that fetch specific column values need not read the entire row data thus improving performance
* Different encoding techniques can be applied to different columns

Apache Parquet is implemented using the [[Apache Thrift]] framework which increases its flexibility; it can work with a number of programming languages like [[C++]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[PHP]], etc.&lt;ref&gt;{{cite web|title=Apache Thrift|url=http://thrift.apache.org/}}&lt;/ref&gt;

As of August 2015,&lt;ref&gt;{{cite web|title=Supported Frameworks|url=https://cwiki.apache.org/confluence/display/Hive/Parquet}}&lt;/ref&gt; Parquet supports the big-data-processing frameworks including [[Apache Hive]], [[Apache Drill]], [[Apache Impala]], [[Apache Crunch]], [[Apache Pig]], [[Cascading (software)|Cascading]], [[Presto (SQL query engine)|Presto]] and [[Apache Spark]].

== Compression and encoding ==
In Parquet, compression is performed column by column, which enables different encoding schemes to be used for text and integer data. This strategy also keeps the door open for newer and better encoding schemes to be implemented as they are invented.

=== Dictionary encoding ===
Parquet has an automatic dictionary encoding enabled dynamically for data with a ''small'' number of unique values (i.e. below 10&lt;sup&gt;5&lt;/sup&gt;) that enables significant compression and boosts processing speed.&lt;ref name=":0"&gt;{{Cite web|url=https://blog.twitter.com/2013/announcing-parquet-10-columnar-storage-for-hadoop|title=Announcing Parquet 1.0: Columnar Storage for Hadoop {{!}} Twitter Blogs|website=blog.twitter.com|access-date=2016-09-14}}&lt;/ref&gt;

=== Bit packing ===
Storage of integers is usually done with dedicated 32 or 64 bits per integer. For small integers, packing multiple integers into the same space makes storage more efficient.&lt;ref name=":0" /&gt;

=== Run-length encoding (RLE) ===
To optimize storage of multiple occurrences of the same value, a single value is stored once along with the number of occurrences.&lt;ref name=":0" /&gt;

Parquet implements a hybrid of bit packing and RLE, in which the encoding switches based on which produces the best compression results. This strategy works well for certain types of integer data and combines well with dictionary encoding.&lt;ref name=":0" /&gt;

==Comparison==
{{Unreferenced section|date=October 2016}}

Apache Parquet is comparable to [[RCFile]] and [[Apache ORC|Optimized Row Columnar (ORC)]] file formats {{mdash}} all three fall under the category of columnar data storage within the Hadoop ecosystem. They all have better compression and encoding with improved read performance at the cost of slower writes. In addition to these features, Apache Parquet supports limited [[schema evolution]], i.e., the schema can be modified according to the changes in the data. It also provides the ability to add new columns and merge schemas that don't conflict.

==See also==
{{Portal|Free and open-source software}}
* [[Pig (programming tool)]]
* [[Apache Hive]]
* [[Apache Impala]]
* [[Apache Drill]]
* [[Apache Kudu]]
* [[Apache Spark]]
* [[Apache Thrift]]
* [[Presto (SQL query engine)]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website}}
* [https://research.google.com/pubs/pub36632.html Dremel paper]
* [https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04 How to Be a Hero with Powerful Apache Parquet, Google and Amazon]

{{Apache Software Foundation}}

{{DEFAULTSORT:Parquet}}
[[Category:2015 software]]
[[Category:Apache Software Foundation projects|Parquet]]
[[Category:Cloud computing]]
[[Category:Free system software]]
[[Category:Hadoop]]
[[Category:Software using the Apache license]]</text>
      <sha1>fd0rfn2wj444um236brohlz40v1sdky</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Pig</title>
    <ns>0</ns>
    <id>29417433</id>
    <revision>
      <id>996940193</id>
      <parentid>972221122</parentid>
      <timestamp>2020-12-29T07:41:13Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 9 templates: hyphenate params (15×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9024" xml:space="preserve">{{Infobox software
| name = Apache Pig
| logo = [[File:Apache_Pig_Logo.svg|250px|Apache Pig Logo]]
| caption = 
| author = 
| developer = [[Apache Software Foundation]], [[Yahoo!|Yahoo Research]]
| released = {{Start date and age|2008|09|11}}
| latest release version = 0.17.0
| latest release date = {{Start date and age|2017|06|19}}
| latest preview version = 
| latest preview date = 
| operating system = [[Microsoft Windows]], [[OS X]], [[Linux]]
| size = 
| programming language = 
| genre = Data analytics
| license = [[Apache License 2.0]]
| website = {{URL|https://pig.apache.org}}
}}
'''Apache Pig'''&lt;ref name="mainpage"&gt;{{cite web |url=http://pig.apache.org/|title=Hadoop: Apache Pig|access-date=Sep 2, 2011}}&lt;/ref&gt;
is a high-level platform for creating programs that run on [[Hadoop|Apache Hadoop]]. The language for this platform is called '''Pig Latin'''.&lt;ref name="mainpage"/&gt;  Pig can execute its Hadoop jobs in [[MapReduce]], Apache Tez, or [[Apache Spark]].&lt;ref&gt;{{Cite web|url=https://issues.apache.org/jira/browse/PIG-4167|title=[PIG-4167] Initial implementation of Pig on Spark - ASF JIRA|website=issues.apache.org|access-date=2018-12-29}}&lt;/ref&gt;  Pig Latin abstracts the programming from the [[Java (programming language)|Java]] MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of [[SQL]] for [[relational database management system]]s. Pig Latin can be extended using [[user-defined function]]s (UDFs) which the user can write in Java, [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]] or [[Groovy (programming language)|Groovy]]&lt;ref&gt;{{cite web|url= http://pig.apache.org/docs/r0.11.1/udf.html|title=Pig user defined functions|access-date=May 3, 2013}}&lt;/ref&gt; and then call directly from the language.

==History==
Apache Pig was originally&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/pig-road-efficient-high-level-language-hadoop-413.html|title=Yahoo Blog:Pig – The Road to an Efficient High-level language for Hadoop|access-date=May 23, 2015|url-status=dead|archive-url=https://web.archive.org/web/20160203181220/https://developer.yahoo.com/blogs/hadoop/pig-road-efficient-high-level-language-hadoop-413.html|archive-date=February 3, 2016}}&lt;/ref&gt; developed at [[Yahoo!|Yahoo Research]] around 2006 for researchers to have an ad-hoc way of creating and executing MapReduce jobs on very large data sets. In 2007,&lt;ref&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/pig-incubation-apache-software-foundation-393.html|title=Pig into Incubation at the Apache Software Foundation|access-date=May 23, 2015|url-status=dead|archive-url=https://web.archive.org/web/20160203162733/https://developer.yahoo.com/blogs/hadoop/pig-incubation-apache-software-foundation-393.html|archive-date=February 3, 2016}}&lt;/ref&gt; it was moved into the [[Apache Software Foundation]].

{| class="wikitable"
|-
! Version
! Original release date
! Latest version
! Release date&lt;ref&gt;{{cite web|url=https://pig.apache.org/releases.html|title=Apache Pig Releases|website=Apache|access-date=2019-03-13}}&lt;/ref&gt;
|-
| {{Version|o|0.1}}
| 2008-09-11
| 0.1.1
| 2008-12-05
|-
| {{Version|o|0.2}}
| 2009-04-08
| 0.2.0
| 2009-04-08
|-
| {{Version|o|0.3}}
| 2009-06-25
| 0.3.0
| 2009-06-25
|-
| {{Version|o|0.4}}
| 2009-08-29
| 0.4.0
| 2009-08-29
|-
| {{Version|o|0.5}}
| 2009-09-29
| 0.5.0
| 2009-09-29
|-
| {{Version|o|0.6}}
| 2010-03-01
| 0.6.0
| 2010-03-01
|-
| {{Version|o|0.7}}
| 2010-05-13
| 0.7.0
| 2010-05-13
|-
| {{Version|o|0.8}}
| 2010-12-17
| 0.8.1
| 2011-04-24
|-
| {{Version|o|0.9}}
| 2011-07-29
| 0.9.2
| 2012-01-22
|-
| {{Version|o|0.10}}
| 2012-01-22
| 0.10.1
| 2012-04-25
|-
| {{Version|o|0.11}}
| 2013-02-21
| 0.11.1
| 2013-04-01
|-
| {{Version|o|0.12}}
| 2013-10-14
| 0.12.1
| 2014-04-14
|-
| {{Version|o|0.13}}
| 2014-07-04
| 0.13.0
| 2014-07-04
|-
| {{Version|o|0.14}}
| 2014-11-20
| 0.14.0
| 2014-11-20
|-
| {{Version|o|0.15}}
| 2015-06-06
| 0.15.0
| 2015-06-06
|-
| {{Version|o|0.16}}
| 2016-06-08
| 0.16.0
| 2016-06-08
|-
| {{Version|c|0.17}}
| 2017-06-19
| 0.17.0
| 2017-06-19
|-
| colspan="5" | {{smalldiv|{{Version |l |show=111110}}}}
|}

==Example==
Below is an example of a "[[word count|Word Count]]" program in Pig Latin:

&lt;syntaxhighlight lang=pig&gt;
 input_lines = LOAD '/tmp/my-copy-of-all-pages-on-internet' AS (line:chararray);
 
 -- Extract words from each line and put them into a pig bag
 -- datatype, then flatten the bag to get one word on each row
 words = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;
 
 -- filter out any words that are just white spaces
 filtered_words = FILTER words BY word MATCHES '\\w+';
 
 -- create a group for each word
 word_groups = GROUP filtered_words BY word;
 
 -- count the entries in each group
 word_count = FOREACH word_groups GENERATE COUNT(filtered_words) AS count, group AS word;
 
 -- order the records by count
 ordered_word_count = ORDER word_count BY count DESC;
 STORE ordered_word_count INTO '/tmp/number-of-words-on-internet';
&lt;/syntaxhighlight&gt;

The above program will generate parallel executable tasks which can be distributed across multiple machines in a Hadoop cluster to count the number of words in a dataset such as all the webpages on the internet.

==Pig vs SQL==
In comparison to SQL, Pig
# has a nested relational model,
# uses [[lazy evaluation]], 
# uses [[extract, transform, load]] (ETL), 
# is able to store data at any point during a [[Pipeline (software)|pipeline]], 
# declares [[execution plan]]s, 
# supports pipeline splits, thus allowing workflows to proceed along [[directed acyclic graph|DAG]]s instead of strictly sequential pipelines. 
On the other hand, it has been argued [[DBMS]]s are substantially faster than the MapReduce system once the data is loaded, but that loading the data takes considerably longer in the database systems. It has also been argued [[Relational database management system|RDBMS]]s offer out of the box support for column-storage, working with compressed data, indexes for efficient random data access, and transaction-level fault tolerance.&lt;ref&gt;{{cite web|url=http://database.cs.brown.edu/papers/stonebraker-cacm2010.pdf|title=Communications of the ACM: MapReduce and Parallel DBMSs: Friends or Foes?|access-date=May 23, 2015|url-status=dead|archive-url=https://web.archive.org/web/20150701205317/http://database.cs.brown.edu/papers/stonebraker-cacm2010.pdf|archive-date=July 1, 2015}}&lt;/ref&gt;

Pig Latin is [[Procedural programming|procedural]] and fits very naturally in the pipeline paradigm while SQL is instead [[Declarative programming|declarative]]. In SQL users can specify that data from two tables must be joined, but not what join implementation to use (You can specify the implementation of JOIN in SQL, thus "... for many SQL applications the query writer may not have enough knowledge of the data or enough expertise to specify an appropriate join algorithm."). Pig Latin allows users to specify an implementation or aspects of an implementation to be used in executing a script in several ways.&lt;ref name = ypgd /&gt; In effect, Pig Latin programming is similar to specifying a query execution plan, making it easier for programmers to explicitly control the flow of their data processing task.&lt;ref&gt;{{cite web |url=http://infolab.stanford.edu/~olston/publications/sigmod08.pdf|title=ACM SigMod 08: Pig Latin: A Not-So-Foreign Language for Data Processing|access-date=May 23, 2015}}&lt;/ref&gt;

SQL is oriented around queries that produce a single result. SQL handles trees naturally, but has no built in mechanism for splitting a data processing stream and applying different operators to each sub-stream. Pig Latin script describes a [[directed acyclic graph]] (DAG) rather than a pipeline.&lt;ref name = ypgd /&gt;

Pig Latin's ability to include user code at any point in the pipeline is useful for pipeline development. If SQL is used, data must first be imported into the database, and then the cleansing and transformation process can begin.&lt;ref name=ypgd&gt;{{cite web|url=https://developer.yahoo.com/blogs/hadoop/comparing-pig-latin-sql-constructing-data-processing-pipelines-444.html|title=Yahoo Pig Development Team: Comparing Pig Latin and SQL for Constructing Data Processing Pipelines|access-date=May 23, 2015|url-status=dead|archive-url=https://web.archive.org/web/20150530103839/https://developer.yahoo.com/blogs/hadoop/comparing-pig-latin-sql-constructing-data-processing-pipelines-444.html|archive-date=May 30, 2015}}&lt;/ref&gt;

==See also==
*[[Apache Hive]]
*[[Sawzall (programming language)|Sawzall]] — similar tool from Google

==References==
{{Reflist}}

==External links==
*{{Official website|https://pig.apache.org}}

{{Apache Software Foundation}}

{{DEFAULTSORT:Pig}}
[[Category:Cloud computing]]
[[Category:Query languages]]
[[Category:Data modeling languages]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation projects|Pig]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Software using the Apache license]]</text>
      <sha1>cjt15vglvpo352csmd9t484z0cduuth</sha1>
    </revision>
  </page>
  <page>
    <title>Apache SystemML</title>
    <ns>0</ns>
    <id>49338480</id>
    <revision>
      <id>982929433</id>
      <parentid>982928819</parentid>
      <timestamp>2020-10-11T06:59:19Z</timestamp>
      <contributor>
        <username>Taxi283</username>
        <id>40340515</id>
      </contributor>
      <minor/>
      <comment>Add archive.org link for the removed external link.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4925" xml:space="preserve">&lt;blockquote&gt;'''Apache SystemML''' is now '''Apache SystemDS'''. Please visit [http://systemds.apache.org systemds.apache.org] for information.&lt;/blockquote&gt;{{Infobox software
| name                   = Apache SystemDS
| logo                   = [[File:Apache SystemML Logo.svg|250px|Apache SystemML Logo]]
| author                 = [[Shivakumar Vaithyanathan]]
| developer              = [[Apache Software Foundation]], [[IBM]]
| released               = {{Start date and age|2015|11|02}}
| latest release version = 1.2.0
| operating system       = [[Linux]], [[macOS]], [[Windows]]
| latest release date    = {{Start date and age|2018|08|24}}
| repo                   = {{URL|https://gitbox.apache.org/repos/asf?p&amp;#61;systemds.git|SystemDS Repository}}
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Machine Learning]], [[Deep Learning]], [[Data Science]]
| license                = [[Apache License 2.0]]
| website                = {{URL|http://systemds.apache.org/}}
}}
'''Apache SystemML''' is a flexible machine learning system that automatically scales to [[Apache Spark|Spark]] and [[Apache Hadoop|Hadoop]] clusters. SystemML's distinguishing characteristics are:

# Algorithm customizability via R-like and Python-like languages.
# Multiple execution modes, including Standalone, [[Apache Spark|Spark]] Batch, [[Apache Spark|Spark]] MLContext, [[Apache Hadoop|Hadoop]] Batch, and JMLC.
# Automatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.

==History==
SystemML was created in 2010 by researchers at the [[IBM Almaden Research Center]] led by IBM Fellow Shivakumar Vaithyanathan. It was observed that data scientists would write machine learning algorithms in languages such as [[R (programming language)|R]] and [[Python (programming language)|Python]] for small data. When it came time to scale to big data, a systems programmer would be needed to scale the algorithm in a language such as [[Scala (programming language)|Scala]]. This process typically involved days or weeks per iteration, and errors would occur translating the algorithms to operate on big data. SystemML seeks to simplify this process. A primary goal of SystemML is to automatically scale an algorithm written in an R-like or Python-like language to operate on big data, generating the same answer without the error-prone, multi-iterative translation approach.

On June 15, 2015, at the Spark Summit in San Francisco, Beth Smith, General Manager of IBM Analytics, announced that IBM was open-sourcing SystemML as part of IBM's major commitment to [[Apache Spark]] and Spark-related projects. SystemML became publicly available on [[GitHub]] on August 27, 2015 and became an [[Apache Incubator]] project on November 2, 2015. On May 17, 2017, the Apache Software Foundation Board approved the graduation of Apache SystemML as an Apache Top Level Project.

==See also==
*[[Comparison of deep learning software]]

==External links==
*[http://systemml.apache.org/ Apache SystemML website]
*[http://researcher.watson.ibm.com/researcher/view_group.php?id=3174 IBM Research - SystemML]
*[https://web.archive.org/web/20180321002223/http://www.spark.tc/q-a-with-shiv-vaithyanathan-creator-of-systemml-and-ibm-fellow/ Q &amp; A with Shiv Vaithyanathan, Creator of SystemML and IBM Fellow]
*[http://www.spark.tc/a-universal-translator-for-big-data-and-machine-learning/ A Universal Translator for Big Data and Machine Learning]
*[https://www.youtube.com/watch?v=WkYqjWL1xzk SystemML: Declarative Machine Learning at Scale presentation by Fred Reiss]
*[http://researcher.watson.ibm.com/researcher/files/us-ytian/systemML.pdf SystemML: Declarative Machine Learning on MapReduce]
*[http://www.vldb.org/pvldb/vol7/p553-boehm.pdf Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML]
*[https://web.archive.org/web/20150218102423/http://sites.computer.org/debull/A14sept/p52.pdf SystemML’s Optimizer: Plan Generation for Large-Scale Machine Learning Programs]
*[http://www.zdnet.com/article/ibms-systemml-machine-learning-system-becomes-apache-incubator-project/ IBM's SystemML machine learning system becomes Apache Incubator project]
*[http://www.theinquirer.net/inquirer/news/2413132/ibm-donates-machine-learning-tech-to-apache-spark-open-source-community IBM donates machine learning tech to Apache Spark open source community]
*[http://www.eweek.com/developer/ibms-systemml-moves-forward-as-apache-incubator-project.html IBM's SystemML Moves Forward as Apache Incubator Project]

{{Apache Software Foundation}}

{{DEFAULTSORT:SystemML}}
[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation projects|SystemML]]
[[Category:Software using the Apache license]]
[[Category:Java platform]]
[[Category:Big data products]]
[[Category:2015 software]]</text>
      <sha1>r4l1bd2yatf4cryemi871ew1dyxhrs6</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Ambari</title>
    <ns>0</ns>
    <id>47478005</id>
    <revision>
      <id>997206831</id>
      <parentid>972187415</parentid>
      <timestamp>2020-12-30T13:31:57Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 3 templates: hyphenate params (1×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2239" xml:space="preserve">{{Advert|date=October 2019}}
{{Infobox software
| name = Ambari
| logo = [[File:Apache_Ambari_Logo.svg|250px|Apache Ambari Logo]]
| developer = [[Apache Software Foundation]]
| discontinued = 
| latest release version = 2.7.4
| latest release date = {{Start date and age|2019|09|13}}&lt;ref&gt;{{cite web|url=https://gitbox.apache.org/repos/asf/ambari/repo?p=ambari.git;a=tag;h=8d5fa35da52c40b2f1dd77521d4309ce210d1fec|title=Release Apache Ambari-2.7.4|access-date=16 September 2019}}&lt;/ref&gt;
| latest preview version = &lt;!--2.7.4 RC0 --&gt;
| latest preview date = &lt;!--{{Start date and age|2019|08|29}}&lt;ref&gt;{{cite|url=https://gitbox.apache.org/repos/asf/ambari/repo?p=ambari.git;a=commit;h=b34482743fce2cf8cf9735f90b025ae59018a05a|title=ASF Git Repos - ambari.git/commit|access-date=7 September 2019}}&lt;/ref&gt;--&gt;
| repo = {{URL|https://gitbox.apache.org/repos/asf/ambari/repo?p&amp;#61;ambari.git;|Ambari Repository}}
| programming language = [[Java (programming language)|Java]], [[JavaScript]], [[Python (programming language)|Python]]
| operating system = [[Cross-platform]]
| genre = [[Distributed computing]]
| license = [[Apache License 2.0]]
| website = {{url|//ambari.apache.org}}
}}
'''Apache Ambari''' is a software project of the [[Apache Software Foundation]].&lt;ref&gt;{{cite web|url=https://ambari.apache.org/|title=Ambari|work=[[Apache Software Foundation]]|access-date=5 December 2016}}&lt;/ref&gt; Ambari enables system administrators to provision, manage and monitor a [[Hadoop]] cluster, and also to integrate Hadoop with the existing enterprise infrastructure. Ambari was a sub-project of Hadoop but is now a [[Apache Software Foundation#Projects|top-level]] project in its own right.

Ambari is used by companies including [[IBM]], [[Hortonworks]],  [[Cardinal Health]], [[EBay]], [[Expedia]], [[Kayak.com]], [[Lending club]], [[Neustar]], [[Macy's]], [[Pandora Radio]], [[Samsung]], [[Shutterfly]], and [[Spotify]].{{citation needed|date=December 2015}}

==References==
{{Reflist}}
{{Portal|Computer programming}}

==External links==
*{{Official website|//ambari.apache.org}}

{{Apache Software Foundation}}

{{DEFAULTSORT:Ambari}}
[[Category:Apache Software Foundation projects|Ambari]]
[[Category:Configuration management]]
[[Category:Hadoop]]</text>
      <sha1>2x4sa1vqicvzaccix9bg9g95rkq09ey</sha1>
    </revision>
  </page>
</mediawiki>
